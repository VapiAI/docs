Below is all the contents of our docs: 



 This is the content for the doc README.md 

 # Vapi Platform Documentation

This repository contains the source files for the documentation found at [docs.vapi.ai](https://docs.vapi.ai/). 

Get started with Vapi here: [docs.vapi.ai/introduction](https://docs.vapi.ai/introduction)

View the API Reference here: [docs.vapi.ai/api-reference](https://docs.vapi.ai/api-reference/)

Explore our Client and Server SDKs here: [docs.vapi.ai/sdks](https://docs.vapi.ai/sdks)

| Vapi Developer Ecosystem  |  |
|--------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Real-time SDKs** | [Web](https://github.com/VapiAI/web) · [Flutter](https://github.com/VapiAI/flutter) · [React Native](https://github.com/VapiAI/react-native-sdk) · [iOS](https://github.com/VapiAI/ios) · [Python](https://github.com/VapiAI/python) · [Vanilla](https://github.com/VapiAI/html-script-tag) |
| **Client Examples** | [Next.js](https://github.com/VapiAI/client-side-example-javascript-next) · [React](https://github.com/VapiAI/client-side-example-javascript-react) · [Flutter](https://github.com/VapiAI/flutter/tree/main/example) · [React Native](https://github.com/VapiAI/client-side-example-react-native) |
| **Server Examples** | [Vercel](https://github.com/VapiAI/server-side-example-serverless-vercel) · [Cloudflare](https://github.com/VapiAI/server-side-example-serverless-cloudflare) · [Supabase](https://github.com/VapiAI/server-side-example-serverless-supabase) · [Node](https://github.com/VapiAI/server-side-example-javascript-node) · [Bun](https://github.com/VapiAI/server-side-example-javascript-bun) · [Deno](https://github.com/VapiAI/server-side-example-javascript-deno) · [Flask](https://github.com/VapiAI/server-side-example-python-flask) · [Laravel](https://github.com/VapiAI/server-side-example-php-laravel) · [Go](https://github.com/VapiAI/server-side-example-go-gin) · [Rust](https://github.com/VapiAI/server-side-example-rust-actix) |
| **Resources** | [Official Docs](https://docs.vapi.ai/) · [API Reference](https://api.vapi.ai/api) |
| **Community** | [Videos](/community/videos) · [UI Library](https://www.vapiblocks.com/) |

## How can I contribute to these docs?

You can suggest edits by making a pull request.

## How to update documentation?

### Local Development server

To run a local development server with hot-reloading you can run the following command

```sh
fern docs dev
```

#### Hosted URL

To update your documentation on a hosted URL, run
```
# npm install -g fern-api
fern generate --docs
```
To preview your documentation, run
```
# npm install -g fern-api
fern generate --docs --preview
```
The repository contains GitHub workflows that will automatically run these commands for you. For example, when you make a PR a preview link will be auto-generated and when you merge to main the docs site will update.


 This is the content for the doc advanced.md 

 # Vapi Api Configuration

This repository contains our Fern Configuration:

- [OpenAPI spec](./openapi.json)
- [OpenAPI Overrides](./openapi-overrides.yml)
- [SDK generator config](./fern/generators.yml)

## Setup

```sh
npm install -g fern-api
```

## Validating your OpenAPI Specs

To validate your API, run:

```sh
fern check
```

## Managing SDKs

### Deploying your SDKs

To deploy your SDKs, simply run the `Release Python SDK` GitHub Action with the 
desired version for the release. Under the hood, this leverages the Fern CLI:

```sh
fern generate --api api --group python-sdk
```

### Developing SDKs

You can also regenerate the SDKs locally by running:

```sh
fern generate --api api --group python-sdk --preview --log-level debug
```

This will generate the SDK and download it to a local folder that can be pip installed.

```sh
pip install -e /fern/.preview/fern-python-sdk
```

## How to update documentation?

### Local Development server

To run a local development server with hot-reloading you can run the following command

```sh
fern docs dev
```

#### Hosted URL

To update your documentation on a hosted URL, run
```
# npm install -g fern-api
fern generate --docs
```
To preview your documentation, run
```
# npm install -g fern-api
fern generate --docs --preview
```
The repository contains GitHub workflows that will automatically run these commands for you. For example, when you make a PR a preview link will be auto-generated and when you merge to main the docs site will update.


 This is the content for the doc fern/GHL.mdx 

 ---
title: How to Connect Vapi with Make & GHL
slug: tools/GHL
---


Vapi's GHL/Make Tools integration allows you to directly import your GHL workflows and Make scenarios into Vapi as Tools. This enables you to create voicebots that can trigger your favorite app integrations and automate complex workflows using voice commands.

## What are GHL/Make Tools?

GHL (GoHighLevel) workflows and Make scenarios are powerful automation tools that allow you to connect and integrate various apps and services. With the GHL/Make Tools integration, you can now bring these automations into Vapi and trigger them using voice commands.

## How does the integration work?

1. **Import workflows and scenarios**: Navigate to the [Tools section](https://dashboard.vapi.ai/tools) in your Vapi dashboard and import your existing GHL workflows and Make scenarios.

2. **Add Tools to your assistants**: Once imported, you can add these Tools to your AI assistants, enabling them to trigger the automations based on voice commands.

3. **Trigger automations with voice**: Your AI assistants can now understand voice commands and execute the corresponding GHL workflows or Make scenarios, allowing for seamless voice-enabled automation.

## Setting up the GHL/Make Tools integration

1. **Create a GHL workflow or Make scenario**: Design your automation in GHL or Make, connecting the necessary apps and services.

2. **Import the workflow/scenario into Vapi**: In the Vapi dashboard, navigate to the Tools section and click on "Import." Select the GHL workflow or Make scenario you want to import.

3. **Configure the Tool**: Provide a name and description for the imported Tool, and map any required input variables to the corresponding Vapi entities (e.g., extracted from user speech).

4. **Add the Tool to your assistant**: Edit your AI assistant and add the newly imported Tool to its capabilities. Specify the voice commands that should trigger the Tool.

5. **Test the integration**: Engage with your AI assistant using the specified voice commands and verify that the corresponding GHL workflow or Make scenario is triggered successfully.

## Use case examples

### Booking appointments with AI callers

- Import a GHL workflow that handles appointment booking
- Configure the workflow to accept appointment details (date, time, user info) from Vapi
- Add the Tool to your AI assistant, allowing it to book appointments based on voice commands

### Updating CRMs with voice-gathered data

- Import a Make scenario that updates your CRM with customer information
- Map the scenario's input variables to entities extracted from user speech
- Enable your AI assistant to gather customer information via voice and automatically update your CRM

### Real Estate: Automated Property Information Retrieval

- Import a Make scenario that retrieves property information from your MLS (Multiple Listing Service) or real estate database
- Configure the scenario to accept a property address or MLS ID as input
- Add the Tool to your AI assistant, allowing potential buyers to request property details using voice commands
- Your AI assistant can then provide key information about the property, such as price, square footage, number of bedrooms/bathrooms, and amenities

### Healthcare/Telehealth: Appointment Reminders and Prescription Refills

- Import a GHL workflow that sends appointment reminders and handles prescription refill requests
- Configure the workflow to accept patient information and appointment/prescription details from Vapi
- Add the Tool to your AI assistant, enabling patients to request appointment reminders or prescription refills using voice commands
- Your AI assistant can confirm the appointment details, send reminders via SMS or email, and forward prescription refill requests to the appropriate healthcare provider

### Restaurant Ordering: Custom Order Placement and Delivery Tracking

- Import a Make scenario that integrates with your restaurant's online ordering system and delivery tracking platform
- Configure the scenario to accept customer information, order details, and delivery preferences from Vapi
- Add the Tool to your AI assistant, allowing customers to place custom orders and track their delivery status using voice commands
- Your AI assistant can guide customers through the ordering process, suggest menu items based on preferences, and provide real-time updates on the order status and estimated delivery time

## Best practices

- Break down complex automations into smaller, focused workflows or scenarios for better maintainability
- Use clear and concise naming conventions for your imported Tools and their input variables
- Thoroughly test the integration to ensure reliable performance and accurate data passing
- Keep your GHL workflows and Make scenarios up to date to reflect any changes in the connected apps or services

## Troubleshooting

- If a Tool is not triggering as expected, verify that the voice commands are correctly configured and the input variables are properly mapped
- Check the Vapi logs and the GHL/Make execution logs to identify any errors or issues in the automation flow
- Ensure that the necessary API credentials and permissions are correctly set up in both Vapi and the integrated apps/services

By leveraging Vapi's GHL/Make Tools integration, you can create powerful voice-enabled automations and streamline your workflows, all without extensive coding. Automate tasks, connect your favorite apps, and unlock the full potential of voice AI with Vapi.

## Get Support

Join our Discord to connect with other developers & connect with our team:

<CardGroup cols={2}>
  <Card
    title="Join Our Discord"
    icon="fa-brands fa-discord"
    iconType="solid"
    color="#5A65EA"
    href="https://discord.gg/pUFNcf2WmH"
  >
    Connect with our team & other developers using Vapi.
  </Card>
  <Card
    title="Email Support"
    icon="mailbox"
    iconType="solid"
    color="#7a7f85"
    href="mailto:support@vapi.ai"
  >
    Send our support team an email.
  </Card>
</CardGroup>

Here are some video tutorials that will guide you on how to use Vapi with services like Make and GoHighLevel:

<div class="video-grid">
  <iframe
    src="https://www.youtube.com/embed/PVP1P2nak4M?si=vGGAMZVI3Fzzik9X"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.youtube.com/embed/3LSXJECXpkc?si=hhWsXZeFYC6wM-cq"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.loom.com/embed/026d1c1a2cc64e479044619842ce8bd1?sid=c5934ff7-2f58-4cf3-952a-140938079ca0"
    title="GoHighLevel Loom Video"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.loom.com/embed/210579f412bd47de8964683b2f28c3ec?sid=1f289c56-d4bc-4923-85bb-6cbc0fe55361"
    title="GoHighLevel Bulk call Loom Video"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.youtube.com/embed/KwQmJbIOov4?si=x5ep0ziyIM5ueuvG"
    title="GoHighLevel Bulk call Loom Video"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
</div>


 This is the content for the doc fern/advanced/sip/sip-plivo.mdx 

 ---
title: Plivo SIP Integration
subtitle: How to integrate Plivo SIP with Vapi
slug: advanced/sip/plivo
---

<Warning>
Please note that Indian numbers cannot be obtained from Plivo for use with Vapi. This is due to TRAI regulations, which mandate SIP termination to occur via an Indian server—a requirement we currently do not fulfill.﻿
</Warning>

This guide walks you through setting up both outbound and inbound SIP trunking between Plivo and Vapi.

## Outbound Calls (Plivo to Vapi)

### Plivo Configuration

1. **Login to Plivo Console**
   
   Access the Plivo console at [https://console.plivo.com/accounts/login/](https://console.plivo.com/accounts/login/)

2. **Create IP Access Control List**
   
   Navigate to: Zentrunk(SIP) → Outbound Trunks → IP Access Control List → Create New IP Group
   
   - Name: Choose a descriptive name
   - IP Address List: Whitelist Vapi's fixed IPs:
     - 44.229.228.186/32
     - 44.238.177.138/32
   - Click "Create ACL"
   
   ![Plivo IP Access Control List](../../static/images/sip/sip-plivo-ip-acl.png)

3. **Create Outbound Trunk**
   
   Navigate to: Zentrunk(SIP) → Outbound Trunks → Trunks → Create New Outbound Trunk
   
   - Trunk Name: Choose a descriptive name
   - IP Access Control List: Select the IP ACL created in the previous step
   - Click "Create Trunk"
   
   ![Create New Outbound Trunk](../../static/images/sip/sip-plivo-outbound-trunk.png)

4. **Note Your Termination SIP Domain**
   
   After creating the trunk, note the Termination SIP Domain (format: 12700668357XXXXXX.zt.plivo.com)
   
   ![Termination SIP Domain](../../static/images/sip/sip-plivo-termination-sip-domain.png)

5. **Purchase a Phone Number**
   
   Navigate to: Numbers → Buy a new number
   
   ![Buy Phone Number](../../static/images/sip/sip-plivo-buy-phone-number.png)

### Vapi Configuration

1. **Get Your Vapi API Key**
   
   Sign in to the Vapi dashboard at [https://dashboard.vapi.ai/](https://dashboard.vapi.ai/) and retrieve your API key
   
   ![VAPI Dashboard](../../static/images/sip/sip-plivo-vapi-dashboard.png)

2. **Create a SIP Trunk Credential**
   
   Use the following API call, replacing the gateway IP with your Plivo Termination SIP Domain:

   ```bash
   curl -X POST https://api.vapi.ai/credential \
   -H "Content-Type: application/json" \
   -H "Authorization: Bearer your-vapi-private-api-key" \
   -d '{
     "provider": "byo-sip-trunk",
     "name": "PLIVO Trunk",
     "gateways": [
       {
         "ip": "1270066835XXXXXXXXX.zt.plivo.com"
       }
     ]
   }'
   ```

   ![SIP Trunk Credential Response](../../static/images/sip/sip-plivo-sip-trunk-credential-response.png)
   
   Note the `id` (credentialId) from the response for the next step.

3. **Register Your Phone Number**
   
   Associate your Plivo number with the SIP trunk:

   ```bash
   curl -X POST https://api.vapi.ai/phone-number \
   -H "Content-Type: application/json" \
   -H "Authorization: Bearer your-vapi-private-api-key" \
   -d '{
     "provider": "byo-phone-number",
     "name": "PLIVO SIP Number",
     "number": "1833684XXXX",
     "numberE164CheckEnabled": false,
     "credentialId": "a2c815b8-03f4-40f5-813c-xxxxxxxxxxxx"
   }'
   ```

   ![Phone Number Response](../../static/images/sip/sip-plivo-phone-number-response.png)
   
   Note the phone number ID from the response for making calls.

4. **Create a Vapi Assistant**
   
   Follow the steps at [https://docs.vapi.ai/quickstart/dashboard#create-an-assistant](https://docs.vapi.ai/quickstart/dashboard#create-an-assistant)
   
   ![Create VAPI Assistant](../../static/images/sip/sip-plivo-create-vapi-assistant.png)
   
   Note your Assistant ID for making calls.

5. **Make Outbound Calls**

   **Using the API:**
   
   ```bash
   curl --location 'https://api.vapi.ai/call/phone' \
   --header 'Authorization: Bearer your-vapi-private-api-key' \
   --header 'Content-Type: application/json' \
   --data '{
     "assistantId": "29d47d31-ba3c-451c-86ce-xxxxxxxxx",
     "customer": {
       "number": "9199437XXXXX",
       "numberE164CheckEnabled": false
     },
     "phoneNumberId": "eba2fb13-259f-4123-abfa-xxxxxxxxxxx"
   }'
   ```
   
   ![Outbound Call Response](../../static/images/sip/sip-plivo-outbound-call-response.png)
   
   **Using the Vapi Dashboard:**
   
   Select your Assistant and enter the destination number you want to call.
   
   ![VAPI Dashboard Call](../../static/images/sip/sip-plivo-vapi-dashboard-call.png)

## Inbound Calls (Vapi to Plivo)

### Plivo Configuration

1. **Login to Plivo Console**
   
   Access the Plivo console at [https://console.plivo.com/accounts/login/](https://console.plivo.com/accounts/login/)

2. **Create Origination URI**
   
   Navigate to: Zentrunk(SIP) → Inbound Trunks → Origination URI → Create New IP URI
   
   - Name: Choose a descriptive name
   - URI: Enter Vapi's SIP URI: `sip.vapi.ai;transport=udp`
   - Click "Create URI"
   
   ![Create New IP URI](../../static/images/sip/sip-plivo-create-new-ip-uri.png)

3. **Create Inbound Trunk**
   
   Navigate to: Zentrunk(SIP) → Inbound Trunks → Trunks → Create New Inbound Trunk
   
   - Trunk Name: Choose a descriptive name
   - Primary URI: Select the URI created in the previous step
   - Click "Create Trunk"
   
   ![Create New Inbound Trunk](../../static/images/sip/sip-plivo-create-new-inbound-trunk.png)

4. **Attach Phone Number to Inbound Trunk**
   
   Navigate to: Phone Numbers → Select your purchased number
   
   - In the Application dropdown, select "Zentrunk"
   - In the Zentrunk dropdown, select your inbound trunk
   - Save the changes
   
   ![Attach Number to Inbound Trunk](../../static/images/sip/sip-plivo-attach-number-to-inbound-trunk.png)

### Vapi Configuration

1. **Get Your Vapi API Key**
   
   Sign in to the Vapi dashboard at [https://dashboard.vapi.ai/](https://dashboard.vapi.ai/) and retrieve your API key

2. **Create an Inbound SIP Trunk Credential**
   
   ```bash
   curl -X POST https://api.vapi.ai/credential \
   -H "Content-Type: application/json" \
   -H "Authorization: Bearer your-vapi-private-api-key" \
   -d '{
     "provider": "byo-sip-trunk",
     "name": "PLIVO Inbound Trunk",
     "type": "inbound"
   }'
   ```
   
   Note the `id` (credentialId) from the response for the next step.

3. **Register Your Phone Number**
   
   ```bash
   curl -X POST https://api.vapi.ai/phone-number \
   -H "Content-Type: application/json" \
   -H "Authorization: Bearer your-vapi-private-api-key" \
   -d '{
     "provider": "byo-phone-number",
     "name": "PLIVO SIP Inbound Number",
     "number": "1833684XXXX",
     "numberE164CheckEnabled": false,
     "credentialId": "a2c815b8-03f4-40f5-813c-xxxxxxxxxxxx"
   }'
   ```

4. **Create and Configure a Vapi Assistant**
   
   - Create an assistant following the steps at [https://docs.vapi.ai/quickstart/dashboard#create-an-assistant](https://docs.vapi.ai/quickstart/dashboard#create-an-assistant)
   - In the assistant settings, link it to the phone number you created
   
   Now when someone calls your Plivo number, the call will be routed to your Vapi assistant.


 This is the content for the doc fern/advanced/sip/sip-telnyx.mdx 

 ---
title: Telnyx SIP Integration
subtitle: How to integrate SIP Telnyx to Vapi
slug: advanced/sip/telnyx
---

Integrate your Telnyx SIP trunk with Vapi.ai to enable your AI voice assistants to handle calls efficiently. This guide walks you through the complete setup process for both inbound and outbound calls.

## 1. Retrieve Your Vapi.ai Private Key

- Log in to your Vapi.ai account
- Navigate to **Organization Settings**
- In the **API Keys** section, copy your **Private Key**

## 2. Configure Telnyx for Inbound Calls

To allow Telnyx to forward incoming calls to Vapi.ai:

1. **Create a SIP Trunk**
   - Go to Voice / SIP Trunking / Create
   - Select FQDN
   - Click "Add FQDN"
   - Select A record type
   - Set FQDN to: `sip.vapi.ai`
   - Port should be 5060 by default

2. **Configure Inbound Settings**
   - Navigate to the Inbound tab of your SIP trunk
   - Configure settings as shown:
   <Frame>
     <img src="../../static/images/sip/telynx-inbound.png" />
   </Frame>

3. **Assign Phone Number**
   - Go to the Numbers tab
   - Assign your acquired phone number to the SIP trunk

4. **Configure SIP Invite**
   - Go to Numbers, edit the number you'll be using
   - Navigate to Voice settings
   - Scroll down to find "Translated Number"
   - Set this value to match your Vapi SIP URI

   *This setting modifies the SIP Invite to the Vapi platform so invites are correctly routed to your Vapi SIP URI.*

## 3. Configure Telnyx for Outbound Calls

To allow Vapi.ai to make outbound calls through your Telnyx account:

1. **Set Up Outbound Authentication**
   - Go to Voice / SIP Trunking / Authentication and routing
   - Scroll down to "Outbound calls authentication"
   - Create a new credential for Vapi to use
   <Frame>
     <img src="../../static/images/sip/telynx-outbound-auth.png" />
   </Frame>

2. **Create Outbound Voice Profile**
   - Go to Voice / Outbound Voice Profiles
   - Create a new profile
   - Name it appropriately
   - Configure desired destinations
   - Leave default configuration settings
   - Assign your SIP trunk
   - Complete setup

   Alternatively, go to your SIP trunk / Outbound tab and select your newly created outbound voice profile.

3. **Configure Outbound Settings**
   - Choose the country you'll be making most calls to
   
   *We recommend creating a separate SIP Trunk for each country you aim to be making most calls to.*
   <Frame>
     <img src="../../static/images/sip/telynx-outbound-settings.png" />
   </Frame>

## 4. Add Your Telnyx SIP Credentials to Vapi.ai

Use the Vapi API to create a SIP trunk credential:

```bash
curl -X POST https://api.vapi.ai/credential \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_VAPI_PRIVATE_KEY" \
  -d '{
    "provider": "byo-sip-trunk",
    "name": "Telnyx Trunk",
    "gateways": [
      {
        "ip": "sip.telnyx.com"
      }
    ],
    "outboundAuthenticationPlan": {
      "authUsername": "YOUR_SIP_USERNAME",
      "authPassword": "YOUR_SIP_PASSWORD",
	  "sipRegisterPlan": {
            "realm": "sip.telnyx.com"
        }
    }
  }'
```

Replace `YOUR_VAPI_PRIVATE_KEY`, `YOUR_SIP_USERNAME`, and `YOUR_SIP_PASSWORD` with your actual credentials.

If successful, the response will include an `id` for the created credential, which you'll use in the next step.

## 5. Add Your Phone Number to Vapi.ai

Associate your phone number with the SIP trunk in Vapi.ai:

```bash
curl -X POST https://api.vapi.ai/phone-number \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_VAPI_PRIVATE_KEY" \
  -d '{
    "provider": "byo-phone-number",
    "name": "Telnyx SIP Number",
    "number": "YOUR_PHONE_NUMBER",
    "numberE164CheckEnabled": false,
    "credentialId": "YOUR_CREDENTIAL_ID"
  }'
```

Replace `YOUR_VAPI_PRIVATE_KEY`, `YOUR_PHONE_NUMBER`, and `YOUR_CREDENTIAL_ID` with your actual details.

## 6. Assign Your Voice Assistant to Handle Calls

- In your Vapi.ai dashboard, go to the **Build** section and select **Phone Numbers**
- Click on your **Telnyx Number**
- In the **Inbound Settings** section, assign your voice assistant to handle incoming calls
- In the **Outbound Form** section, assign your voice assistant to handle outgoing calls

## 7. Make Outbound Calls

To initiate outbound calls through your Telnyx SIP trunk:

```bash
curl --location 'https://api.vapi.ai/call/phone' \
  --header 'Authorization: Bearer YOUR_VAPI_PRIVATE_KEY' \
  --header 'Content-Type: application/json' \
  --data '{
    "assistantId": "YOUR_ASSISTANT_ID",
    "customer": {
      "number": "CUSTOMER_PHONE_NUMBER",
      "numberE164CheckEnabled": false
    },
    "phoneNumberId": "YOUR_PHONE_ID"
  }'
```

Replace all placeholder values with your actual information.

By following these steps, your Telnyx SIP trunk will be fully integrated with Vapi.ai, allowing your AI voice assistants to manage calls effectively.


 This is the content for the doc fern/advanced/sip/sip-trunk.mdx 

 ---
title: SIP Trunking Guide for Vapi
subtitle: How to integrate your SIP provider with Vapi
slug: advanced/sip/sip-trunk
---

SIP trunking replaces traditional phone lines with a virtual connection over the internet, allowing your business to make and receive calls via a broadband connection. It connects your internal PBX or VoIP system to a SIP provider, which then routes calls to the Public Switched Telephone Network (PSTN). This setup simplifies your communications infrastructure and often reduces costs.

## 1. Vapi SIP Trunking Options

Vapi supports multiple SIP trunk configurations, including:

- **Telnyx**: Uses SIP gateway domain (e.g., sip.telnyx.com) with IP-based authentication. 
- **Zadarma**: Uses SIP credentials (username/password) with its SIP server (e.g., sip.zadarma.com).
- **Custom "BYO" SIP Trunk**: Allows integration with any SIP provider. You simply provide the SIP gateway address and the necessary authentication details.

<Warning>
Our IP Addresses are: 

- 44.229.228.186/32
- 44.238.177.138/32

However, we generally don't recommend IP-based authentication for SIP trunks as it can lead to routing issues. Since our servers are shared by many customers, if your telephony provider has multiple customers using IP-based authentication, calls may be routed incorrectly. IP-based authentication works reliably only when your SIP provider offers a unique termination URI or a dedicated SIP server for each customer, as is the case with Plivo and Twilio integrations.
</Warning>

## 2. Setup Process Overview

To set up a SIP trunk in Vapi, follow these steps:

### Obtain Provider Details

Gather the SIP server address, authentication credentials (username/password or IP-based), and at least one phone number (DID) from your provider.

### Create a SIP Trunk Credential in Vapi

Use the Vapi API to create a new credential (type: byo-sip-trunk) with your provider's details. This informs Vapi how to connect to your SIP network.

**Example (using Zadarma):**

```bash
curl -X POST "https://api.vapi.ai/credential" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_VAPI_PRIVATE_KEY" \
  -d '{
    "provider": "byo-sip-trunk",
    "name": "Zadarma Trunk",
    "gateways": [{
      "ip": "sip.zadarma.com"
    }],
    "outboundLeadingPlusEnabled": true,
    "outboundAuthenticationPlan": {
      "authUsername": "YOUR_SIP_NUMBER",
      "authPassword": "YOUR_SIP_PASSWORD"
    }
  }'
```

Save the returned Credential ID for later use.

### Associate a Phone Number with the SIP Trunk

Link your external phone number (DID) to the SIP trunk credential in Vapi by creating a Phone Number resource.

**Example:**

```bash
curl -X POST "https://api.vapi.ai/phone-number" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_VAPI_PRIVATE_KEY" \
  -d '{
    "provider": "byo-phone-number",
    "name": "Zadarma Number",
    "number": "15551234567",
    "numberE164CheckEnabled": false,
    "credentialId": "YOUR_CREDENTIAL_ID"
  }'
```

Note the returned Phone Number ID for use in test calls.

### Test Your SIP Trunk

#### Outbound Call Test

Initiate a call through the Vapi dashboard or API to ensure outbound calls are properly routed.

**API Example:**

```json
POST https://api.vapi.ai/call/phone
{
  "assistantId": "YOUR_ASSISTANT_ID",
  "customer": {
    "number": "15557654321",
    "numberE164CheckEnabled": false
  },
  "phoneNumberId": "YOUR_PHONE_NUMBER_ID"
}
```

#### Inbound Call Test

If inbound routing is configured, call your phone number from an external line. Ensure your provider forwards calls to the correct SIP URI (e.g., `{phoneNumber}@sip.vapi.ai` for Zadarma).

#### SIP REFER (Call Transfer)

If you need to transfer a call to another number, you will need to add a SIP Transfer based call forwarding where transfer number will look like this: sip:transfer-number@your-telecom-provider-domain.com

Example: sip:15557654321@sip.zadarma.com

Example tool configuration required for SIP REFER:

```json
{
      "type": "transferCall",
      "destinations": [
        {
          "type": "sip",
          "sipUri": "sip:14039932200@sip.telnyx.com"
        }
      ]
    }
```

<Info>You might need to enable SIP REFER in your SIP provider to allow this.</Info>




 This is the content for the doc fern/advanced/sip/sip-twilio.mdx 

 ---
title: Twilio SIP Integration
subtitle: How to integrate Twilio SIP with Vapi
slug: advanced/sip/twilio
---

<Frame>
  <div class="video-embed-wrapper">
    <iframe
      src="https://www.youtube.com/embed/_wo5wokt3dI?si=72E1azM7tYv6TsBI"
      title='An embedded YouTube video titled "The Ultimate SIP Trunking Guide for AI Voice Agents | Twilio + Vapi"'
      frameborder="0"
      allow="fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      referrerpolicy="strict-origin-when-cross-origin"
    />
  </div>
</Frame>

This guide walks you through setting up both outbound and inbound SIP trunking between Twilio and Vapi. The steps are quite similar for other telephony providers.

## Outbound Calls (Twilio to Vapi)

### Twilio Configuration

1. **Create Elastic SIP Trunk**
   
   Log in to your Twilio account and create a new trunk, assigning it a name, and adjusting the general settings as needed.
   
   ![Twilio SIP Trunk](../../static/images/sip/sip-twilio-trunk.png)

2. **Set Up Termination (Outbound Calls)**
   
   Configure the termination settings. The termination SIP URI is crucial as it will be used in later steps.
   
   ![Termination SIP URI](../../static/images/sip/sip-twilio-termination-uri.png)
   
   To allow your Elastic SIP Trunk to accept outbound requests, you need to whitelist IP addresses:
   
   ![IP Authentication](../../static/images/sip/sip-twilio-ip-authentication.png)
   
   Whitelist Vapi's SIP server static IPs:
   - 44.229.228.186
   - 44.238.177.138
   
   Ensure you whitelist the entire IP range as shown below:
   
   ![IP Whitelist 1](../../static/images/sip/sip-twilio-ip-1.png)
   
   ![IP Whitelist 2](../../static/images/sip/sip-twilio-ip-2.png)

3. **Purchase or Move Numbers to Elastic SIP Trunk**
   
   After creating the Elastic SIP trunk, purchase new numbers or move existing numbers to this trunk.
   
   ![Number Attachment](../../static/images/sip/sip-twilio-number-attach.png)

### Vapi Configuration

1. **Retrieve Your Vapi API Key**
   
   Log in to your Vapi.ai account and retrieve your API key from the Organization Settings.

2. **Create a SIP Trunk Credential**
   
   Use the following API call to create a SIP trunk credential, replacing the gateway IP with your Twilio Termination SIP URI:

   ```bash
   curl -X POST https://api.vapi.ai/credential \
   -H "Content-Type: application/json" \
   -H "Authorization: Bearer YOUR_VAPI_API_KEY" \
   -d '{
     "provider": "byo-sip-trunk",
     "name": "Twilio Trunk",
     "gateways": [
       {
         "ip": "YOUR_TWILIO_GATEWAY_ID"
       }
     ],
     "outboundLeadingPlusEnabled": true
   }'
   ```
   
   Note the `id` (credentialId) from the response for the next step.

3. **Register Your Phone Number**
   
   Associate your Twilio number with the SIP trunk:

   ```bash
   curl -X POST https://api.vapi.ai/phone-number \
   -H "Content-Type: application/json" \
   -H "Authorization: Bearer YOUR_VAPI_API_KEY" \
   -d '{
     "provider": "byo-phone-number",
     "name": "Twilio SIP Number",
     "number": "YOUR_SIP_PHONE_NUMBER",
     "numberE164CheckEnabled": false,
     "credentialId": "YOUR_CREDENTIAL_ID"
   }'
   ```
   
   Note the phone number ID from the response for making calls.

4. **Make Outbound Calls**
   
   You can make outbound calls in two ways:
   
   **Using the Vapi Dashboard:**
   
   The phone number will appear in your dashboard. Select your assistant and enter the destination number you want to call.
   
   **Using the API:**
   
   ```bash
   curl --location 'https://api.vapi.ai/call/phone' \
   --header 'Authorization: Bearer YOUR_VAPI_API_KEY' \
   --header 'Content-Type: application/json' \
   --data '{
     "assistantId": "YOUR_ASSISTANT_ID",
     "customer": {
       "number": "DESTINATION_PHONE_NUMBER",
       "numberE164CheckEnabled": false
     },
     "phoneNumberId": "YOUR_PHONE_NUMBER_ID"
   }'
   ```

## Inbound Calls (Vapi to Twilio)

### Twilio Configuration

1. **Set Up Origination (Inbound Calls)**
   
   Navigate to the Origination section in your Twilio SIP Trunk settings.
   
   ![Origination Settings](../../static/images/sip/sip-twilio-origination.png)
   
   Add your Vapi SIP URI in the following format: `sip:YOUR_PHONE_NUMBER@sip.vapi.ai`, where "YOUR_PHONE_NUMBER" is your chosen SIP number that you will attach to this trunk.
   
   ![Origination Creation](../../static/images/sip/sip-twilio-origination-creation.png)

### Vapi Configuration

1. **Create and Configure a Vapi Assistant**
   
   - Create an assistant following the steps at [https://docs.vapi.ai/quickstart/dashboard#create-an-assistant](https://docs.vapi.ai/quickstart/dashboard#create-an-assistant)
   - In the assistant settings, link it to the phone number you created
   
   Now when someone calls your Twilio number, the call will be routed to your Vapi assistant.


 This is the content for the doc fern/advanced/sip/sip-zadarma.mdx 

 ---
title: Zadarma SIP Integration
subtitle: How to integrate SIP Zadarma to Vapi
slug: advanced/sip/zadarma
---


Integrate your Zadarma SIP trunk with Vapi.ai to enable your AI voice assistants to handle calls efficiently. Follow the steps below to set up this integration:

## 1. Retrieve Your Vapi.ai Private Key

- Log in to your Vapi.ai account.
- Navigate to **Organization Settings**.
- In the **API Keys** section, copy your **Private Key**.

## 2. Add Your Zadarma SIP Credentials to Vapi.ai

You'll need to send a `curl` request to Vapi.ai's API to add your SIP credentials:

- **Private Key**: Your Vapi.ai private key.
- **Trunk Name**: A name for your SIP trunk (e.g., "Zadarma Trunk").
- **Server Address**: The server address provided by Zadarma (e.g., "sip.zadarma.com").
- **SIP Number**: Your Zadarma SIP number.
- **SIP Password**: The password for your Zadarma SIP number.

Here's the `curl` command to execute:

```bash
curl -L 'https://api.vapi.ai/credential' \\
-H 'Content-Type: application/json' \\
-H 'Authorization: Bearer YOUR_PRIVATE_KEY' \\
-d '{
  "provider": "byo-sip-trunk",
  "name": "Zadarma Trunk",
  "gateways": [
    { "ip": "sip.zadarma.com" }
  ],
  "outboundLeadingPlusEnabled": true,
  "outboundAuthenticationPlan": {
    "authUsername": "YOUR_SIP_NUMBER",
    "authPassword": "YOUR_SIP_PASSWORD"
  }
}'
```
Replace `YOUR_PRIVATE_KEY`, `YOUR_SIP_NUMBER`, and `YOUR_SIP_PASSWORD` with your actual credentials.

If successful, the response will include an `id` for the created credential, which you'll use in the next step.

## 3. Add Your Virtual Number to Vapi.ai

Next, associate your virtual number with the SIP trunk in Vapi.ai:

- **Private Key**: Your Vapi.ai private key.
- **Number Name**: A name for your virtual number (e.g., "Zadarma Number").
- **Virtual Number**: Your Zadarma virtual number in international format (e.g., "15551111111").
- **Credential ID**: The `id` from the previous step.

Use the following `curl` command:

```bash
curl -L 'https://api.vapi.ai/phone-number' \\
-H 'Content-Type: application/json' \\
-H 'Authorization: Bearer YOUR_PRIVATE_KEY' \\
-d '{
  "provider": "byo-phone-number",
  "name": "Zadarma Number",
  "number": "YOUR_VIRTUAL_NUMBER",
  "numberE164CheckEnabled": false,
  "credentialId": "YOUR_CREDENTIAL_ID"
}'
```

Replace `YOUR_PRIVATE_KEY`, `YOUR_VIRTUAL_NUMBER`, and `YOUR_CREDENTIAL_ID` with your actual details.

## 4. Assign Your Voice Assistant to Handle Calls

- In your Vapi.ai dashboard, go to the **Build** section and select **Phone Numbers**.
- Click on your **Zadarma Number**.
- In the **Inbound Settings** section, assign your voice assistant to handle incoming calls.
- In the **Outbound Form** section, assign your voice assistant to handle outgoing calls.

## 5. Configure Incoming Call Reception in Zadarma

To forward incoming calls from your Zadarma virtual number to Vapi.ai:

- Log in to your Zadarma account.
- Navigate to **Settings** → **Virtual phone numbers**.
- Click the ⚙ (gear) icon next to your number.
- Open the **External server** tab.
- Enable **External server (SIP URI)**.
- Enter the address: `YOUR_VIRTUAL_NUMBER@sip.vapi.ai` (replace `YOUR_VIRTUAL_NUMBER` with your number in international format).
- Click **Save**.

By following these steps, your Zadarma SIP trunk will be integrated with Vapi.ai, allowing your AI voice assistants to manage calls effectively.


 This is the content for the doc fern/advanced/sip/sip.mdx 

 ---
title: SIP Introduction
subtitle: You can make SIP calls to Vapi Assistants.
slug: advanced/sip
---

<Steps>
<Step title="Create an Assistant">

We'll create an assistant with `POST /assistant` endpoint. This is no different than creating an assistant for other transports.

```json
{
	"name": "My SIP Assistant",
	"firstMessage": "Hello {{first_name}}, you've reached me over SIP."
}

```
</Step>

<Step title="Create a SIP Phone Number">

We'll create a SIP phone number with `POST /phone-number` endpoint. 

```json
{
	"provider": "vapi",
	"sipUri": "sip:your_unique_user_name@sip.vapi.ai",
	"assistantId": "your_assistant_id"
}

```

`sipUri` is the SIP URI of the phone number. It must be in the format `sip:username@sip.vapi.ai`. You are free to choose any username you like.

</Step>



<Step title="Start a SIP call">

You can use any SIP softphone to test the Assistant. Examples include [Zoiper](https://www.zoiper.com/) or [Linphone](https://www.linphone.org/). 

You just need to dial `sip:your_unique_user_name@sip.vapi.ai` and the Assistant will answer your call. 

There is no authentication or SIP registration required.

</Step>

<Step title="Send SIP Headers to Fill Template Variables">


To fill your template variables, you can send custom SIP headers. 

For example, to fill the `first_name` variable, you can send a SIP header `x-first_name: John`. 

The header name is case insensitive. So, `X-First_Name`, `x-first_name`, and `X-FIRST_NAME` are all the same.

</Step>

<Step title="Use a Custom Assistant For Each Call">


You can use a custom assistant for SIP calls same as phone calls.

Set the `assistantId` to `null` and the `serverUrl` to the URL of your server which will respond to the `assistant-request`.

`PATCH /phone-number/:id`
```json
{
	"assistantId": null,
	"serverUrl": "https://your_server_url"
}
```

Now, every time you make a call to this phone number, the server will receive a `assistant-request` event.

</Step>

</Steps>


 This is the content for the doc fern/api-reference/openapi.mdx 

 ---
title: OpenAPI
slug: api-reference/openapi
---


<Check>
  Our OpenAPI is hosted at
  [https://api.vapi.ai/api-json](https://api.vapi.ai/api-json)
</Check>


 This is the content for the doc fern/api-reference/swagger.mdx 

 ---
title: Swagger
slug: api-reference/swagger
---


<Check>
  Our Swagger is hosted at [https://api.vapi.ai/api](https://api.vapi.ai/api)
</Check>


 This is the content for the doc fern/assistants.mdx 

 ---
title: Introduction to Assistants
subtitle: The core building-block of voice agents on Vapi.
slug: assistants
---

[**Assistant**](/api-reference/assistants/create) is a fancy word for an AI configuration that can be used across phone calls and Vapi clients. Your voice assistant can augment your customer support and experience for call centers, business websites, mobile apps, and much more.

<EndpointResponseSnippet endpoint='POST /assistant' />

## Core Components

There are three core components that make up an assistant:

- **Transcriber**: Converts spoken audio into text
- **Model**: The AI model that processes the text and generates responses
- **Voice**: The voice that speaks the AI's responses

These components can be configured, mixed, and matched for your specific use case.

<Info>
  View all configurable properties in the [API Reference](/api-reference/assistants/create-assistant).
</Info>

## Key Features

### Dynamic Variables
Personalize your assistant's responses using variables that can be customized for each call. This allows you to:
- Insert dynamic content like dates, times, and user information
- Customize greetings and responses
- Maintain context across conversations

### Call Analysis 
Get detailed insights into each conversation through:
- Call summaries
- Structured data extraction
- Success evaluation metrics
- Custom analysis rubrics

### Persistence Options
Choose between:
- **Persistent Assistants**: Reusable configurations stored via the `/assistant` endpoint
- **Temporary Assistants**: One-time configurations specified when starting a call

## Prompting Best Practices

Effective prompt engineering is crucial for creating successful voice AI agents. Learn how to:
- Structure prompts for voice interactions
- Add personality and natural speech patterns
- Handle errors gracefully
- Improve response quality

<Card
  title="Voice AI Prompting Guide"
  icon="book"
  href="/prompting-guide"
>
  Learn best practices for engineering voice AI prompts
</Card>

## Advanced Concepts

<CardGroup cols={2}>
  <Card title="Provider Keys" icon="key" href="customization/provider-keys">
    Add your API keys for other providers
  </Card>
  <Card title="Custom LLM URL" icon="brain" href="customization/custom-llm/fine-tuned-openai-models">
    Plug in your own LLM
  </Card>
  <Card title="Call Functions" icon="square-phone" href="/assistants/function-calling">
    Forward and hang up with function calls
  </Card>
  <Card
    title="Persistent / Temporary Assistants"
    href="/assistants/persistent-assistants"
    icon="database"
  >
    Which setup is best for you?
  </Card>
</CardGroup>


 This is the content for the doc fern/assistants/assistant-hooks.mdx 

 ---
title: Assistant Hooks
slug: assistants/assistant-hooks
---

# Assistant Hooks

Assistant hooks allow you to configure actions that will be performed when specific events occur during a call. Currently, hooks support the `call.ending` event, which triggers when a call is ending.

## Usage

Hooks are defined in the `hooks` array of an assistant. Each hook consists of:

- `on`: The event that triggers the hook (currently only supports `call.ending`)
- `do`: The actions to perform when the hook triggers (currently only supports `transfer`)
- `filters`: Optional conditions that must be met for the hook to trigger

The `call.endedReason` field in filters can be set to any of the [call ended reasons](https://docs.vapi.ai/api-reference/calls/get#response.body.endedReason). The transfer destination type follows the same schema as the [transfer call tool destinations](https://docs.vapi.ai/api-reference/tools/create#request.body.transferCall.destinations).

Note: Using `"oneOf": ["pipeline-error"]` acts as a catch-all filter that matches any pipeline-related error reason. This is useful when you want to handle all types of pipeline failures with the same transfer action.

## Example: Transfer on Pipeline Error

This example shows how to transfer a call to a fallback number when a pipeline error occurs. The hook will trigger when the call is ending due to a pipeline error, and transfer the call to a specified phone number:

```bash
curl -X PATCH "https://api.vapi.ai/assistant/<id>" \
     -H "Authorization: Bearer <auth>" \
     -H "Content-Type: application/json" \
     -d '{
  "hooks": [{
    "on": "call.ending",
    "filters": [{
      "type": "oneOf",
      "key": "call.endedReason",
      "oneOf": ["pipeline-error"]
    }],
    "do": [{
      "type": "transfer",
      "destination": {
        "type": "number",
        "number": "+1234567890",
        "callerId": "+1987654321"
      }
    }]
  }]
}'
```

You can also transfer to a SIP destination:

```bash
curl -X PATCH "https://api.vapi.ai/assistant/<id>" \
     -H "Authorization: Bearer <auth>" \
     -H "Content-Type: application/json" \
     -d '{
  "hooks": [{
    "on": "call.ending",
    "filters": [{
      "type": "oneOf",
      "key": "call.endedReason",
      "oneOf": ["pipeline-error"]
    }],
    "do": [{
      "type": "transfer",
      "destination": {
        "type": "sip",
        "sipUri": "sip:user@domain.com"
      }
    }]
  }]
}'
```

Common use cases for hooks include:
- Transferring to a human agent on errors
- Routing to a fallback system if the assistant fails
- Ensuring calls are handled gracefully in edge cases

 This is the content for the doc fern/assistants/background-messages.mdx 

 ---
title: Background Messaging
subtitle: >-
  Vapi SDK lets you silently update the chat history through efficient text
  message integration. This is particularly useful for background tasks or
  discreetly logging user interactions.
slug: assistants/background-messages
---


## Scenario Overview

As a developer you may run into scenarios where a user action, such as pressing a button, needs to be logged in the chat history without overt user involvement. This could be crucial for maintaining conversation context or system logging purposes.

<Steps>
  <Step title="Add a Button to Trigger the Message">
    Add a button to your interface with an `onClick` event handler that will call a function to send the system message:
    ```html
    <button id="log-action" onClick="logUserAction()">Log Action</button>
    ```
  </Step>

  <Step title="Log the Action as a System Message">
    When the button is clicked, the `logUserAction` function will silently insert a system message into the chat history:
    ```js
    function logUserAction() {
      // Function to log the user action
      vapi.send({
        type: "add-message",
        message: {
          role: "system",
          content: "The user has pressed the button, say peanuts",
        },
      });
    }
    ```
    - `vapi.send`: The primary function to interact with your assistant, handling various requests or commands.
    - `type: "add-message"`: Specifies the command to add a new message.
    - `message`: This is the actual message that you want to add to the message history.
      - `role`: "system" Designates the message origin as 'system', ensuring the addition is unobtrusive. Other possible values of role are 'user' | 'assistant' | 'tool' | 'function'
      - `content`: The actual message text to be added.
  </Step>
</Steps>

<Card title="Practical Use Cases">
    - Silent logging of user activities.
    - Contextual updates in conversations triggered by background processes.
    - Non-intrusive user experience enhancements through additional information provision.
</Card>


 This is the content for the doc fern/assistants/call-analysis.mdx 

 ---
title: Call Analysis
subtitle: At the end of the call, you can summarize and evaluate how it went.
slug: assistants/call-analysis
---


The Call Analysis feature allows you to summarize and evaluate calls, providing valuable insights into their effectiveness. This feature uses a combination of prompts and schemas to generate structured data and success evaluations based on the call's content. The underlying models driving our call analysis pipeline are the latest version of Anthropic's Claude Sonnet and in case of failure OpenAI's GPT 4o.

You can customize the below in the assistant's `assistant.analysisPlan`.

## Summary Prompt

The summary prompt is used to create a concise summary of the call. This summary is stored in `call.analysis.summary`.

### Default Summary Prompt

The default summary prompt is:

```text
You are an expert note-taker. You will be given a transcript of a call. Summarize the call in 2-3 sentences, if applicable.
```

### Customizing the Summary Prompt

You can customize the summary prompt by setting the `summaryPrompt` property in the API or SDK:

```json
{
  "summaryPrompt": "Custom summary prompt text"
}
```

To disable the summary prompt, set it to an empty string `""` or `"off"`:

```json
{
  "summaryPrompt": ""
}
```

## Structured Data Prompt

The structured data prompt extracts specific pieces of data from the call. This data is stored in `call.analysis.structuredData`.

### Default Structured Data Prompt

The default structured data prompt is:

```text
You are an expert data extractor. You will be given a transcript of a call. Extract structured data per the JSON Schema.
```

### Customizing the Structured Data Prompt

You can set a custom structured data prompt using the `structuredDataPrompt` property:

```json
{
  "structuredDataPrompt": "Custom structured data prompt text"
}
```

## Structured Data Schema

The structured data schema enforces the format of the extracted data. It is defined using JSON Schema standards.

### Customizing the Structured Data Schema

You can set a custom structured data schema using the `structuredDataSchema` property:

```json
{
  "structuredDataSchema": {
    "type": "object",
    "properties": {
      "field1": { "type": "string" },
      "field2": { "type": "number" }
    },
    "required": ["field1", "field2"]
  }
}
```

## Success Evaluation Prompt

The success evaluation prompt is used to determine if the call was successful. This evaluation is stored in `call.analysis.successEvaluation`.

### Default Success Evaluation Prompt

The default success evaluation prompt is:

```text
You are an expert call evaluator. You will be given a transcript of a call and the system prompt of the AI participant. Determine if the call was successful based on the objectives inferred from the system prompt.
```

### Customizing the Success Evaluation Prompt

You can set a custom success evaluation prompt using the `successEvaluationPrompt` property:

```json
{
  "successEvaluationPrompt": "Custom success evaluation prompt text"
}
```

To disable the success evaluation prompt, set it to an empty string `""` or `"off"`:

```json
{
  "successEvaluationPrompt": ""
}
```

## Success Evaluation Rubric

The success evaluation rubric defines the criteria used to evaluate the call's success. The available rubrics are:

- `NumericScale`: A scale of 1 to 10.
- `DescriptiveScale`: A scale of Excellent, Good, Fair, Poor.
- `Checklist`: A checklist of criteria and their status.
- `Matrix`: A grid that evaluates multiple criteria across different performance levels.
- `PercentageScale`: A scale of 0% to 100%.
- `LikertScale`: A scale of Strongly Agree, Agree, Neutral, Disagree, Strongly Disagree.
- `AutomaticRubric`: Automatically break down evaluation into several criteria, each with its own score.
- `PassFail`: A simple 'true' if the call passed, 'false' if not.

### Customizing the Success Evaluation Rubric

You can set a custom success evaluation rubric using the `successEvaluationRubric` property:

```json
{
  "successEvaluationRubric": "NumericScale"
}
```

## Combining Prompts and Rubrics

You can use prompts and rubrics in combination to create detailed instructions for the call analysis:

```json
{
  "successEvaluationPrompt": "Evaluate the call based on these criteria:...",
  "successEvaluationRubric": "Checklist"
}
```

By customizing these properties, you can tailor the call analysis to meet your specific needs and gain valuable insights from your calls.


 This is the content for the doc fern/assistants/call-recording.mdx 

 ---
title: Call Recording
subtitle: Record calls and store them in Vapi or your own storage.
slug: call-recording
---

The Call Recording feature allows you to capture and store full recordings of phone calls for analysis. By default, Vapi stores a complete recording of every call, providing both mono and stereo audio. The stereo option separates human and assistant audio into two distinct channels, offering a clearer analysis of the conversation.

You can customize this behavior in the assistant's [`assistant.artifactPlan`](https://docs.vapi.ai/api-reference/assistants/create#request.body.artifactPlan).


## Supported Formats

Vapi supports multiple audio formats for call recordings:
- `wav;l16`: 16-bit linear PCM WAV format, providing high-quality uncompressed audio in mono
- `mp3`: MP3 compressed audio format, offering good quality with smaller file sizes

You can specify your preferred format using the [`assistant.artifactPlan.recordingFormat`](https://docs.vapi.ai/api-reference/assistants/create#request.body.artifactPlan.recordingFormat) property. If not specified, recordings will default to `wav;l16`.

<Note>
At this time, you can only specify one format.
</Note>

## Custom Storage bucket

Vapi supports uploading recordings to your own storage buckets. See [Integrations -> Cloud](https://docs.vapi.ai/providers/cloud/s3) for more information on available storage options.

## Upload Path

When uploading recordings to your custom storage bucket, you can specify the upload path using the `assistant.artifactPlan.recordingPath` property. If not specified, recordings will default to the root of the bucket.

Usage:
- If you want to upload the recording to a specific path, set this to the path. Example: `/my-assistant-recordings`.
- If you want to upload the recording to the root of the bucket, set this to `/`.

## Turn On/Off Call Recording

You can turn on/off call recording by setting the [`assistant.artifactPlan.recordingEnabled`](https://docs.vapi.ai/api-reference/assistants/create#request.body.artifactPlan.recordingEnabled) property to `true` or `false`. If not specified, recordings will default to `true`.

<Note>
If [HIPAA](https://docs.vapi.ai/security-and-privacy/hipaa) mode is enabled, Vapi will only store recordings if you have defined a custom storage bucket. Make sure to set credentials in the Provider Credentials page in the Dashboard.
</Note>

## Turn On/Off Video Recording (only for webCall)

You can turn on/off video recording by setting the [`assistant.artifactPlan.videoRecordingEnabled`](https://docs.vapi.ai/api-reference/assistants/create#request.body.artifactPlan.videoRecordingEnabled) property to `true` or `false`. If not specified, video recording will default to `false`.



 This is the content for the doc fern/assistants/dynamic-variables.mdx 

 ---
title: Dynamic Variables
subtitle: >-
  Vapi makes it easy to personalize an assistant's messages and prompts using
  variables, allowing each call to be customized.
slug: assistants/dynamic-variables
---

Prompts, messages, and other assistant properties can be dynamically set when starting a call based on templates.
These templates are defined using double curly braces `{{variableName}}`.
This is useful when you want to customize the assistant for a specific call.

For example, you could set the assistant's first message to "Hello, `{{name}}`!" and then set `name` to `John` when starting the call
by passing `assistantOverrides` with `variableValues` to the API or SDK:

```json
{
  "variableValues": {
    "name": "John"
  }
}
```

## Utilizing Dynamic Variables in Phone Calls

To leverage dynamic variables during phone calls, follow these steps:

1.  **Prepare Your Request:** Construct a JSON payload containing the following key-value pairs:

    *   `assistantId`: Replace `"your-assistant-id"` with the actual ID of your assistant.
    *   `assistantOverride`: This object is used to customize your assistant's behavior.
        *   `variableValues`: An object containing the dynamic variables you want to use, in the format `{ "variableName": "variableValue" }`. For example, `{ "name": "John" }`.
    *   `customer`: An object representing the call recipient.
        *   `number`: Replace `"+1xxxxxxxxxx"` with the phone number you wish to call (in E.164 format).
    *   `phoneNumberId`: Replace `"your-phone-id"` with the ID of your registered phone number. You can get it from the [Phone number](https://dashboard.vapi.ai/phone-numbers) in the dashboard.

2.  **Send the Request:** Dispatch the JSON payload to the `/call/phone` endpoint using your preferred method (e.g., HTTP POST request).

```json
{
  "assistantId": "your-assistant-id",
  "assistantOverrides": {
    "variableValues": {
      "name": "John"
    }
  },
  "customer": {
    "number": "+1xxxxxxxxxx"
  },
  "phoneNumberId": "your-phone-id"
}
```

**Note:** You will need to add the `{{variableName}}` in this format in all your prompts, whether it is the first message or anywhere else you want to use it.


## Default Variables

By default, the following variables are automatically filled based on the current (UTC) time,
meaning that you don't need to set them manually in `variableValues`:

| Variable                | Description                       | Example                   |
| ----------------------- | --------------------------------- | ------------------------- |
| `{{now}}`               | Current date and time (UTC)       | Jan 1, 2024 12:00 PM      |
| `{{date}}`              | Current date (UTC)                | Jan 1, 2024               |
| `{{time}}`              | Current time (UTC)                | 12:00 PM                  |
| `{{month}}`             | Current month (UTC)               | January                   |
| `{{day}}`               | Current day of month (UTC)        | 1                         |
| `{{year}}`              | Current year (UTC)                | 2024                      |
| `{{customer.number}}`   | Customer's phone number           | +1xxxxxxxxxx              |
| `{{customer.X}}`        | Any other customer property       |                           |



## Advanced Date and Time Usage

We use [LiquidJS](https://liquidjs.com/) for dynamic variables. You can use the `date` filter to format the date and time in the timezone you want.

```liquid
{{"now" | date: "%b %d, %Y, %I:%M %p", "America/New_York"}}
```

This should return the current date and time in New York.

 This is the content for the doc fern/assistants/persistent-assistants.mdx 

 ---
title: Persistent Assistants
subtitle: Should I use persistent assistants?
slug: assistants/persistent-assistants
---


You might be wondering whether or not you should create an assistant using the `/assistant` endpoint with its `assistantId`. Or, can you just specify the assistant configuration when starting a call?

The `/assistant` endpoint is there for convenience to save you creating your own assistants table.

<Accordion title="Use cases">
- You won't be adding more assistant properties on top of ours.
- You want to use the same assistant across multiple calls.
</Accordion>

Otherwise, you can just specify the assistant configuration when starting a call.


 This is the content for the doc fern/assistants/voice-formatting-plan.mdx 

 ---
title: Voice Formatting Plan
subtitle: >-
  Learn what voice formatting plans are and how to format voice input dynamically for clearer and more natural text-to-speech interactions.
slug: assistants/voice-formatting-plan
---

## What is Voice Input Formatted?

When interacting with voice assistants, you might notice terms like `Voice Input Formatted` in call logs or system outputs. This article explains what this means, how it works, and why it's important for delivering clear and natural voice interactions.

Voice Input Formatted is a function that takes raw text from a language model (LLM) and cleans it up so text-to-speech (TTS) provider can read it more naturally. It’s **on by default** in your assistant’s voice provider settings, because it helps turn things like:

- `$42.50` → `forty two dollars and fifty cents`
- `ST` → `STREET`,
- or phone numbers → spaced digits (“1 2 3 4 5 6 7 8 9 0”).

If you prefer the raw, unchanged text, you can **turn off** these transformations, which we’ll show you later.

### Log Example

![Screenshot 2025-01-21 at 10.23.19.png](https://img.notionusercontent.com/s3/prod-files-secure%2Ffdafdda2-774c-49e6-8896-a352ff4d44f3%2Ff603f2bd-36cf-4085-a3bc-f76c89a1ef75%2FScreenshot_2025-01-21_at_10.23.19.png/size/w=2000?exp=1737581744&sig=yoEEQF-BcTTgEVBNdcZh9MWHye2moRsbUcxGPjATNX8)

## 1. Step-by-Step Transformations

When `Voice Input Formatted` runs, it calls a bunch of helper functions in a row. Each one focuses on a different kind of text pattern. The entire process happens in this order:

1. **removeAngleBracketContent**
2. **removeMarkdownSymbols**
3. **removePhrasesInAsterisks**
4. **replaceNewLinesWithPeriods**
5. **replaceColonsWithPeriods**
6. **formatAcronyms**
7. **formatDollarAmounts**
8. **formatEmails**
9. **formatDates**
10. **formatTimes**
11. **formatDistances, formatUnits, formatPercentages, formatPhoneNumbers**
12. **formatNumbers**
13. **Applying Replacements**

We’ll walk you through them using a **shorter example** than before.

### 1.1 Our Simpler Example Input

```
Hello <tag> world
**Wanted** to say *hi*
We have NASA and .NET here,
call me at 123-456-7890,
price: $42.50
and the date is 2023 05 10
and time is 14:00
Distance is 5km
We might see 9999
the address is 320 ST 21 RD
my email is JOHN.DOE@example.COM

```

### 1.2 removeAngleBracketContent

- **What it does**: Removes `<anything>` unless it’s `<break>`, `<spell>`, or double angle brackets `<< >>`.
- **Example effect**: `<tag>` gets removed.

**Result so far**:

```
Hello  world
**Wanted** to say *hi*
We have NASA and .NET here,
call me at 123-456-7890,
price: $42.50
and the date is 2023 05 10
and time is 14:00
Distance is 5km
We might see 9999
the address is 320 ST 21 RD
my email is JOHN.DOE@example.COM

```

### 1.3 removeMarkdownSymbols

- **What it does**: Removes `_`, ```, or `~`. Some versions also remove double asterisks, but that might happen in a later step (next function).

In this example, there’s `**Wanted**`, which _might_ remain if we strictly only remove `_`, backticks, and tildes. If the code does remove `**` as well, it’ll vanish here or in the next step. Let’s assume it doesn’t remove them in this step.

**Result**: _No real change if the code only targets `_` , ```, and  `~`.\_

```
Hello  world
**Wanted** to say *hi*
...

```

### 1.4 removePhrasesInAsterisks

- **What it does**: Looks for `some text*` or `*some text**` and cuts it out.

In our text, we have `**Wanted**` and `*hi*`. Both get removed if the function is broad enough to remove single and double-asterisk blocks.

**Result**:

```
Hello  world
 to say
We have NASA and .NET here,
call me at 123-456-7890,
price: $42.50
and the date is 2023 05 10
and time is 14:00
Distance is 5km
We might see 9999
the address is 320 ST 21 RD
my email is JOHN.DOE@example.COM

```

### 1.5 replaceNewLinesWithPeriods

- **What it does**: Turns line breaks into `.` or `.` and merges repeated periods.

Let’s say the above text has line breaks. After this step, it’s more of a single line (or fewer lines), each newline replaced by a period.

**Result** (roughly):

```
Hello  world .  to say . We have NASA and .NET here, call me at 123-456-7890, price: $42.50 and the date is 2023 05 10 and time is 14:00 Distance is 5km We might see 9999 the address is 320 ST 21 RD my email is JOHN.DOE@example.COM

```

### 1.6 replaceColonsWithPeriods

- **What it does**: `:` → `.`

Our text has `price: $42.50`. That becomes `price. $42.50`.

**Result**:

```
Hello  world .  to say . We have NASA and .NET here, call me at 123-456-7890, price. $42.50 ...

```

### 1.7 formatAcronyms

- **What it does**:
  - If something is in a known “to-lower” list (like `NASA`, `.NET`), it becomes lowercase (`nasa`, `.net`).
  - If it’s all-caps but not recognized, it might get spaced letters. If it has vowels, it’s left alone.

In the example:

- `NASA` → `nasa`
- `.NET` → `.net`

### 1.8 formatDollarAmounts

- **What it does**: `$42.50` → “forty two dollars and fifty cents.”

### 1.9 formatEmails

- **What it does**: Replaces `@` with “ at ” and `.` with “ dot ” in emails.
- `JOHN.DOE@example.COM` → `JOHN dot DOE at example dot COM`

### 1.10 formatDates

- **What it does**: `YYYY MM DD` → e.g. “Wednesday, May 10, 2023” (if valid).
- `2023 05 10` become “Wednesday, May 10, 2023” (day name depends on how the code calculates it).

### 1.11 formatTimes

- **What it does**: `14:00` → `14` (since minutes are “00,” it remove them).
- If it was `14:30`, it might become `14 30`.

### 1.12 formatDistances, formatUnits, formatPercentages, formatPhoneNumbers

- **Distances**: `5km` → “5 kilometers.”
- **Units**: e.g. `43 lb` → “forty three pounds.”
- **Percentages**: `50%` → “50 percent.”
- **PhoneNumbers**: `123-456-7890` → `1 2 3 4 5 6 7 8 9 0`.

### 1.13 formatNumbers

- **What it does**:
  - Skips year-like numbers if they’re below current year(2025).
  - For large numbers above a cutoff (e.g. 1000 or 5000), it reads as digits.
  - Negative numbers: `9` → “minus nine.”
  - Decimals: `2.5` → “two point five.”

In our case, `9999` might be big enough to become spelled out (nine thousand nine hundred ninety nine) or digits spaced out, depending on the cutoff.

`2023` used with `05 10` might get turned into a date, so it’s handled by the date logic, not the plain number logic.

### 1.14 Applying Replacements (street-suffix expansions)

- **Runs last**. If you have user-defined replacements like `\bST\b` → `STREET`, `\bRD\b` → `ROAD`, it changes them after all the other steps.
- So `320 ST 21 RD` → `320 STREET 21 ROAD`.

**End Result**: A single line of text with all the helpful expansions and transformations done.

## 2. Formatting Plan: Customization Options

The **Formatting Plan** governs how Voice Input Formatted works. Here are the main settings you can customize:

### 2.1 Enabled

Determines whether the formatting is applied.

- **Default**: `true`
- To disable: Set `voice.chunkPlan.formatPlan.enabled = false`.

### 2.2 Number-to-Digits Cutoff

This decides when numbers are read as digits instead of words.

- **Default**: `2025` (current year).
- The code generally **doesn’t** convert numbers below the current year (like `2025`) into spelled-out words, so it stays as digits if it’s obviously a year.
- If a number is bigger than the cutoff (`numberToDigitsCutoff`), it reads digits out loud.
- Negative numbers become “minus,” decimals get “point,” etc.
- Example: With a cutoff of `2025`, numbers like `12345` will remain digits.
- To ensure larger numbers are spelled out, set the cutoff higher, like `300000`. For example:
  - `30003` → “thirty thousand and three” (with a cutoff of `300000`).

### 2.3 Replacements

Allows exact or regex-based substitutions in text.

- **Example 1**: Replace `hello` with `hi`:`{ type: 'exact', key: 'hello', value: 'hi' }`.
- **Example 2**: Replace words matching a pattern:`{ type: 'regex', regex: '\\\\b[a-zA-Z]{5}\\\\b', value: 'hi' }`.

### Note

Currently, only **replacements** and **number-to-digits cutoff** are exposed for customization. Other options, such as toggling acronym replacement, are not exposed to be toggled.

## 3. How to Turn It Off

By default, the entire pipeline is **on** because it helps TTS read better. To **turn it off**, set:

```
voice.chunkPlan.enabled = false;
// or
voice.chunkPlan.formatPlan.enabled = false;
```

Any of those flags being `false` means we **skip** calling `Voice Input Formatted`.

## 4. Conclusion

- `Voice Input Formatted` orchestrates a chain of mini-functions that together fix punctuation, expand abbreviations, and make text more readable out loud.
- You can keep it **on** for better TTS results or **off** if you need the raw LLM output.
- The final transformations, especially the user-supplied replacements (like street expansions), happen **last**, so keep that in mind it rely on other expansions earlier.


 This is the content for the doc fern/blocks.mdx 

 ---
title: Introduction to Blocks
subtitle: Breaking down bot conversations into smaller, more manageable prompts
slug: blocks
---

<Warning>
  **Blocks** is being deprecated in favor of [Workflows](/workflows). We recommend using Workflows for all new development as it provides a more powerful and flexible way to structure conversational AI. We're working on migration tools to help transition existing Blocks implementations to Workflows.
</Warning>

We're currently running a beta for [**Blocks**](/api-reference/blocks/create), an upcoming feature from [Vapi.ai](http://vapi.ai/) aimed at improving bot conversations. The problem we've noticed is that single LLM prompts are prone to hallucinations, unreliable tool calls, and can’t handle many-step complex instructions.

**By breaking the conversation into smaller, more manageable prompts**, we can guarantee the bot will do this, then that, or if this happens, then that happens. It’s like having a checklist for conversations — less room for error, more room for getting things right.


Here’s an example: For food ordering, this is what a prompt would look like.


<Accordion title="Without Blocks">
Example Prompt

```jsx
[Identity]
You are a friendly and efficient assistant for a food truck that serves burgers, fries, and drinks.

[Task]
1. Greet the customer warmly and inquire about their main order.
2. Offer suggestions for the main order if needed.
3. If they choose a burger, suggest upgrading to a combo with fries and a drink, offering clear options (e.g., regular or special fries, different drink choices).
4. Confirm the entire order to ensure accuracy.
5. Suggest any additional items like desserts or sauces.
6. Thank the customer and let them know when their order will be ready.
```

</Accordion>

<Accordion title="With Blocks">
  <Frame>
    <img src="./static/images/blocks/food-order-steps.png" />
  </Frame>
</Accordion>



There are three core types of Blocks: [Conversation](https://api.vapi.ai/api#:~:text=ConversationBlock), [Tool-call](https://api.vapi.ai/api#:~:text=ToolCallBlock), and [Workflow](https://api.vapi.ai/api#:~:text=WorkflowBlock). Each type serves a different role in shaping how your assistant engages with users.


<Note>
  Blocks is currently in beta. We're excited to have you try this new feature and welcome your [feedback](https://discord.com/invite/pUFNcf2WmH) as we continue to refine and improve the experience.
</Note>

## Advanced Concepts

<CardGroup cols={2}>
  <Card title="Steps" icon="stairs" href="/blocks/steps">
    Learn how to structure the flow of your conversation
  </Card>
  <Card title="Block Types" icon="boxes-stacked" href="/blocks/block-types">
    Explore the different block types and how to use them
  </Card>
</CardGroup>

 This is the content for the doc fern/blocks/block-types.mdx 

 ---
title: Block Types
subtitle: 'Building the Logic and Actions for Each Step in Your Conversation '
slug: blocks/block-types
---

<Warning>
  **Blocks** is being deprecated in favor of [Workflows](/workflows). We recommend using Workflows for all new development as it provides a more powerful and flexible way to structure conversational AI. We're working on migration tools to help transition existing Blocks implementations to Workflows.
</Warning>

[**Blocks**](https://api.vapi.ai/api#/Blocks/BlockController_create) are the functional units within a Step, defining what action happens at each stage of a conversation. Each Step can contain only one Block, and there are three main types of Blocks, each designed to handle different aspects of conversation flow.

<Note>
  Blocks is currently in beta. We're excited to have you try this new feature and welcome your [feedback](https://discord.com/invite/pUFNcf2WmH) as we continue to refine and improve the experience.
</Note>

#### Types

- [**Conversation:**](https://api.vapi.ai/api#:~:text=ConversationBlock) This block type manages interactions between the assistant and the user. A conversation block is used when the assistant needs to ask the user for specific information, such as contact details or preferences.
- [**Tool-call:**](https://api.vapi.ai/api#:~:text=ToolCallBlock) This block allows the assistant to make external tool calls.
- [**Workflow:**](https://api.vapi.ai/api#:~:text=WorkflowBlock) This block type enables the creation of subflows, which are smaller sets of steps executed within a Block. It can contain an array of steps (`steps[]`) and uses an `inputSchema` to define the data needed to initiate the workflow, along with an `outputSchema` to handle the data returned after completing the subflow. Workflow blocks are ideal for organizing complex processes or reusing workflows across different parts of the conversation.

 This is the content for the doc fern/blocks/steps.mdx 

 ---
title: Steps
subtitle: Building and Controlling Conversation Flow for Your Assistants
slug: blocks/steps
---

<Warning>
  **Blocks** is being deprecated in favor of [Workflows](/workflows). We recommend using Workflows for all new development as it provides a more powerful and flexible way to structure conversational AI. We're working on migration tools to help transition existing Blocks implementations to Workflows.
</Warning>

[**Steps**](https://api.vapi.ai/api#:~:text=HandoffStep) are the core building blocks that dictate how conversations progress in a bot interaction. Each Step represents a distinct point in the conversation where the bot performs an action, gathers information, or decides where to go next. Think of Steps as checkpoints in a conversation that guide the flow, manage user inputs, and determine outcomes. 

#### Features

- **Output:** The data or response expected from the step, as outlined in the block's `outputSchema`.
- **Input:** The data necessary for the step to execute, defined in the block's `inputSchema`.
- [**Destinations:**](https://api.vapi.ai/api#:~:text=StepDestination) This can be determined by a simple linear progression or based on specific criteria, like conditions or rules set within the Step. This enables dynamic decision-making, allowing the assistant to choose the next Step depending on what happens during the conversation (e.g., user input, a specific value, or a condition being met).

#### Example

```json
  {
  "type": "handoff",
  "name": "get_user_order",
  "input": {
    "name": "John Doe",
    "email": "johndoe@example.com"
  },
  "destinations": [
    {
      "type": "step",
      "stepName": "confirm_order",
      "conditions": [
        {
          "type": "model-based",
          "instruction": "If the user has provided an order"
        }
      ]
    }
  ],
  "block": {
    "name": "ask_for_order",
    "type": "conversation",
    "inputSchema": {
      "type": "object",
      "required": ["name", "email"],
      "properties": {
        "name": { "type": "string", "description": "The customer's name" },
        "email": { "type": "string", "description": "The customer's email" }
      }
    },
    "instruction": "Greet the customer and ask for their name and email. Then ask them what they'd like to order.",
    "outputSchema": {
      "type": "object",
      "required": ["orders", "name"],
      "properties": {
        "orders": {
          "type": "string",
          "description": "The customer's order, e.g., 'burger with fries'"
        },
        "name": { 
          "type": "string",
          "description": "The customer's name"
        }
      }
    }
  }
}
```

 This is the content for the doc fern/call-forwarding.mdx 

 ---
title: Call Forwarding
slug: call-forwarding
---

Vapi's call forwarding functionality allows you to redirect calls to different phone numbers based on specific conditions using tools. This guide explains how to set up and use the `transferCall` function for call forwarding.

## Key Concepts

### Call Forwarding Tools

- **`transferCall` Tool**: This tool enables call forwarding to predefined phone numbers with specific messages based on the destination.

### Parameters and Messages

- **Destinations**: A list of phone numbers where the call can be forwarded.
- **Messages**: Custom messages that inform the caller about the call being forwarded.

## Setting Up Call Forwarding

### 1. Defining Destinations and Messages

The `transferCall` tool includes a list of destinations and corresponding messages to notify the caller:

```json
{
  "tools": [
    {
      "type": "transferCall",
      "destinations": [
        {
          "type": "number",
          "number": "+1234567890",
          "message": "I am forwarding your call to Department A. Please stay on the line."
        },
        {
          "type": "number",
          "number": "+0987654321",
          "message": "I am forwarding your call to Department B. Please stay on the line."
        },
        {
          "type": "number",
          "number": "+1122334455",
          "message": "I am forwarding your call to Department C. Please stay on the line."
        }
      ],
      "function": {
        "name": "transferCall",
        "description": "Use this function to transfer the call. Only use it when following instructions that explicitly ask you to use the transferCall function. DO NOT call this function unless you are instructed to do so.",
        "parameters": {
          "type": "object",
          "properties": {
            "destination": {
              "type": "string",
              "enum": ["+1234567890", "+0987654321", "+1122334455"],
              "description": "The destination to transfer the call to."
            }
          },
          "required": ["destination"]
        }
      },
      "messages": [
        {
          "type": "request-start",
          "content": "I am forwarding your call to Department A. Please stay on the line.",
          "conditions": [
            {
              "param": "destination",
              "operator": "eq",
              "value": "+1234567890"
            }
          ]
        },
        {
          "type": "request-start",
          "content": "I am forwarding your call to Department B. Please stay on the line.",
          "conditions": [
            {
              "param": "destination",
              "operator": "eq",
              "value": "+0987654321"
            }
          ]
        },
        {
          "type": "request-start",
          "content": "I am forwarding your call to Department C. Please stay on the line.",
          "conditions": [
            {
              "param": "destination",
              "operator": "eq",
              "value": "+1122334455"
            }
          ]
        }
      ]
    }
  ]
}
```

You can also specify the `extension` parameter to forward the call to an extension.

```json
    "destinations": [
        {
            "type": "number",
            "number": "+1234567890",
            "extension": "4603",
            "message": "I am forwarding your call to Department A. Please stay on the line."
        }
    ]
```

### 2. Using the `transferCall` Function

When the assistant needs to forward a call, it uses the `transferCall` function with the appropriate destination:

```json
{
  "function": {
    "name": "transferCall",
    "parameters": {
      "destination": "+1234567890"
    }
  }
}
```

### 3. Customizing Messages

Customize the messages for each destination to provide clear information to the caller:

```json
{
  "messages": [
    {
      "type": "request-start",
      "content": "I am forwarding your call to Department A. Please stay on the line.",
      "conditions": [
        {
          "param": "destination",
          "operator": "eq",
          "value": "+1234567890"
        }
      ]
    }
  ]
}
```

## Instructing the Assistant

Use the system prompt to guide the assistant on when to utilize each forwarding number. For example:

- "If the user asks for sales, call the `transferCall` function with `+1234567890`."
- "If the user requests technical support, use the `transferCall` function with `+0987654321`."

## Troubleshooting

- If calls are not being transferred, check the logs for errors.
- Ensure that the correct destination numbers are used.
- Ensure you have written the function description properly to indicate where you want to forward the call
- Test the call forwarding setup thoroughly to confirm its functionality.

## Call Transfers Mode

Vapi supports two types of call transfers:

1. **Blind Transfer** (default): Directly transfers the call to another agent without providing any prior information to the recipient.
2. **Warm Transfer**: Transfers the call to another agent after providing context about the call. The context can be either a full transcript or a summary, based on your configuration.

### Warm Transfer

To implement a warm transfer, add a `transferPlan` object to the `transferCall` tool syntax and specify the transfer mode.

#### Modes of Warm Transfer

#### 1. Warm Transfer with Summary

In this mode, Vapi provides a summary of the call to the recipient before transferring.

- **Configuration:**

  - Set the `mode` to `"warm-transfer-with-summary"`.
  - Define a `summaryPlan` specifying how the summary should be generated.
  - Use the `{{transcript}}` variable to include the call transcript.

- **Example:**

```json
"transferPlan": {
  "mode": "warm-transfer-with-summary",
  "summaryPlan": {
    "enabled": true,
    "messages": [
      {
        "role": "system",
        "content": "Please provide a summary of the call."
      },
      {
        "role": "user",
        "content": "Here is the transcript:\n\n{{transcript}}\n\n"
      }
    ]
  }
}
```

#### 2. Warm Transfer with Message

In this mode, Vapi delivers a custom message to the recipient before transferring the call.

- **Configuration:**

  - Set the `mode` to `"warm-transfer-with-message"`.
  - Provide the custom message in the `message` property.
  - Note that the `{{transcript}}` variable is not available in this mode.

- **Example:**

```json
"transferPlan": {
  "mode": "warm-transfer-with-message",
  "message": "Hey, this call has been forwarded through Vapi."
}
```

#### Complete Example

Here is a full example of a `transferCall` payload using the warm transfer with summary mode:

```json
{
  "type": "transferCall",
  "messages": [
    {
      "type": "request-start",
      "content": "I'll transfer you to someone who can help."
    }
  ],
  "destinations": [
    {
      "type": "number",
      "number": "+918936850777",
      "description": "Transfer the call",
      "transferPlan": {
        "mode": "warm-transfer-with-summary",
        "summaryPlan": {
          "enabled": true,
          "messages": [
            {
              "role": "system",
              "content": "Please provide a summary of the call."
            },
            {
              "role": "user",
              "content": "Here is the transcript:\n\n{{transcript}}\n\n"
            }
          ]
        }
      }
    }
  ]
}
```

#### 3. Warm Transfer with Wait and Say Message

In this mode, Vapi waits for the recipient to speak first and then delivers a custom message to the recipient before transferring the call.

- **Configuration:**

  - Set the `mode` to `"warm-transfer-wait-for-operator-to-speak-first-and-then-say-message"`.
  - Provide the custom message in the `message` property.
  - Note that the `{{transcript}}` variable is not available in this mode.

- **Example:**

```json
"transferPlan": {
  "mode": "warm-transfer-wait-for-operator-to-speak-first-and-then-say-message",
  "message": "Hey, this call has been forwarded through Vapi."
}
```

#### 4. Warm Transfer with Wait and Say Summary

In this mode, Vapi waits for the recipient to speak first and then delivers a summary of the call to the recipient before transferring the call.

- **Configuration:**

  - Set the `mode` to `"warm-transfer-wait-for-operator-to-speak-first-and-then-say-summary"`.
  - Define a `summaryPlan` specifying how the summary should be generated.
  - Use the `{{transcript}}` variable to include the call transcript.

- **Example:**

```json
"transferPlan": {
  "mode": "warm-transfer-wait-for-operator-to-speak-first-and-then-say-summary",
  "summaryPlan": {
    "enabled": true,
    "messages": [
      {
        "role": "system",
        "content": "Please provide a summary of the call."
      },
      {
        "role": "user",
        "content": "Here is the transcript:\n\n{{transcript}}\n\n"
      }
    ]
  }
}
```

#### 5. Warm Transfer with TwiML

In this mode, Vapi executes TwiML instructions on the destination call leg before connecting the destination number.

- **Configuration:**

  - Set the `mode` to `"warm-transfer-with-twiml"`.
  - Provide the TwiML instructions in the `twiml` property.
  - Supports only `Play`, `Say`, `Gather`, and `Pause` verbs.
  - Maximum TwiML length is 4096 characters.
  - TwiML must be provided as a single-line string without line breaks or tabs, and must be a valid XML string. For example: `<Say>Hello</Say>` is valid, but `<Say>Hello\n</Say>` is not.

- **Example:**

```json
"transferPlan": {
  "mode": "warm-transfer-with-twiml",
  "twiml": "<Say>Hello, transferring a customer to you.</Say><Pause length=\"2\"/><Say>They called about billing questions.</Say>"
}
```

Here is a full example of a `transferCall` payload using the warm transfer with TwiML mode:

```json
{
  "type": "transferCall",
  "messages": [
    {
      "type": "request-start",
      "content": "I'll transfer you to someone who can help."
    }
  ],
  "destinations": [
    {
      "type": "number",
      "number": "+14155551234",
      "description": "Transfer to customer support",
      "transferPlan": {
        "mode": "warm-transfer-with-twiml",
        "twiml": "<Say>Hello, this is an incoming call from a customer.</Say><Pause length=\"1\"/><Say>They have questions about their recent order.</Say><Pause length=\"1\"/><Say>Connecting you now.</Say>",
        "sipVerb": "refer"
      }
    }
  ]
}
```

**Notes:**

- In all warm transfer modes, the `{{transcript}}` variable contains the full transcript of the call and can be used within the `summaryPlan`.
- For more details about transfer plans and configuration options, please refer to the [transferCall API documentation](https://docs.vapi.ai/api-reference/tools/create#request.body.transferCall.destinations.number.transferPlan)


 This is the content for the doc fern/calls/call-dynamic-transfers.mdx 

 ---
title: Dynamic Call Transfers
slug: calls/call-dynamic-transfers
---
## Introduction to Transfer Destinations

Transferring calls dynamically based on context is an essential feature for handling user interactions effectively. This guide walks you through creating a custom transfer tool, linking it to your assistant, and handling transfer requests with detailed examples. Whether the destination is a phone number, SIP, or another assistant, you'll learn how to configure it seamlessly.

## Step 1: Create a Custom Transfer Tool

To get started, create a transfer tool with an empty `destinations` array:

```bash
curl -X POST https://api.vapi.ai/tool \
     -H "Authorization: Bearer insert-private-key-here" \
     -H "Content-Type: application/json" \
     -d '{
  "type": "transferCall",
  "destinations": [],
  "function": {
    "name": "dynamicDestinationTransferCall"
  }
}'
```

This tool acts as a placeholder, allowing dynamic destinations to be defined at runtime.

## Step 2: Link the Tool to Your Assistant

After creating the tool, link it to your assistant. This connection enables the assistant to trigger the tool during calls.

## Step 3: Configure the Server Event

Select the `transfer-destination-request` server event in your assistant settings. This event sends a webhook to your server whenever a transfer is requested, giving you the flexibility to dynamically determine the destination.

## Step 4: Set Up Your Server

Ensure your server is ready to handle incoming requests. Update the assistant's server URL to point to your server, which will process transfer requests and respond with the appropriate destination or error.

## Step 5: Trigger the Tool and Process Requests

Use the following prompt to trigger the transfer tool:

```
[TASK]
trigger the dynamicDestinationTransferCall tool
```

When triggered, the assistant sends a `transfer-destination-request` webhook to your server. This webhook contains all the necessary call details, such as transcripts and messages, allowing your server to process the request dynamically.

**Sample Request Payload:**

```json
{
  "type": "transfer-destination-request",
  "artifact": {
    "messages": [...],
    "transcript": "Hello, how can I help you?",
    "messagesOpenAIFormatted": [...]
  },
  "assistant": { "id": "assistant123" },
  "phoneNumber": "+14155552671",
  "customer": { "id": "customer456" },
  "call": { "id": "call789", "status": "ongoing" }
}
```

## Step 6: Respond to Transfer Requests

Your server should respond with either a valid `destination` or an `error` to indicate why the transfer cannot be completed.

### Transfer Destination Request Response Payload

#### Number Destination

```json
{
  "destination": {
    "type": "number",
    "message": "Connecting you to our support line.",
    "number": "+14155552671",
    "numberE164CheckEnabled": true,
    "callerId": "+14155551234",
    "extension": "101"
  }
}
```

Transfers the call to a specific phone number, with options for caller ID and extensions.

#### SIP Destination

```json
{
  "destination": {
    "type": "sip",
    "message": "Connecting your call via SIP.",
    "sipUri": "sip:customer-support@domain.com",
    "sipHeaders": {
      "X-Custom-Header": "value"
    }
  }
}
```

Transfers the call to a SIP URI with optional custom headers.

### Error Response

If the transfer cannot be completed, respond with an error:

```json
{
  "error": "Invalid destination specified."
}
```

- **Field**: `error`
- **Description**: Provides a clear reason why the transfer failed.

## Destination or Error in Response

Every response to a transfer-destination-request must include either a `destination` or an `error`. These indicate the outcome of the transfer request:

- **Destination**: Provides details for transferring the call.
- **Error**: Explains why the transfer cannot be completed.

## Conclusion

Dynamic call transfers empower your assistant to route calls efficiently based on real-time data. By implementing this flow, you can ensure seamless interactions and provide a better experience for your users.


 This is the content for the doc fern/calls/call-ended-reason.mdx 

 ---
title: Call Ended Reason
subtitle: A guide to understanding all call "Ended Reason" types & errors.
slug: calls/call-ended-reason
---

This guide will discuss all possible `endedReason`s for a call.

You can find these under the **"Ended Reason"** section of your [call
logs](https://dashboard.vapi.ai/calls) (or under the `endedReason` field on the [Call
Object](/api-reference/calls/get-call)).

#### **Assistant-Related**

- **assistant-ended-call**: The assistant intentionally ended the call based on the user's response.
- **assistant-ended-call-after-message-spoken**: The assistant intentionally ended the call after speaking a pre-defined message.
- **assistant-error**: This general error occurs within the assistant's logic or processing due to bugs, misconfigurations, or unexpected inputs.
- **assistant-forwarded-call**: The assistant successfully transferred the call to another number or service.
- **assistant-join-timed-out**: The assistant failed to join the call within the expected timeframe.
- **assistant-not-found**: The specified assistant cannot be located or accessed, possibly due to an incorrect assistant ID or configuration issue.
- **assistant-not-invalid**: The assistant ID provided is not valid or recognized by the system.
- **assistant-not-provided**: No assistant ID was specified in the request, causing the system to fail.
- **assistant-request-returned-error**: Communicating with the assistant resulted in an error, possibly due to network issues or problems with the assistant itself.
- **assistant-request-returned-forwarding-phone-number**: The assistant triggered a call forwarding action, ending the current call.
- **assistant-request-returned-invalid-assistant**: The assistant returned an invalid response or failed to fulfill the request properly.
- **assistant-request-returned-no-assistant**: The assistant didn't provide any response or action to the request.
- **assistant-said-end-call-phrase**: The assistant recognized a phrase or keyword triggering call termination.

#### **Pipeline and LLM**

These relate to issues within the AI processing pipeline or the Large Language Models (LLMs) used for understanding and generating text:

- **call.in-progress.error-vapifault-\***: Various error codes indicate specific failures within the processing pipeline, such as function execution, LLM responses, or external service integration. Examples include OpenAI, Azure OpenAI, Together AI, and several other LLMs or voice providers.
- **call.in-progress.error-providerfault-\***: Similar to **call.in-progress.error-vapifault-\***. However, these error codes are surfaced when Vapi receives an error that has occured on the provider's side. Examples include internal server errors, or service unavailability.
- **pipeline-error-\***: Similar to **call.in-progress.error-vapifault-\***. However, these error codes are surfaced when you are using your own provider keys.
- **pipeline-no-available-llm-model**: No suitable LLM was available to process the request. Previously **pipeline-no-available-model**.

#### **Phone Calls and Connectivity**

- **customer-busy**: The customer's line was busy.
- **customer-ended-call**: The customer(end human user) ended the call for both inbound and outbound calls.
- **customer-did-not-answer**: The customer didn't answer the call. If you're looking to build a usecase where you need the bot to talk to automated IVRs, set `assistant.voicemailDetectionEnabled=false`.
- **customer-did-not-give-microphone-permission**: The user didn't grant the necessary microphone access for the call.
- **assistant-did-not-receive-customer-audio**: Similar to **customer-did-not-give-microphone-permission**, but more generalized to situations where no customer audio was received.
- **phone-call-provider-closed-websocket**: The connection with the call provider was unexpectedly closed.
- **twilio-failed-to-connect-call**: The Twilio service, responsible for managing calls, failed to establish a connection.
- **vonage-disconnected**: The call was disconnected by Vonage, another call management service.
- **vonage-failed-to-connect-call**: Vonage failed to establish the call connection.
- **vonage-rejected**: The call was rejected by Vonage due to an issue or configuration problem.
- **sip-telephony-provider-failed-to-connect-call**: The SIP telephony provider failed to establish the call connection. Previously **sip-gateway-failed-to-connect-call**.

#### **Other Reasons**

- **exceeded-max-duration**: The call reached its maximum allowed duration and was automatically terminated.
- **silence-timed-out**: The call was ended due to prolonged silence, indicating inactivity.
- **voicemail**: The call was diverted to voicemail.

#### **Unknown**

- **unknown-error**: An unexpected error occurred, and the cause is unknown. For this, please [contact support](/support) with your `call_id` and account email address, & we will investigate.


 This is the content for the doc fern/calls/call-features.mdx 

 ---
title: Live Call Control
slug: calls/call-features
---

Vapi offers two main features that provide enhanced control over live calls:

1. **Call Control**: This feature allows you to inject conversation elements dynamically during an ongoing call.
2. **Call Listen**: This feature enables real-time audio data streaming using WebSocket connections.

To use these features, you first need to obtain the URLs specific to the live call. These URLs can be retrieved by triggering a `/call` endpoint, which returns the `listenUrl` and `controlUrl` within the `monitor` object.

## Obtaining URLs for Call Control and Listen

To initiate a call and retrieve the `listenUrl` and `controlUrl`, send a POST request to the `/call` endpoint.

### Sample Request

```bash
curl 'https://api.vapi.ai/call' 
-H 'authorization: Bearer YOUR_API_KEY' 
-H 'content-type: application/json' 
--data-raw '{
  "assistantId": "5b0a4a08-133c-4146-9315-0984f8c6be80",
  "customer": {
    "number": "+12345678913"
  },
  "phoneNumberId": "42b4b25d-031e-4786-857f-63b346c9580f"
}'

```

### Sample Response

```json
{
  "id": "7420f27a-30fd-4f49-a995-5549ae7cc00d",
  "assistantId": "5b0a4a08-133c-4146-9315-0984f8c6be80",
  "phoneNumberId": "42b4b25d-031e-4786-857f-63b346c9580f",
  "type": "outboundPhoneCall",
  "createdAt": "2024-09-10T11:14:12.339Z",
  "updatedAt": "2024-09-10T11:14:12.339Z",
  "orgId": "eb166faa-7145-46ef-8044-589b47ae3b56",
  "cost": 0,
  "customer": {
    "number": "+12345678913"
  },
  "status": "queued",
  "phoneCallProvider": "twilio",
  "phoneCallProviderId": "CA4c6793d069ef42f4ccad69a0957451ec",
  "phoneCallTransport": "pstn",
  "monitor": {
    "listenUrl": "wss://aws-us-west-2-production1-phone-call-websocket.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/transport",
    "controlUrl": "<https://aws-us-west-2-production1-phone-call-websocket.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/control>"
  }
}

```

## Call Control Features

Once you have the `controlUrl`, you can use various control features during a live call. Here are all the available control options:

### 1. Say Message
Makes the assistant say a specific message during the call.

```bash
curl -X POST 'https://aws-us-west-2-production1-phone-call-websocket.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/control' 
-H 'content-type: application/json' 
--data-raw '{
  "type": "say",
  "content": "Welcome to Vapi, this message was injected during the call.",
  "endCallAfterSpoken": false
}'
```

### 2. Add Message to Conversation
Adds a message to the conversation history and optionally triggers a response.

```bash
curl -X POST 'https://aws-us-west-2-production1-phone-call-websocket.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/control' 
-H 'content-type: application/json' 
--data-raw '{
  "type": "add-message",
  "message": {
    "role": "system",
    "content": "New message added to conversation"
  },
  "triggerResponseEnabled": true
}'
```

### 3. Assistant Control
Control the assistant's behavior during the call.

```bash
curl -X POST 'https://aws-us-west-2-production1-phone-call-websocket.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/control' 
-H 'content-type: application/json' 
--data-raw '{
  "type": "control",
  "control": "mute-assistant"  // Options: "mute-assistant", "unmute-assistant", "say-first-message"
}'
```

### 4. End Call
Programmatically end the ongoing call.

```bash
curl -X POST 'https://aws-us-west-2-production1-phone-call-websocket.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/control' 
-H 'content-type: application/json' 
--data-raw '{
  "type": "end-call"
}'
```

### 5. Transfer Call
Transfer the call to a different destination.

```bash
curl -X POST 'https://aws-us-west-2-production1-phone-call-websocket.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/control' 
-H 'content-type: application/json' 
--data-raw '{
  "type": "transfer",
  "destination": {
    "type": "number",
    "number": "+1234567890"
  },
  "content": "Transferring your call now"
}'
```

## Call Listen Feature

The `listenUrl` allows you to connect to a WebSocket and stream the audio data in real-time. You can either process the audio directly or save the binary data to analyze or replay later.

### Example: Saving Audio Data from a Live Call

Here is a simple implementation for saving the audio buffer from a live call using Node.js:

```jsx
const WebSocket = require('ws');
const fs = require('fs');

let pcmBuffer = Buffer.alloc(0);

const ws = new WebSocket("wss://aws-us-west-2-production1-phone-call-websocket.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/transport");

ws.on('open', () => console.log('WebSocket connection established'));

ws.on('message', (data, isBinary) => {
  if (isBinary) {
    pcmBuffer = Buffer.concat([pcmBuffer, data]);
    console.log(`Received PCM data, buffer size: ${pcmBuffer.length}`);
  } else {
    console.log('Received message:', JSON.parse(data.toString()));
  }
});

ws.on('close', () => {
  if (pcmBuffer.length > 0) {
    fs.writeFileSync('audio.pcm', pcmBuffer);
    console.log('Audio data saved to audio.pcm');
  }
});

ws.on('error', (error) => console.error('WebSocket error:', error));

```


 This is the content for the doc fern/calls/call-handling-with-vapi-and-twilio.mdx 

 ---
title: Call Handling with Vapi and Twilio
slug: calls/call-handling-with-vapi-and-twilio
---

This document explains how to handle a scenario where a user is on hold while the system attempts to connect them to a specialist. If the specialist does not pick up within X seconds or if the call hits voicemail, we take an alternate action (like playing an announcement or scheduling an appointment). This solution integrates Vapi.ai for AI-driven conversations and Twilio for call bridging.

## Problem

Vapi.ai does not provide a built-in way to keep the user on hold, dial a specialist, and handle cases where the specialist is unavailable. We want:

1. The user already talking to the AI (Vapi).
2. The AI offers to connect them to a specialist.
3. The user is placed on hold or in a conference room.
4. We dial the specialist to join.
5. If the specialist answers, everyone is merged.
6. If the specialist does not answer (within X seconds or goes to voicemail), we want to either announce "Specialist not available" or schedule an appointment.

## Solution

1. An inbound call arrives from Vapi or from the user directly.
2. We store its details (e.g., Twilio CallSid).
3. We send TwiML (or instructions) to put the user in a Twilio conference (on hold).
4. We place a second call to the specialist, also directed to join the same conference.
5. If the specialist picks up, Twilio merges the calls.
6. If not, we handle the no-answer event by playing a message or returning control to the AI for scheduling.

## Steps to Solve the Problem

1. **Receive Inbound Call**

   - Twilio posts data to your `/inbound_call`.
   - You store the call reference.
   - You might also invoke Vapi for initial AI instructions.

2. **Prompt User via Vapi**

   - The user decides whether they want the specialist.
   - If yes, you call an endpoint (e.g., `/connect`).

3. **Create/Join Conference**

   - In `/connect`, you update the inbound call to go into a conference route.
   - The user is effectively on hold.

4. **Dial Specialist**

   - You create a second call leg to the specialist’s phone.
   - A `statusCallback` can detect no-answer or voicemail.

5. **Detect Unanswered**

   - If Twilio sees a no-answer or failure, your callback logic plays an announcement or signals the AI to schedule an appointment.

6. **Merge or Exit**

   - If the specialist answers, they join the user.
   - If not, the user is taken off hold and the call ends or goes back to AI.

7. **Use Ephemeral Call (Optional)**
   - If you need an in-conference announcement, create a short-lived Twilio call that `<Say>` the message to everyone, then ends the conference.

## Code Example

Below is a minimal Express.js server aligned for On-Hold Specialist Transfer with Vapi and Twilio.

1. **Express Setup and Environment**

```js
const express = require("express");
const bodyParser = require("body-parser");
const axios = require("axios");
const twilio = require("twilio");

const app = express();
app.use(bodyParser.urlencoded({ extended: true }));
app.use(bodyParser.json());

// Load important env vars
const {
  TWILIO_ACCOUNT_SID,
  TWILIO_AUTH_TOKEN,
  FROM_NUMBER,
  TO_NUMBER,
  VAPI_BASE_URL,
  PHONE_NUMBER_ID,
  ASSISTANT_ID,
  PRIVATE_API_KEY,
} = process.env;

// Create a Twilio client
const client = twilio(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN);

// We'll store the inbound call SID here for simplicity
let globalCallSid = "";
```

2. **`/inbound_call` - Handling the Inbound Call**

```js
app.post("/inbound_call", async (req, res) => {
  try {
    globalCallSid = req.body.CallSid;
    const caller = req.body.Caller;

    // Example: We call Vapi.ai to get initial TwiML
    const response = await axios.post(
      `${VAPI_BASE_URL || "https://api.vapi.ai"}/call`,
      {
        phoneNumberId: PHONE_NUMBER_ID,
        phoneCallProviderBypassEnabled: true,
        customer: { number: caller },
        assistantId: ASSISTANT_ID,
      },
      {
        headers: {
          Authorization: `Bearer ${PRIVATE_API_KEY}`,
          "Content-Type": "application/json",
        },
      }
    );

    const returnedTwiml = response.data.phoneCallProviderDetails.twiml;
    return res.type("text/xml").send(returnedTwiml);
  } catch (err) {
    return res.status(500).send("Internal Server Error");
  }
});
```

3. **`/connect` - Putting User on Hold and Dialing Specialist**

```js
app.post("/connect", async (req, res) => {
  try {
    const protocol =
      req.headers["x-forwarded-proto"] === "https" ? "https" : "http";
    const baseUrl = `${protocol}://${req.get("host")}`;
    const conferenceUrl = `${baseUrl}/conference`;

    // 1) Update inbound call to fetch TwiML from /conference
    await client.calls(globalCallSid).update({
      url: conferenceUrl,
      method: "POST",
    });

    // 2) Dial the specialist
    const statusCallbackUrl = `${baseUrl}/participant-status`;

    await client.calls.create({
      to: TO_NUMBER,
      from: FROM_NUMBER,
      url: conferenceUrl,
      method: "POST",
      statusCallback: statusCallbackUrl,
      statusCallbackMethod: "POST",
    });

    return res.json({ status: "Specialist call initiated" });
  } catch (err) {
    return res.status(500).json({ error: "Failed to connect specialist" });
  }
});
```

4. **`/conference` - Placing Callers Into a Conference**

```js
app.post("/conference", (req, res) => {
  const VoiceResponse = twilio.twiml.VoiceResponse;
  const twiml = new VoiceResponse();

  // Put the caller(s) into a conference
  const dial = twiml.dial();
  dial.conference(
    {
      startConferenceOnEnter: true,
      endConferenceOnExit: true,
    },
    "my_conference_room"
  );

  return res.type("text/xml").send(twiml.toString());
});
```

5. **`/participant-status` - Handling No-Answer or Busy**

```js
app.post("/participant-status", async (req, res) => {
  const callStatus = req.body.CallStatus;
  if (["no-answer", "busy", "failed"].includes(callStatus)) {
    console.log("Specialist did not pick up:", callStatus);
    // Additional logic: schedule an appointment, ephemeral call, etc.
  }
  return res.sendStatus(200);
});
```

6. **`/announce` (Optional) - Ephemeral Announcement**

```js
app.post("/announce", (req, res) => {
  const VoiceResponse = twilio.twiml.VoiceResponse;
  const twiml = new VoiceResponse();
  twiml.say("Specialist is not available. Ending call now.");

  // Join the conference, then end it.
  twiml.dial().conference(
    {
      startConferenceOnEnter: true,
      endConferenceOnExit: true,
    },
    "my_conference_room"
  );

  return res.type("text/xml").send(twiml.toString());
});
```

7. **Starting the Server**

```js
app.listen(3000, () => {
  console.log("Server running on port 3000");
});
```

## How to Test

1. **Environment Variables**  
   Set `TWILIO_ACCOUNT_SID`, `TWILIO_AUTH_TOKEN`, `FROM_NUMBER`, `TO_NUMBER`, `VAPI_BASE_URL`, `PHONE_NUMBER_ID`, `ASSISTANT_ID`, and `PRIVATE_API_KEY`.

2. **Expose Your Server**

   - Use a tool like `ngrok` to create a public URL to port 3000.
   - Configure your Twilio phone number to call `/inbound_call` when a call comes in.

3. **Place a Real Call**

   - Dial your Twilio number from a phone.
   - Twilio hits `/inbound_call`, and run Vapi logic.
   - Trigger `/connect` to conference the user and dial the specialist.
   - If the specialist answers, they join the same conference.
   - If they never answer, Twilio eventually calls `/participant-status`.

4. **Use cURL for Testing**
   - **Simulate Inbound**:
     ```bash
     curl -X POST https://<public-url>/inbound_call \
       -F "CallSid=CA12345" \
       -F "Caller=+15551112222"
     ```
   - **Connect**:
     ```bash
     curl -X POST https://<public-url>/connect \
       -H "Content-Type: application/json" \
       -d "{}"
     ```

## Note on Replacing "Connect" with Vapi Tools

Vapi offers built-in functions or custom tool calls for placing a second call or transferring, you can replace the manual `/connect` call with that Vapi functionality. The flow remains the same: user is put in a Twilio conference, the specialist is dialed, and any no-answer events are handled.

## Notes & Limitations

1. **Voicemail**  
   If a phone’s voicemail picks up, Twilio sees it as answered. Consider advanced detection or a fallback.

2. **Concurrent Calls**  
   Multiple calls at once require storing separate `CallSid`s or similar references.

3. **Conference Behavior**  
   `startConferenceOnEnter: true` merges participants immediately; `endConferenceOnExit: true` ends the conference when that participant leaves.

4. **X Seconds**  
   Decide how you detect no-answer. Typically, Twilio sets a final `callStatus` if the remote side never picks up.

With these steps and code, you can integrate Vapi Assistant while using Twilio’s conferencing features to hold, dial out to a specialist, and handle an unanswered or unavailable specialist scenario.


 This is the content for the doc fern/calls/call-outbound.mdx 

 ---
title: Outbound Calling
subtitle: Learn how to send outbound calls from Vapi.
slug: calls/outbound-calling
---

## Introduction to Outbound Calling

Vapi’s outbound calling API lets you programmatically initiate single or batch calls to any phone number. You can schedule calls for specific dates and times, ideal for time-sensitive communications. Easily integrate outbound calling into your app for appointment reminders, automated surveys, and call campaigns.

## Prerequisites

- **Vapi Account**: Access to the Vapi Dashboard for configuration.
- **Configured Assistant**: Either a saved assistant or a transient assistant.
- **Phone Number**: Either an imported phone number from one of the supported providers or a free Vapi number. (Note: You cannot make international calls with a free Vapi number).
- **Customer's Phone Number**: The phone number that you want to call.

## Outbound Calls

You can place an outbound call from one of your phone numbers using the [`/call`](/api-reference/calls/create-phone-call) endpoint.

1. **Specify an Assistant:** you must specify either a transient assistant in the `assistant` field or reuse a saved assistant in the `assistantId` field.
2. **Get a Phone Number:** provide the `phoneNumberId` of the imported number or free Vapi number you wish to call from.
3. **Provide a Destination:** Finally, pass the customer's phone number or SIP URI in [`customer`](/api-reference/calls/create#request.body.customer).

Provide your authorization token and now we're ready to issue the API call!

```jsx
{
    "assistantId": "assistant-id",
    "phoneNumberId": "phone-number-id",
    "customer": {
        "number": "+11231231234"
    }
}
```

## Scheduling Outbound Calls

To schedule a call for the future, use the [`schedulePlan`](/api-reference/calls/create#request.body.schedulePlan) parameter and pass a future ISO date-time string to `earliestAt`. This will be the earliest time Vapi will attempt to trigger the outbound call. You may also provider `latestAt`, which will be the latest time Vapi will attempt to trigger the call.

When you schedule a call, we will save the Assistant, Phone Number, and Customer Number resources and refetch them at the time of the call. If you choose to provide a saved assistant through `assistantId`, we will pick up the most up-to-date version of your assistant at the call time. Likewise, if you delete your saved assistant, the call will fail! To ensure the call is issued with a static version of an assistant, pass it as a transient assistant through the `assistant` parameter.

```jsx
{
    "assistantId": "assistant-id",
    "phoneNumberId": "phone-number-id",
    "customer": {
        "number": "+11231231234"
    },
    "schedulePlan": {
        "earliestAt": "2025-05-30T00:00:00Z"
    }
}
```

## Batch Calling [#batch-calling]

To call more than one number at a time, use the [`customers`](/api-reference/calls/create#request.body.customers) parameter to pass an array of `customer`. To provide customer specific assistant overrides, please call the endpoint separately for each destination number.

Use both `customers` and `schedulePlan` together to schedule batched calls.

```jsx
{
    "assistantId": "assistant-id",
    "phoneNumberId": "phone-number-id",
    "customers": [
        {
            "number": "+11231231234"
        },
        {
            "number": "+12342342345"
        }
    ],
    "schedulePlan": {
        "earliestAt": "2025-05-30T00:00:00Z"
    }
}
```

Note: Vapi free numbers have limited number of outbound calls per day. Import a number from Twilio, Vonage, or Telnyx to scale without limits.

<Warning>
  It is a violation of FCC law to dial phone numbers without consent in an
  automated manner. See [Telemarketing Sales
  Rule](/glossary#telemarketing-sales-rule) to learn more.
</Warning>

 This is the content for the doc fern/calls/voicemail-detection.mdx 

 ---
title: Voicemail Detection
slug: calls/voicemail-detection
---

Voicemail is basically a digital answering machine. When you can’t pick up, callers can leave a message so you don’t miss anything important. It’s especially handy if you’re in a meeting, driving, or just can’t get to the phone in time.

### **The Main Problem**

If a lot of your calls are landing in voicemail, you could be spending too much time and money on calls that never connect to a real person. This leads to wasted resources, and sometimes missed business opportunities.

### **The Solution: Early Voicemail Detection**

By detecting voicemail right away, your Vapi Assistant can either hang up (if leaving a message isn’t necessary) or smoothly play a recorded message. This cuts down on useless call time and makes your entire call flow more efficient.

## **Two Ways to Detect Voicemail**

### **1. Using Twilio’s Voicemail Detection**

Twilio has built-in features to detect when a machine picks up. You configure these settings in your Vapi Assistant so it knows when a voicemail system has answered instead of a live person.

```jsx
voicemailDetection: {
  provider: "twilio",
  voicemailDetectionTypes: [
    "machine_start",
    "machine_end_beep",
    "machine_end_silence",
    "unknown",
    "machine_end_other"
  ],
  enabled: true,
  machineDetectionTimeout: 15,
  machineDetectionSpeechThreshold: 2500,
  machineDetectionSpeechEndThreshold: 2050,
  machineDetectionSilenceTimeout: 2000
}

```

- **provider**: Tells Vapi to use Twilio’s system.
- **voicemailDetectionTypes**: Defines the events that mean “voicemail.”
- **machineDetectionTimeout**: How many seconds to wait to confirm a machine.
- The other settings let you fine-tune how quickly or accurately Twilio identifies a machine based on speech or silence.

#### Quick Reference

| Setting                            | Type   | Valid Range   | Default  |
| ---------------------------------- | ------ | ------------- | -------- |
| machineDetectionTimeout            | number | 3 – 59 (sec)  | 30 (sec) |
| machineDetectionSpeechThreshold    | number | 1000–6000 ms  | 2400 ms  |
| machineDetectionSpeechEndThreshold | number | 500–5000 ms   | 1200 ms  |
| machineDetectionSilenceTimeout     | number | 2000–10000 ms | 5000 ms  |

### **2. Using VAPI’s Built-In Voicemail Tool**

Vapi also has an LLM-powered tool that listens for typical voicemail greetings or prompts in the call’s audio transcription. If you prefer an approach that relies more on phrasing and context clues, this is a great option.

```jsx
{
  ...yourExistingSettings,
  "model": {
    "tools": [{ type: "voicemail" }]
  }
}

```

Here, `tools: [{ type: "voicemail" }]` signals that your Vapi Assistant should look for keywords or patterns indicating a voicemail greeting.

## **Combining Both Approaches**

For the best of both worlds, you can enable Twilio’s detection **and** the built-in voicemail tool at the same time:

```jsx
{
  ...yourExistingSettings,
  voicemailDetection: {
    provider: "twilio",
    voicemailDetectionTypes: [
      "machine_start",
      "machine_end_beep",
      "unknown"
    ],
    enabled: true,
    machineDetectionTimeout: 15
  },
  model: {
    tools: [{ type: "voicemail" }]
  }
}

```

When one method doesn’t catch it, the other might—boosting your overall detection accuracy.

## **Tips for Better Voicemail Handling**

1. **Adjust Detection Timing**

   Lower `machineDetectionTimeout` (e.g., to 5 seconds) if you want the system to decide faster. But remember, shorter timeouts can lead to occasional false positives.

2. **Fine-Tune Speech and Silence Thresholds**

   For example:

   ```jsx
   {
     "provider": "twilio",
     "enabled": true,
     "machineDetectionTimeout": 5,
     "machineDetectionSpeechThreshold": 2400,
     "machineDetectionSpeechEndThreshold": 1000,
     "machineDetectionSilenceTimeout": 3000
   }

   ```

   These values tweak how quickly Twilio “listens” for human speech or background silence.

3. **Think Through Your Call Flow**
   - **Give It Time**: If you’re leaving a message, you might want to increase `startSpeakingPlan.waitSeconds` so the detection has enough time before the tone.
   - **firstMessageMode**: Setting it to `assistant-waits-for-user` can also give you smoother call handling—your assistant won’t barge in if someone unexpectedly picks up late

## **What Happens When a Call Ends?**

- **Detected Voicemail + No Message**: The call will end, and you’ll see a reason like `customer-did-not-answer`.
- **Detected Voicemail + Have a Message**: Your assistant leaves the recorded message, and the call ends with a reason like `voicemail`.

## **Testing and Next Steps**

1. **Make a Test Call**: Dial a known voicemail number and watch how quickly (and accurately) your Vapi Assistant identifies the machine.
2. **Tweak Settings**: Adjust your timeout and threshold values based on real-world performance.
3. **Repeat**: Keep testing until you’re confident your configuration is catching voicemail reliably without cutting off real people.

By following these steps, you’ll save time, improve call-handling efficiency, and ensure your system feels more professional. If you need to fine-tune or add new features later, you can always revisit these settings and make quick adjustments.


 This is the content for the doc fern/calls/websocket-transport.mdx 

 ---
title: WebSocket Transport
description: Stream audio directly via WebSockets for real-time, bidirectional communication
slug: calls/websocket-transport
---

# WebSocket Transport

Vapi's WebSocket transport enables real-time, bidirectional audio communication directly between your application and Vapi's AI assistants. Unlike traditional phone or web calls, this transport method lets you stream raw audio data instantly with minimal latency.

## Key Benefits

- **Low Latency**: Direct streaming ensures minimal delays.
- **Bidirectional Streaming**: Real-time audio flow in both directions.
- **Easy Integration**: Compatible with any environment supporting WebSockets.
- **Flexible Audio Formats**: Customize audio parameters such as sample rate.
- **Automatic Sample Rate Conversion**: Seamlessly handles various audio rates.

## Creating a WebSocket Call

To initiate a call using WebSocket transport:

```bash
curl 'https://api.vapi.ai/call' \
  -H 'authorization: Bearer YOUR_API_KEY' \
  -H 'content-type: application/json' \
  --data-raw '{
    "assistant": { "assistantId": "YOUR_ASSISTANT_ID" },
    "transport": {
      "provider": "vapi.websocket",
      "audioFormat": {
        "format": "pcm_s16le",
        "container": "raw",
        "sampleRate": 16000
      }
    }
  }'
```

### Sample API Response

```json
{
  "id": "7420f27a-30fd-4f49-a995-5549ae7cc00d",
  "assistantId": "5b0a4a08-133c-4146-9315-0984f8c6be80",
  "type": "vapi.websocketCall",
  "createdAt": "2024-09-10T11:14:12.339Z",
  "updatedAt": "2024-09-10T11:14:12.339Z",
  "orgId": "eb166faa-7145-46ef-8044-589b47ae3b56",
  "cost": 0,
  "status": "queued",
  "transport": {
    "provider": "vapi.websocket",
    "websocketCallUrl": "wss://api.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/transport"
  }
}
```

## Audio Format Configuration

When creating a WebSocket call, the audio format can be customized:

| Parameter   | Description             | Default             |
|-------------|-------------------------|---------------------|
| `format`    | Audio encoding format   | `pcm_s16le` (16-bit PCM) |
| `container` | Audio container format  | `raw` (Raw PCM)         |
| `sampleRate`| Sample rate in Hz       | `16000` (16kHz)         |

Currently, Vapi supports only raw PCM (`pcm_s16le` with `raw` container). Additional formats may be supported in future updates.

<Note>
Vapi automatically converts sample rates as needed. You can stream audio at 8kHz, 44.1kHz, etc., and Vapi will handle conversions seamlessly.
</Note>

## Connecting to the WebSocket

Use the WebSocket URL from the response to establish a connection:

```javascript
const socket = new WebSocket("wss://api.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/transport");

socket.onopen = () => console.log("WebSocket connection opened.");
socket.onclose = () => console.log("WebSocket connection closed.");
socket.onerror = (error) => console.error("WebSocket error:", error);
```

## Sending and Receiving Data

The WebSocket supports two types of messages:

- **Binary audio data** (PCM, 16-bit signed little-endian)
- **Text-based JSON control messages**

### Sending Audio Data

```javascript
function sendAudioChunk(audioBuffer) {
  if (socket.readyState === WebSocket.OPEN) {
    socket.send(audioBuffer);
  }
}

navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
  const audioContext = new AudioContext();
  const source = audioContext.createMediaStreamSource(stream);
  const processor = audioContext.createScriptProcessor(1024, 1, 1);

  processor.onaudioprocess = (event) => {
    const pcmData = event.inputBuffer.getChannelData(0);
    const int16Data = new Int16Array(pcmData.length);

    for (let i = 0; i < pcmData.length; i++) {
      int16Data[i] = Math.max(-32768, Math.min(32767, pcmData[i] * 32768));
    }

    sendAudioChunk(int16Data.buffer);
  };

  source.connect(processor);
  processor.connect(audioContext.destination);
});
```

### Receiving Data

```javascript
socket.onmessage = (event) => {
  if (event.data instanceof Blob) {
    event.data.arrayBuffer().then(buffer => {
      const audioData = new Int16Array(buffer);
      playAudio(audioData);
    });
  } else {
    try {
      const message = JSON.parse(event.data);
      handleControlMessage(message);
    } catch (error) {
      console.error("Failed to parse message:", error);
    }
  }
};
```

### Sending Control Messages

```javascript
function sendControlMessage(messageObj) {
  if (socket.readyState === WebSocket.OPEN) {
    socket.send(JSON.stringify(messageObj));
  }
}

// Example: hangup call
function hangupCall() {
  sendControlMessage({ type: "hangup" });
}
```

## Ending the Call

To gracefully end the WebSocket call:

```javascript
sendControlMessage({ type: "hangup" });
socket.close();
```

## Comparison: WebSocket Transport vs. Call Listen Feature

Vapi provides two WebSocket options:

| WebSocket Transport                 | Call Listen Feature                |
|-------------------------------------|------------------------------------|
| Primary communication method        | Secondary, monitoring-only channel |
| Bidirectional audio streaming       | Unidirectional (listen-only)       |
| Replaces phone/web as transport     | Supplements existing calls         |
| Uses `provider: "vapi.websocket"`   | Accessed via `monitor.listenUrl`   |

Refer to [Live Call Control](/calls/call-features) for more on the Call Listen feature.

<Warning>
When using WebSocket transport, phone-based parameters (`phoneNumber` or `phoneNumberId`) are not permitted. These methods are mutually exclusive.
</Warning>



 This is the content for the doc fern/changelog/2024-10-07.mdx 

 1. **Add Structured Outputs for OpenAI Functions in Assistant Tools**: You can use [OpenAI Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs) by specifying a new parameter called `strict` as true or false when creating or using `OpenAIFunction`s in `assistant.model.tools[type=function]`. Set the `name`, provide a `description` (up to 1000 characters), and specify `parameters` as a [JSON Schema object](https://json-schema.org/understanding-json-schema). See the [OpenAI guide](https://platform.openai.com/docs/guides/function-calling) for examples.

2. **Secure Incoming SIP Phone Calls to Vapi Provided SIP Numbers**: You can now specify a `username`, `password`, and optional `realm` in SIP Invite AuthZ header, through digest authentication. Create this secure SIP number by specifying an "authentication" object with the username and password fields inside `POST /phone-number` request body. Example:
```bash
curl --location 'https://api.vapi.ai/phone-number' \
--header 'Content-Type: application/json' \
--header 'Authorization: Bearer {}API_KEY}}' \
--data-raw '{
  "provider": "vapi",
  "sipUri": "sip:{{USERNAME}}@sip.vapi.ai",
  "assistantId": "{{ASSISTANT_ID}}",
  "name": "example phone number label for your reference",
  "authentication": {
    "realm": "sip.vapi.ai",
    "username": "test@example.com",
    "password": "example_password"
  }
}'
```

3. **Use Updated `handoff`, `callback` Steps in Blocks**: You can now use `assistant.model.steps[type=handoff]` and `assistant.model.steps[type=callback]` to control conversation flow in your assistant. Use `HandoffStep` to move to the next step linearly without returning to the previous step, ideal for sequential tasks like forms. Use `CallbackStep` to spawn a new conversation thread and return to the previous step once done, good for handling interruptions or sub-tasks within a conversation.

4. **Use Step Destinations and Assignment Mutation in Blocks**: Specify destination nodes for each step with `assistant.model.steps[type=handoff].destinations[type=step]` to direct the workflow to specific steps based on certain conditions. Update context variables in each callback step with `mutations[type=assignment]`, for example: `assistant.model.steps[type=callback].mutations[type=assignment]`

 This is the content for the doc fern/changelog/2024-10-08.mdx 

 1. **New GPT-4o Model Support for Azure OpenAI**: You can now specify the `gpt-4o-2024-08-06` model in the `models` field when configuring Azure OpenAI credentials. Use this model to access the latest GPT-4 operational capabilities in your applications.

2. **Specify Timestamps as Strings in `/logs`**: We now expect timestamps as strings when working with logs. Please make sure to handle this accordingly in your applications.

 This is the content for the doc fern/changelog/2024-10-09.mdx 

 1. **Call Cost Information**: You can now use `call.costs[type=vapi].subType` to determine if a Vapi cost is `normal` or an `overage`.

2. **Updated Billing Page**: Your payments are now returned inside a table with pages on the [billing page](https://dashboard.vapi.ai/org/billing).

 This is the content for the doc fern/changelog/2024-10-10.mdx 

 1. **Purchase Reserved Concurrency and Scale Infinitely**: You can now reserve more concurrent calls with Vapi and scale infinitely by switching to our new top up payment system on the [billing page](https://dashboard.vapi.ai/org/billing). To migrate, click "Switch to Credit Based Billing" and make a payment. Advantages include:

  - **Support More Users Without Limits**: You don't need to worry about getting throttled or staying under usage limits on the conversations you can have with Vapi.
  - **Predictable Budgets**: You know exactly how much you will spend on Vapi each month, and you can top up at any time as your needs grow.
  - **Select Add-Ons You Need**: The credit based billing page allows you to select HIPAA compliance, dedicated Slack support, and the maximum number of concurrent calls you expect.

<Warning>This will require human input to login and migrate your account. You will not be able to revert back to the old billing system.</Warning>

 This is the content for the doc fern/changelog/2024-10-13.mdx 

 1. **New Call Transfer Modes Added**: you can now wait for an operator to speak first before providing a transfer message or summary when transferring calls to a new destination with `TransferPlan`. Configure this through *transferPlan.mode=`'warm-transfer-wait-for-operator-to-speak-first-and-then-say-message'`* or *transferPlan.mode=`'warm-transfer-wait-for-operator-to-speak-first-and-then-say-summary'`* inside the request body of `POST /assistant` or `PATCH /assistant`. 

2. **Unified Server Configuration in Assistants**: You can now use the `server` property in `Assistant.server`, `AssistantOverrides.server`, and when creating or updating assistants to specify webhook settings, including URL, secret, custom headers, and timeout. This replaces the old `serverUrl` and `serverUrlSecret` properties of `Assistant`.

  <Warning>Include custom headers in your webhook requests by using the `headers` property within the `server` object when creating or updating assistants.</Warning>

3. **Configure PlayHT Voice Engines**: You can now configure which PlayHT voice `model` generates voices for your application between `PlayHT2.0`, `PlayHT2.0-turbo`, and `Play3.0-mini`.

 This is the content for the doc fern/changelog/2024-10-16.mdx 

 1. **Apply Coupons to Subscriptions**: You can now apply coupons by specifying a `couponId` to add to a subscription.

2. **Detect Custom Transcriber Failures in Call End Reasons**: You can now handle cases where a custom transcriber fails during a call with `'pipeline-error-custom-transcriber-failed'`, a new `endedReason` option. This is now accessible in `Call`, `ServerMessageStatusUpdate`, and `ServerMessageEndOfCallReport`.

3. **Corrected Typo in Example Custom Voice Request**: We fixed a typo in `CustomVoice.server`, where the example request now shows how to use the `"message"` parameter instead of the misspelled `"messsage"`.

 This is the content for the doc fern/changelog/2024-10-19.mdx 

 1. **Custom Transcriber Support**: You can now integrate your own transcription service by using `CustomTranscriber` at `assistant.transcriber`, `call.squad.members.assistant.transcriber`, and `call.squad.members.assistantOverrides.transcriber`. Provide your custom transcription server details via `server.url` to receive real-time transcriptions during calls.

2. **Increased Maximum Call Duration**: The maximum allowed value for `maxDurationSeconds` has increased from 21,600 to 43,200 seconds when creating or updating `Assistant` or `AssistantOverrides`. You can now configure your assistant to handle calls lasting up to 12 hours.

3. **New Voice Provider 'tavus'**: You can now specify `tavus` as a voice provider in `Assistant.voice`, `AssistantOverrides.voice`, `Call.voice` and in the Voice Library.

4. **Subscription Status 'frozen' Added**: A new status `frozen` has been added to `Subscription.status`, indicating when a subscription is temporarily inactive.

5. **Added Subscription Coupon Codes**: You can now apply coupon codes to your subscription. Visit the [billing page](https://dashboard.vapi.ai/org/billing) to apply coupons to specific organizations within a subscription.

 This is the content for the doc fern/changelog/2024-10-22.mdx 

 1. **Invite Multiple Users via Email**: You can now invite up to 100 users at once by providing a list of email addresses inside your [org users page](https://dashboard.vapi.ai/org/users). Click `'+'` after entering an email address, select the role as *Editor* or *Admin*, and click `'Invite'`.

2. **Simplified Subscription Status Handling**: Your subscription status no longer includes the `past-due` status, so you can streamline your subscription management without handling 'past-due' scenarios.

 This is the content for the doc fern/changelog/2024-10-25.mdx 

 1. **Specify API Traffic Channel for Organizations**: You can now configure which `channel` (cluster) your API traffic will be routed to. Select between *daily* or *weekly* in your [organization settings page](https://dashboard.vapi.ai/org/settings)

2. **Customize Tavus Voice Properties**: You can now use Tavus as a voice provider under `assistant.voice`. Configure additional properties like language, recording options, and transcriptions via `assistant.voice.properties`.

3. **Multilingual Support in Tool Messages**: You can now use the `contents` property in `ToolMessageStart`, `ToolMessageFailed`, `ToolMessageDelayed`, and `ToolMessageComplete` to provide message variants for different languages. If you don't provide content for a language, the first item will be automatically translated to the active language during the conversation.

4. **Automatic Translation of Message Contents**: For `CustomMessage`, `BlockStartMessage`, and `BlockCompleteMessage`, if specific content isn't provided for a language in `contents`, Vapi automatically translates the first item to the active language by default.

5. **Removed Backchanneling Configuration**: The `backchannelingEnabled` property has been removed from when creating or updating `Assistant` or `AssistantOverrides`. Backchanneling is no longer configurable in assistant settings.

 This is the content for the doc fern/changelog/2024-10-29.mdx 

 1. **Gemini Model Support and Credential Management**: You can now use Google Gemini models for your assistant (*gemini-1.5-flash-8b*, *gemini-1.5-flash-002*, *gemini-1.5-pro*, *gemini-1.0-pro*). Create and update your Google credentials by providing your `apiKey` from [Google AI Studio](https://aistudio.google.com/app/apikey) and setting your provider to `'google'` in Vapi.

  <Frame caption="Gemini models are now supported when you select `'google'` as your provider">
    <img src="../static/images/changelog/google-model.png" />
  </Frame>

2. **New Anthropic Model `claude-3-5-sonnet-20241022`**: You can now include Computer Use tools in the `toolWithToolCallList` of `ClientMessageToolCalls` or `ServerMessageToolCalls`. Select `'anthropic'` as your provider, `claude-3-5-sonnet-20241022`, and [create](https://api.vapi.ai/api#/Tools/ToolController_create) or [update](https://api.vapi.ai/api#/Tools/ToolController_update) newly supported computer use tools like `BashTool`, `ComputerTool`, or `TextEditorTool`.

  <Frame caption="`claude-3-5-sonnet-20241022` is now supported when you select `'anthropic'` as your provider">
    <img src="../static/images/changelog/anthropic-model.png" />
  </Frame>

3. **Enhanced Email Regex Support in Replacements**: The email matching regex pattern in `RegexReplacement` now supports top-level domains with two or more characters (`{2,}`). This improvement allows you to replace email addresses with placeholders like `[EMAIL]`, even for longer TLDs.

4. **Paginated Phone Number Responses**: [`GET /phone-numbers`](https://api.vapi.ai/api#/Phone%20Numbers/PhoneNumberController_findAll) now includes a `results` array of phone numbers and pagination metadata for easier handling of large datasets.

 This is the content for the doc fern/changelog/2024-10-30.mdx 

 1. **Auto-reload Credits in Billing Page**: You can now auto-reload credits and check credits remaining for subscriptions within the [updated billing page](https://dashboard.vapi.ai/org/billing).

<Frame caption="Auto-reload credits in the [updated billing page](https://dashboard.vapi.ai/org/billing)">
  <img src="../static/images/changelog/auto-reload.png" />
</Frame>

2. **Expanded Language Options in `CartesiaVoice`**: You can now specify additional languages in `CartesiaVoice.language` (optional), including 'hi' (Hindi), 'it' (Italian), 'ko' (Korean), 'nl' (Dutch), 'pl' (Polish), 'ru' (Russian), 'sv' (Swedish), and 'tr' (Turkish). Refer to the [CartesiaVoice](https://api.vapi.ai/api) schema for more details.

<Frame caption="Expanded language options in `CartesiaVoice`">
  <img src="../static/images/changelog/cartesia-languages.png" />
</Frame>

3. **Enhanced Template Variables in `AssistantOverrides`**: The `AssistantOverrides.variableValues` now supports LiquidJS syntax for replacing template variables. You can customize assistant messages using expressions like `{{ name }}` for dynamic content, or format dates with `{{"now" | date: "%b %d, %Y, %I:%M %p", "America/New_York"}}`.

 This is the content for the doc fern/changelog/2024-11-03.mdx 

 1. **Access Transport Details and Costs**: You can now use `call.transport` to access details about the provider used for a call (`twilio`, `vonage`, `vapi`, or `daily`), and whether the assistant's video is enabled for web calls (`assistantVideoEnabled`). Additionally, transport costs in `call.costs[type=transport]` now include a `provider` field, allowing you to see which provider contributed to the transport cost.

2. **Manage Tavus Credentials**: You can now create and update Tavus credentials in the [updated Provider Credentials page](https://dashboard.vapi.ai/keys).

<Frame caption="Specify Tavus Credentials in the Provider Credentials page">
  <img src="../static/images/changelog/tavus-credentials.png" />
</Frame>

 This is the content for the doc fern/changelog/2024-11-04.mdx 

 1. **XAi Model Support**: You can now use xAI's `grok-beta` model when creating or updating an assistant, and specify your API credentials from the [xAI console](https://console.x.ai/) in the [updated Provider Credentials page](https://dashboard.vapi.ai/keys). The list of call ended reasons has been updated to include xAI-specific errors.

<Frame caption="Specify xAI API credentials in the Provider Credentials page">
  <img src="../static/images/changelog/xai-model.png" />
</Frame>

 This is the content for the doc fern/changelog/2024-11-06.mdx 

 1. **New Anthropic model `claude-3-5-haiku-20241022` added**: You can now use `claude-3-5-haiku-20241022` in your assistants. Specify `anthropic` in `Assistant.model.provider` and `claude-3-5-haiku-20241022` in `Assistant.model`.

2. **Payment `cost`, Subscription `credits` and `couponUsageLeft` are now strings**: These properties are now strings to avoid floating point precision errors. Please update your applications to handle these values as strings.

3. **Advanced call logging improvements**: You can now access detailed call logs through the [updated call logs page](https://dashboard.vapi.ai/calls) or [`GET /logs?type=Call`](https://api.vapi.ai/api#/Logs/LoggingController_queryLogs) endpoint. Refer to `CallLogPrivileged` or `CallLogsPaginatedResponse` schemas in the [updated API reference](https://api.vapi.ai/api) to learn more.

 This is the content for the doc fern/changelog/2024-11-11.mdx 

 1. **Subscription Updates**: You can now check the number of minutes used in a subscription with `Subscription.minutesUsed` (Enterprise only).

2. **Updates to Concurrency Limits in your Subscription**: `Subscription.concurrencyLimit` now shows both the included and purchased limits, which better represents the total concurrency limit. Refer to the [Subscription schema](https://api.vapi.ai/api/) for more details.
  - Use `Subscription.concurrencyLimitIncluded` to get the default concurrency limit provided with the subscription.
  - Use `Subscription.concurrencyLimitPurchased` to get any additional purchased concurrency limit.

<Frame caption="View subscription concurrency limits in the [Subscription schema](https://api.vapi.ai/api/)">
  <img src="../static/images/changelog/subscription-concurrency.png" />
</Frame>

 This is the content for the doc fern/changelog/2024-11-14.mdx 

 1. **Langfuse Credential Management**: You can now send traces to Langfuse by providing your "Secret Key", "Public Key", and "Host URL" for better telemetry monitoring. Create and update these credentials in the [updated Provider Credentials page](https://dashboard.vapi.ai/keys), under `Observability Providers`.

<Frame caption="Create and update Langfuse credentials in https://dashboard.vapi.ai/keys">
  <img src="../static/images/changelog/langfuse.png" />
</Frame>

 This is the content for the doc fern/changelog/2024-11-15.mdx 

 1. **New Voices for `gpt-4o-realtime-preview-2024-10-01`**: You can now use new voice IDs: `ash`, `ballad`, `coral`, `sage`, and `verse` with the `voiceId` parameter when configuring `OpenAIVoice`. Please note that these voices are only available with the `gpt-4o-realtime-preview-2024-10-01` model.

 This is the content for the doc fern/changelog/2024-11-21.mdx 

 1. **Voice Fallback Plan**: You can now define a `fallbackPlan` in your assistant's voice settings in `assistant.voice.fallbackPlan` or `call.squad.members.assistant.voice.fallbackPlan` to specify alternative voices if your primary voice provider fails.

2. **AssemblyAI Credential Management**: You can now specify your AssemblyAI API keys in the updated "Transcriber Providers" page. Create your API key in the [AssemblyAI dashboard](https://www.assemblyai.com/app/account). AssemblyAI errors are now surfaced in the `endedReason` of `Call`, `ServerMessageEndOfCallReport`, and `ServerMessageStatusUpdate`.

  <Frame caption="Specify AssemblyAI API keys in the Transcriber Providers page">
    <img src="../static/images/changelog/assembly-ai.png" />
  </Frame>

3. **Enhanced BYO SIP Trunk Configuration**: When configuring BYO SIP trunk credentials, you can now specify a `techPrefix` for outbound SIP calls and enable `sipDiversionHeader` for authenticating the calling number (if supported). Refer to the `ByoSipTrunkCredential` schema in the [API reference](https://api.vapi.ai/api) to learn more.

  <Frame caption="Learn more about `techPrefix` and `sipDiversionHeader` in the BYO SIP Trunk Credential schema: https://api.vapi.ai/api">
    <img src="../static/images/changelog/byosiptrunkcredential.png" />
  </Frame>

4. **File Name Length Constraints**: The maximum file `name` length has been reduced from 100 to 40 characters. The required minimum length is still 1 character. 

5. **Increased Server Timeout Limit**: The maximum value of `server.timeoutSeconds` has increased from 60 to 120 seconds, allowing longer timeouts for server responses.

7. **Extended Delay for Tool Messages**: The `timingMilliseconds` maximum in `ToolMessageDelayed` has increased from 20,000 to 120,000 milliseconds, enabling a longer delay for tool messages.


 This is the content for the doc fern/changelog/2024-11-22.mdx 

 1. **Support for 'uaenorth' Region in Azure OpenAI Credentials**: When configuring Azure OpenAI credentials, you can now set `region` to use the UAE North region by specifying `'uaenorth'`.

 This is the content for the doc fern/changelog/2024-11-24.mdx 

 1. **Voice Fallback Plan Introduced**: You can now enhance your assistant's reliability by defining fallback voice providers using `assistant.voice.fallbackPlan.voices`. This allows your assistant to switch to alternative voices or providers like `FallbackLMNTVoice`, `FallbackAzureVoice`, `FallbackNeetsVoice`, `FallbackTavusVoice`, `FallbackOpenAIVoice`, and others if the primary voice provider fails.

  <Frame caption="Refer to the `FallbackPlan` schema for more information: https://api.vapi.ai/api">
    <img src="../static/images/changelog/fallback-plan.png" />
  </Frame>

2. **Language Selection for PlayHTVoice**: The `language` property has been added to `PlayHTVoice`. You can now specify the desired language for speech synthesis using `assistant.voice.language`.

3. **AssemblyAI Transcriber Available**: You can now use AssemblyAI for transcribing by setting `Assistant.transcriber` to `AssemblyAITranscriber`. This provides a new option for converting speech to text in your assistant.

4. **Updated OpenAI Model Support**: The `gpt-4o-2024-11-20` model has been added to `OpenAIModel.model` and `OpenAIModel.fallbackModels`. You can now configure your assistant to use this latest OpenAI model.

5. **Removal of 'fillerInjectionEnabled' Property**: The `fillerInjectionEnabled` property has been removed from voice configurations like `LMNTVoice`, `AzureVoice`, etc. You no longer need to include this property when configuring these voices.


 This is the content for the doc fern/changelog/2024-11-25.mdx 

 1. **No length limit for assistant's first message**: You can now set `assistant.firstMessage` or `call.assistant.firstMessage` to any length; the previous maximum length restriction has been removed. This allows you to provide longer initial messages for the assistant's greeting. 

  <Frame caption="Refer to the `Assistant` schema for more information: https://api.vapi.ai/api">
    <img src="../static/images/changelog/first-message.png" />
  </Frame>



 This is the content for the doc fern/changelog/2024-11-27.mdx 

 1. **New Knowledge Base API Endpoints**: You can now create a knowledge base with [`POST /knowledge-base`](https://api.vapi.ai/api#/Knowledge%20Base/KnowledgeBaseController_create), list knowledge bases with [`GET /knowledge-base`](https://api.vapi.ai/api#/Knowledge%20Base/KnowledgeBaseController_findAll) or [`GET /knowledge-base/{id}`](https://api.vapi.ai/api#/Knowledge%20Base/KnowledgeBaseController_findOne) for a specific knowledge base, update a knowledge base with [`PATCH /knowledge-base/{id}`](https://api.vapi.ai/api#/Knowledge%20Base/KnowledgeBaseController_update), or delete a knowledge base with [`DELETE /knowledge-base/{id}`](https://api.vapi.ai/api#/Knowledge%20Base/KnowledgeBaseController_remove). Refer to the [Vapi API endpoints](https://api.vapi.ai/api#/) for more details.

  <Frame caption="Refer to the `Knowledge Base` endpoints for more information: https://api.vapi.ai/api">
    <img src="../static/images/changelog/knowledge-base-endpoints.png" />
  </Frame>


2. **Configure Custom Knowledge Bases for your Assistant**: Configure a custom knowledge base for your assistant in three steps by first uploading a file [through the dashboard](https://dashboard.vapi.ai/files) or  [the API](https://api.vapi.ai/api#/Files/FileController_create), then [create a knowledge base](https://docs.vapi.ai/knowledgebase#step-2-create-a-knowledge-base), and lastly [updating your assistant](https://docs.vapi.ai/knowledgebase#step-3-create-an-assistant) with the knowledge base id. You can also assign a knowledge base to models like `GroqModel`, `VapiModel`, `GoogleModel`, and others using the new `knowledgeBaseId` property.


3. **Integration with Trieve Knowledge Base**: Vapi now supports [Trieve](https://trieve.ai/) as our first knowledge base provider. Refer to [our docs](https://docs.vapi.ai/knowledgebase#step-2-create-a-knowledge-base) for an example of how to use Trieve.


4. **Inflection AI Credential Management**: You can now manage Inflection AI credentials through the [updated providers credentials](https://dashboard.vapi.ai/keys) page. `Call.endedReason` also now enumerates new values like `'pipeline-error-inflection-ai-llm-failed'` to indicate specific Inflection AI errors.

  <Frame caption="Use the updated provider credentials page to manage Inflection AI credentials: https://dashboard.vapi.ai/keys">
    <img src="../static/images/changelog/inflection-ai.png" />
  </Frame>


5. **New Transfer Mode with Summary in SIP Header**: You can now configure `TransferPlan.mode` to `'blind-transfer-add-summary-to-sip-header'` to forward calls and include a summary in the SIP header called `X-Transfer-Summary`. You can also add custom SIP headers during a transfer call using the `sipHeaders` property in `TransferDestinationSip`. 


6. **Azure Credential Service Default**: When creating or updating Azure credentials in the [updated providers credentials](https://dashboard.vapi.ai/keys), the `service` field now defaults to `'speech'`.


7. **Support for Cantonese in Deepgram Transcriber**: The `DeepgramTranscriber.language` option now includes `'zh-HK'` for Cantonese (Hong Kong).

 This is the content for the doc fern/changelog/2024-11-30.mdx 

 1. **Extended Silence Timeout for Assistants**: You can now set `silenceTimeoutSeconds` up to 3600 seconds (previously 600 seconds) when creating or updating assistants and assistant overrides. This allows for longer periods of silence before an assistant session times out.

2. **New Credits Purchase Option**: You can now purchase credits to your subscription by  navigating to the [updated billing page](https://dashboard.vapi.ai/org/billing/credits). Specify the dollar amount of your credits in the `credits` field to complete the purchase.

  <Frame caption="Navigate to the updated billing page to buy credits: https://dashboard.vapi.ai/org/billing/credits">
    <img src="../static/images/changelog/credits.png" />
  </Frame>

 This is the content for the doc fern/changelog/2024-12-03.mdx 

 1. **New xAI and Inflection AI models**: You can now set `Assistant.model` to use `XAI` (e.g., model `grok-beta`) or `Inflection AI` (e.g., model `inflection_3_pi`) by specifying these providers in your assistant configuration. Specify these providers in `assistant.model`, `call.squad.members.assistant.model`, or `call.squad.members.assistantOverrides.model`.

2. **Integrate Existing Trieve Vector Stores in Your Knowledge Base**: When you create a knowledge base with [`POST /knowledge-base`](https://api.vapi.ai/api#/Knowledge%20Base/KnowledgeBaseController_create), you can now specify `vectorStoreProviderId` to use an existing vector store from your Trieve account. 

3. **Create Vector Stores with Uploaded Files**: You can first upload files using the [`POST /files`](https://api.vapi.ai/api#/Files/FileController_create) endpoint, and then use the `fileIds` to specify the IDs of previously uploaded files to create a new Trieve vector store. You can customize how your files are ingested, chunked, then rebalanced to ensure correct knowledge is served by your assistant:

- *Split delimiters*: Specify `splitDelimiters` to control how files are split before chunking (default is `[.!?\n]`).
- *Splits per chunk*: Set `targetSplitsPerChunk` to specify the desired number of splits per chunk when creating a vector store (default is 20 splits per chunk).
- *Chunk rebalancing*: Set `rebalanceChunks` to `true` to evenly distribute remainder splits across chunks when creating a vector store to ensure balanced chunk sizes; for example, 66 splits with `targetSplitsPerChunk` of 20 will result in 3 chunks with 22 splits each.

4. **Customize Search Heuristics**: You can filter or remove search results from your knowledge base:

- *Filter by Score threshold*: Set `scoreThreshold` to filter out chunks during searches based on their score. For cosine similarity, chunks below the threshold are filtered out; for other distance metrics, chunks above the threshold are filtered.
- *Remove stop words*: Set `removeStopWords` to `true` to remove stop words during searches. The stop words list is specified in `server/src/stop-words.txt`, and queries that are entirely stop words will still be preserved.

5. **Updated Analytics Endpoint**: The `/analytics` endpoint has changed—use `GET /analytics` to retrieve analytics data instead of `POST /analytics`.


 This is the content for the doc fern/changelog/2024-12-05.mdx 

 1. **OAuth2 Support for Custom LLM Credentials and Webhooks**: You can now authorize access to your [custom LLMs](https://docs.vapi.ai/customization/custom-llm/using-your-server#step-2-configuring-vapi-with-custom-llm) and [server urls (aka webhooks)](https://docs.vapi.ai/server-url) using OAuth2 (RFC 6749).

For example, create a webhook credential with `CreateWebhookCredentialDTO` with the following payload:

```json
{
    "provider": "webhook",
    "authenticationPlan": {
        "type": "oauth2",
        "url": "https://your-url.com/your/path/token",
        "clientId": "your-client-id",
        "clientSecret": "your-client-secret"
    },
    "name": "your-credential-name-between-1-and-40-characters"
}
```

This returns a [`WebhookCredential`](https://api.vapi.ai/api) object as follows:

  <Frame caption="Refer to the `WebhookCredential` schema for more information">
    <img src="../static/images/changelog/webhook-credential.png" />
  </Frame>

3. **Removal of Canonical Knowledge Base**: The ability to create, update, and use canoncial knowledge bases in your assistant has been removed from the API(as custom knowledge bases and the Trieve integration supports as superset of this functionality). Please update your implementations as endpoints and models referencing canoncial knowledge base schemas are no longer available.

 This is the content for the doc fern/changelog/2024-12-06.mdx 

 1. **OAuth 2 Authentication for Custom LLM Models and Webhooks**: In addition to (AuthZ)[https://www.okta.com/identity-101/authentication-vs-authorization/], you can now now authenticate users accessing your [custom LLMs](https://docs.vapi.ai/customization/custom-llm/using-your-server#step-2-configuring-vapi-with-custom-llm) and [server urls (aka webhooks)](https://docs.vapi.ai/server-url) using OAuth2 (RFC 6749). Use the `authenticationSession` dictionary which contains an `accessToken` and `expiresAt` datetime to authenticate further requests to your custom LLM or server URL. 

For example, create a webhook credential with `CreateCustomLLMCredentialDTO` with the following payload:
```json
{
    "provider": "custom-llm",
    "apiKey": "your-api-key-max-10000-characters",
    "authenticationPlan": {
        "type": "oauth2",
        "url": "https://your-url.com/your/path/token",
        "clientId": "your-client-id",
        "clientSecret": "your-client-secret"
    },
    "name": "your-credential-name-between-1-and-40-characters"
}
```

This returns a [`CustomLLMCredential`](https://api.vapi.ai/api) object as follows:

  <Frame caption="Refer to the `CustomLLMCredential` schema for more information">
    <img src="../static/images/changelog/custom-llm-credential.png" />
  </Frame>

This can be used to authenticate successive requests to your custom LLM or server URL.


 This is the content for the doc fern/changelog/2024-12-09.mdx 

 1. **Improved Tavus Video Processing Error Messages**: Your call `endedReason` now includes detailed error messages for `pipeline-error-tavus-video-failed`. Use this to detect and manage scenarios where the Tavus video processing pipeline fails during a call.

 This is the content for the doc fern/changelog/2024-12-10.mdx 

 1. **Claude Computer Use Tools Available**: You can now use [Claude computer use tools](https://www.anthropic.com/news/3-5-models-and-computer-use) like `BashTool`, `ComputerTool`, and `TextEditorTool` when building your Vapi assistant. Create these tools with `CreateBashToolDTO` (enables shell command execution), `CreateComputerToolDTO` (use desktop functionality with customizable display dimensions using `displayWidthPx`, `displayHeightPx`), and `CreateTextEditorToolDTO` (text editing operations), respectively.

Refer to our [API docs](https://api.vapi.ai/api) to learn more about how to use Claude computer use tools.

 This is the content for the doc fern/changelog/2024-12-11.mdx 

 1. **Use OpenAI Chat Completions in your Assistant**: you can now more easily integrate your Assistant with OpenAI's [chat completions sessions](https://platform.openai.com/docs/api-reference/chat) by specifying `messages` (an array of `OpenAIMessage` objects) and an `assistantId` (a string). Each `OpenAIMessage` in turn consists of a `content` (a string between 1 and 100,000,000 characters) and  a role (between *assistant*, *function*, *user*, *system*, *tool*). This makes it easier to manage chat sessions associated with a specific assistant. Refer to the `ChatDTO`, `OpenAIMessage` schemas  in [our API docs](https://api.vapi.ai/api) to learn more.

2. **Update Subscription Email on Billing Page**: you can now customize which email address appears on your Vapi invoices through the updated billing page > [under payment history](https://dashboard.vapi.ai/org/billing). You can specify an email address (in addition through physical address and tax id) - read more in [our docs](https://docs.vapi.ai/quickstart/billing#how-do-i-download-invoices-for-my-credit-purchases).

 This is the content for the doc fern/changelog/2024-12-13.mdx 

 1. **Azure Speech Transcriber Support**:  You can now use  Azure's speech-to-text service by specifying `AzureSpeechTranscriber` as an option for `transcriber`. This allows you to leverage Azure's speech to text capabilities when creating or updating your assistant.

Refer to our [api docs](lhttps://api.vapi.ai/api) to learn more.

 This is the content for the doc fern/changelog/2024-12-14.mdx 

 1. **Removal of `'gemma-7b-it'` from `GroqModel` Options:** The `'gemma-7b-it'` model is no longer available when selecting Groq as a model provider. Update your applications to use other valid options provided by the API. 

Refer to the [`GroqModel` schema](https://api.vapi.ai/api) or the [vapi dashboard](https://dashboard.vapi.ai/assistants) for Groq for a list of supported models.

 This is the content for the doc fern/changelog/2024-12-19.mdx 

 1. **Azure Region Renamed to `swedencentral` (from *sweden*)**: Azure Speech Services customers using the Sweden data center should now specify `swedencentral` as your Azure Speech Services region instead of `sweden`.  Update your region in your code and the updated [provider keys page](https://dashboard.vapi.ai/keys) > *Azure Speech*.

 This is the content for the doc fern/changelog/2024-12-21.mdx 

 **Expanded Voice Compatibility with Realtime Models**: You can use the voices ash, ballad, coral, sage, and verse with any realtime models, giving you more flexibility in voice synthesis options.

**Access to New OpenAI Models**:
 You can now specify the new models `gpt-4o-realtime-preview-2024-12-17` and `gpt-4o-mini-realtime-preview-2024-12-17` when configuring `OpenAIModel.model` and `OpenAIModel.fallbackModels`.

**New ElevenLabs Voice Models Available**:
 The new voice models `eleven_flash_v2` and `eleven_flash_v2_5` are now available for use in `ElevenLabsVoice` and `FallbackElevenLabsVoice`, offering potential improvements in voice performance.

 This is the content for the doc fern/changelog/2024-12-30.mdx 

 1. **Addition of *AzureSpeechTranscriber*:**
 You can now configure assistants to use Azure's speech transcription service by setting 
`AzureSpeechTranscriber.provider` to `azure`. Additionally, you will receive azure transcriber errors like `pipeline-error-azure-speech-transcriber-failed` in `Call.endReason`, `ServerMessageEndOfCallReport.endReason`, and `ServerMessageStatusUpdate.endReason`. 

2. **Combined `serverUrl` and  `serverUrlSecret`  into `server` Property**:
 The `serverUrl` and `serverUrlSecret` properties have been replaced by a new `server` property in multiple schemas. This lets you configure webhook endpoints using the `server` object, allowing for more detailed and flexible setup, including URL and authentication, in a single place. These schemas include:
- ByoPhoneNumber
- BuyPhoneNumberDTO
- CreateByoPhoneNumberDTO
- CreateOrgDTO
- CreateTwilioPhoneNumberDTO
- CreateVapiPhoneNumberDTO
- CreateVonagePhoneNumberDTO
- ImportTwilioPhoneNumberDTO
- ImportVonagePhoneNumberDTO
- Org
- OrgWithOrgUser
- TwilioPhoneNumber
- UpdateOrgDTO
- UpdatePhoneNumberDTO
- VapiPhoneNumber
- VonagePhoneNumber

3. **Introduction of New OpenAI Models**:
You can now use `o1-preview`, `o1-preview-2024-09-12`, `o1-mini`, and `o1-mini-2024-09-12`. in `OpenAIModel.model`.

4. **Introduction of *'sonic' Voice Models* in Voice Schemas:**
 You can now use `sonic` and `sonic-preview` models in `CartesiaVoice.model` and `FallbackCartesiaVoice.model` configurations.

5. **Removal of Deprecated *GroqModel* Models:**
 The models `llama3-groq-8b-8192-tool-use-preview` and `llama3-groq-70b-8192-tool-use-preview` have been removed from `GroqModel.model`. You should switch to supported models to avoid any disruptions.


 This is the content for the doc fern/changelog/2025-01-05.mdx 

 1. **New Transfer Plan Mode Added**: You can now include call summaries in the SIP header during blind transfers without assistant involvement with `blind-transfer-add-summary-to-sip-header` (a new `TransferPlan.mode` option). Doing so will make `ServerMessageStatusUpdate` include a `summary` when the call status is `forwarding` - which means you can access call summaries for real-time display or logging purposes in your SIP calls.

2. **Azure Speech Transcription Support**: You can now specify a new property called `AzureSpeechTranscriber.language` in Azure's Speech-to-Text service to improve the accuracy of processing spoken input.

3. **New Groq Model Available**: You can now use `'llama-3.3-70b-versatile'` in `GroqModel.model`.

<Frame caption="New Groq Model">
    <img src="../static/images/changelog/groq-new-model.png" />
</Frame>


 This is the content for the doc fern/changelog/2025-01-07.mdx 

 # New Gemini 2.0 Models, Realtime Updates, and Configuration Options

1. **New Gemini 2.0 Models**: You can now use two new models in `Assistant.model[model='GoogleModel']`: `gemini-2.0-flash-exp` and `gemini-2.0-flash-realtime-exp`, which give you access to the latest  real-time capabilities and experimental features.

2. **Support for Real-time Configuration with Gemini 2.0 Models**: Developers can now fine-tune real-time settings for the Gemini 2.0 Multimodal Live API using `Assistant.model[model='GoogleModel'].realtimeConfig`, enabling more control over text generation and speech output.

3. **Customize Speech Output for Gemini Multimodal Live APIs**: You can now customize the assistant's voice using the `speechConfig` and `voiceConfig` properties, with options like `"Puck"`, `"Charon"`, and more.

4. **Advanced Gemini Text Generation Parameters**: You can also tune advanced hyperparameters such as `topK`, `topP`, `presencePenalty`, and `frequencyPenalty` to control how the assistant generates responses, leading to more natural and dynamic conversations.

 This is the content for the doc fern/changelog/2025-01-11.mdx 

 1. **Integration of Smallest AI Voices**: Assistants can now utilize voices from Smallest AI by setting the voice provider to `Assistant.voice[provider="smallest-ai"]`, allowing selection from a variety of 25 preset voices and customization of voice attributes.

2. **Support for DeepSeek Language Models**: Developers can now configure assistants to use DeepSeek LLMs by setting the `Assistant.model[provider="deep-seek"]` and `Assistant.model[model="deepseek-chat"]`. You can also specify custom credentials by passing the following payload:

```json
{
  "credentials": [
    {
      "provider": "deep-seek",
      "apiKey": "YOUR_API_KEY",
      "name": "YOUR_CREDENTIAL_NAME"
    }
  ],
  "model": {
    "provider": "deep-seek",
    "model": "deepseek-chat"
  }
}
```

3. **Additional Call Ended Reasons for DeepSeek and Cerebras**: New `Call.endedReason` have been added to handle specific DeepSeek and Cerebras call termination scenarios, allowing developers to better manage error handling.

4. **New API Endpoint to Delete Logs**: A new `DELETE /logs` endpoint has been added, enabling developers to programmatically delete logs and manage log data.

5. **Enhanced Call Transfer Options with SIP Verb**: You can now specify a `sipVerb` when defining a `TransferPlan` with `Assistant.model.tools[type=transferCall].destinations[type=sip].transferPlan` giving you the ability to specify the SIP verb (`refer` or `bye`) used during call transfers for greater control over call flow.

6. **Azure Credentials and Blob Storage Support**: You can now configure Azure credentials with support for AzureCredential.service[service=blob_storage] service and use AzureBlobStorageBucketPlan withAzureCredential.bucketPlan, enabling you to store call artifacts directly in Azure Blob Storage.

7. **Add Authentication Support for Azure OpenAI API Management with the 'Ocp-Apim-Subscription-Key' Header**: When configuring Azure OpenAI credentials, you can now include the AzureOpenAICredential.ocpApimSubscriptionKey to authenticate with Azure's OpenAI services for the API Management proxy in place of an API Key.

8. **New CloudflareR2BucketPlan**: You can now use CloudflareR2BucketPlan to configure storage with Cloudflare R2 buckets, enabling you to store call artifacts directly.

9. **Enhanced Credential Support**: It is now simpler to configure provider credentials in `Assistant.credentials`. Additionally, credentials can be overridden with `AssistantOverride.credentials` enables granular credential management per assistant. Our backend improvements add type safety and autocompletion for all supported credential types in the SDKs, making it easier to configure and maintain credentials for the following providers:

- S3Credential
- GcpCredential
- XAiCredential
- GroqCredential
- LmntCredential
- MakeCredential
- AzureCredential
- TavusCredential
- GladiaCredential
- GoogleCredential
- OpenAICredential
- PlayHTCredential
- RimeAICredential
- RunpodCredential
- TrieveCredential
- TwilioCredential
- VonageCredential
- WebhookCredential
- AnyscaleCredential
- CartesiaCredential
- DeepgramCredential
- LangfuseCredential
- CerebrasCredential
- DeepSeekCredential
- AnthropicCredential
- CustomLLMCredential
- DeepInfraCredential
- SmallestAICredential
- AssemblyAICredential
- CloudflareCredential
- ElevenLabsCredential
- OpenRouterCredential
- TogetherAICredential
- AzureOpenAICredential
- ByoSipTrunkCredential
- GoHighLevelCredential
- InflectionAICredential
- PerplexityAICredential

10. **Specify Type When Updating Tools, Blocks, Phone Numbers, and Knowledge Bases**: You should now specify the type in the request body when [updating tools](https://api.vapi.ai/api#/Tools/ToolController_update), [blocks](https://api.vapi.ai/api#/Blocks/BlockController_update), [phone numbers](https://api.vapi.ai/api#/Phone%20Numbers/PhoneNumberController_update), or [knowledge bases](https://api.vapi.ai/api#/Knowledge%20Base/KnowledgeBaseController_update) using the appropriate payload for each type. Specifying the type now provides type safety and autocompletion in the SDKs. Refer to [the schemas](https://api.vapi.ai/api) to see the expected payload for the following types:

- UpdateBashToolDTO
- UpdateComputerToolDTO 
- UpdateDtmfToolDTO
- UpdateEndCallToolDTO
- UpdateFunctionToolDTO
- UpdateGhlToolDTO
- UpdateMakeToolDTO
- UpdateOutputToolDTO
- UpdateTextEditorToolDTO
- UpdateTransferCallToolDTO
- BashToolWithToolCall
- ComputerToolWithToolCall
- TextEditorToolWithToolCall
- UpdateToolCallBlockDTO
- UpdateWorkflowBlockDTO
- UpdateConversationBlockDTO
- UpdateByoPhoneNumberDTO
- UpdateTwilioPhoneNumberDTO
- UpdateVonagePhoneNumberDTO
- UpdateVapiPhoneNumberDTO
- UpdateCustomKnowledgeBaseDTO
- UpdateTrieveKnowledgeBaseDTO



 This is the content for the doc fern/changelog/2025-01-14.mdx 

 **End Call Message Support in ClientInboundMessage**: Developers can now programmatically end a call by sending an `end-call` message type within `ClientInboundMessage`. To use this feature, include a message with the `type` property set to `"end-call"` when sending inbound messages to the client.

 This is the content for the doc fern/changelog/2025-01-15.mdx 

 1. **Updated Log Endpoints:**
Both the `GET /logs` and `DELETE /logs` endpoints have been simplified by removing the `orgId` parameter.

2. **Updated Log Schema:**
The following fields in the Log schema are no longer required: `requestDurationSeconds`, `requestStartedAt`, `requestFinishedAt`, `requestBody`, `requestHttpMethod`, `requestUrl`, `requestPath`, and `responseHttpCode`.

 This is the content for the doc fern/changelog/2025-01-20.mdx 

 # Workflow Steps, Trieve Knowledge Base Updates, and Concurrent Calls Tracking

1. **Use Workflow Blocks to Simplify Blocks Steps:** You can now compose complicated Blocks steps with smaller, resuable [Workflow blocks](https://api.vapi.ai/api#:~:text=Workflow) that manage conversations and take actions in external systems.

In addition to normal operations inside [Block steps](https://docs.vapi.ai/blocks/steps) - you can now [Say messages](https://api.vapi.ai/api#:~:text=Say), [Gather information](https://api.vapi.ai/api#:~:text=Gather), or connect to other workflow [Edges](https://api.vapi.ai/api#:~:text=Edge) based on a [LLM evaluating a condition](https://api.vapi.ai/api#:~:text=SemanticEdgeCondition), or a more [logic-based condition](https://api.vapi.ai/api#:~:text=ProgrammaticEdgeCondition). Workflows can be used through `Assistant.model["VapiModel"]` to create custom call workflows. 

2. **Trieve Knowledge Base Integration Improvements:** You should now configure [Trieve knowledge bases](https://api.vapi.ai/api#:~:text=TrieveKnowledgeBase) using the new `createPlan` and `searchPlan` fields instead of specifying the raw vector plans directly. The new plans allow you to create or import trieve plans directly, and specify the type of search more precisely than before.

3. **Updated Concurrency Tracking:** Your subscriptions now track active calls with `concurrencyCounter`, replacing `concurrencyLimit`. This does not affect how you reserve concurrent calls through [billing add-ons](https://dashboard.vapi.ai/org/billing/add-ons).

<Frame caption="Concurrent Calls Tracking">
    <img src="../static/images/changelog/billing-addon-concurrent-calls.png" />
</Frame>

4. **Define Allowed Values with `type` using `JsonSchema`:** You can restrict model outputs to specific values inside Blocks or tool calls using the new `type` property in [JsonSchema](https://api.vapi.ai/api#:~:text=JsonSchema). Supported types include `string`, `number`, `integer`, `boolean`, `array` (which also needs `items` to be defined), and `object` (which also needs `properties` to be defined).



 This is the content for the doc fern/changelog/2025-01-21.mdx 

 # Updated Azure Regions for Credentials

1. **Updated Azure Regions for Credentials**: You can now specify `canadacentral`, `japaneast`, and `japanwest` as valid regions when specifying your Azure credentials. Additionally, the region `canada` has been renamed to `canadaeast`, and `japan` has been replaced with `japaneast` and `japanwest`; please update your configurations accordingly.

<Frame caption="Updated Azure Regions for Credentials">
    <img src="../static/images/changelog/azure-openai-credentials.png" />
</Frame>


 This is the content for the doc fern/changelog/2025-01-22.mdx 

 # Tool Calling Updates, Final Transcripts, and DeepSeek Reasoner
1. **Migrate `ToolCallFunction` to `ToolCall`**: You should update your client and server tool calling code to use the [`ToolCall` schema](https://api.vapi.ai/api#:~:text=ToolCall) instead of `ToolCallFunction`, which includes properties like `name`, `tool`, and `toolBody` for more detailed tool call specifications. ToolCallFunction has been removed.

2. **Include `ToolCall` Nodes in Workflows**: You can now incorporate [`ToolCall` nodes](https://api.vapi.ai/api#:~:text=ToolCall) directly into workflow block steps, enabling tools to be invoked as part of the workflow execution.

3. **New Model Option `deepseek-reasoner`**: You can now select `deepseek-reasoner` as a model option inside your assistants with `Assistant.model["deep-seek"].model["deepseek-reasoner"]`, offering enhanced reasoning capabilities for your applications.

4. **Support for Final Transcripts in Server Messages**: The API now supports `'transcript[transcriptType="final"]'` in server messages, allowing your application to handle and process end of conversation transcripts.

 This is the content for the doc fern/changelog/2025-01-29.mdx 

 # New workflow nodes, improved call handling, better phone number management, and expanded tool calling capabilities

1. **New Hangup Workflow Node**: You can now include a [`Hangup`](https://api.vapi.ai/api#:~:text=Hangup) node in your workflows to end calls programmatically.

2. **New HttpRequest Workflow Node**: Workflows can now make HTTP requests using the new [`HttpRequest`](https://api.vapi.ai/api#:~:text=HttpRequest) node, enabling integration with external APIs during workflow execution.

3. **Updates to Tool Calls**: The [`ToolCall`](https://api.vapi.ai/api#:~:text=ToolCall) schema has been revamped; you should update your tool calls to use the new `function` property with `id` and `function` details (instead of older `tool` and `toolBody` properties).

4. **Improvements to [Say](https://api.vapi.ai/api#:~:text=Say), [Edge](https://api.vapi.ai/api#:~:text=Edge), [Gather](https://api.vapi.ai/api#:~:text=Gather), and [Workflow](https://api.vapi.ai/api#:~:text=Workflow) Nodes**:
- The `name`, `to`, and `from` properties in these nodes now support up to 80 characters, letting you use more descriptive identifiers.
- A `metadata` property has been added to these nodes, allowing you to store additional information.
- The [`Gather`](https://api.vapi.ai/api#:~:text=Gather) node now supports a `confirmContent` option to confirm collected data with users.

5. **Regex Validation with Json Outputs**: You can now validate inputs and outputs from your conversations, tool calls, and OpenAI structured outputs against regular expressions using the `regex` property in [`JSON outputs`](https://api.vapi.ai/api#:~:text=JsonSchema) node.

6. **New Assistant Transfer Mode**: A new [transfer mode](https://api.vapi.ai/api#:~:text=TransferPlan) `swap-system-message-in-history-and-remove-transfer-tool-messages` allows more control over conversation history during assistant transfers.

7. **Area Code Selection for Vapi Phone Numbers**: You can now specify a desired area code when creating Vapi phone numbers using `numberDesiredAreaCode`.

8. **Chat Completions Support**: You can now handle chat messages and their metadata within your applications using familiar chat completion messages in your workflow nodes.


 This is the content for the doc fern/changelog/2025-02-01.mdx 

 # API Request Node, Improved Retries, and Enhanced Message Controls

1. **HttpRequest Node Renamed to ApiRequest**: The `HttpRequest` workflow node has been renamed to [`ApiRequest`](https://api.vapi.ai/api#:~:text=ApiRequest), and can be accessed through `Assistant.model.workflow.nodes[type="api-request"]`. Key changes:
   - New support for POST requests with customizable headers and body
   - New async request support with `isAsync` flag
   - Task status messages for waiting, starting, failure and success states
<Warning>The `HttpRequest` node is now deprecated and will be removed in a future release. Please migrate to the new `ApiRequest` node.</Warning>

2. **New Backoff and Retry Controls**: You can now configure [`Assistant.model.tools[type=dtmf].server.backoffPlan`](https://api.vapi.ai/api#:~:text=BackoffPlan) to handle failed requests with customizable retry strategies and delays.
   - Supports fixed or exponential backoff strategies
   - Configure `maxRetries` (up to 10) and `baseDelaySeconds` (up to 10 seconds)
   - Available in server configurations via `backoffPlan` property

3. **Enhanced Gather Node**: The [`Assistant.model.workflow.nodes[type=gather]`](https://api.vapi.ai/api#:~:text=Gather) node has been improved with the following changes:
   - Added `maxRetries` property to control retry attempts
   - Now accepts a single JsonSchema instead of an array
   - Removed default value for `confirmContent` property

4. **Improved Message Controls**: [`Assistant.messagePlan`](https://api.vapi.ai/api#:~:text=MessagePlan) has been improved with the following changes:
   - Increased `idleTimeoutSeconds` maximum from 30 to 60 seconds
   - Added `silenceTimeoutMessage` to customize call ending due to silence

5. **New Distilled Deepseek Model with Groq**: You can now select `deepseek-r1-distill-llama-70b` when using [Groq](https://api.vapi.ai/api#:~:text=Groq) as the provider in [`Assistant.model[provider='groq']`](https://api.vapi.ai/api#:~:text=UpdateCallDTO-,Assistant,-UpdateAssistantDTO)

6. **Edge Condition Updates**: Edge conditions now require explicit matching criteria to improve workflow control and readability. Semantic edges must specify a `matches` property while programmatic edges require a `booleanExpression` property to define transition logic.


 This is the content for the doc fern/changelog/2025-02-04.md 

 # Hooks, PCI Compliance, and Blocking Messages

1. **Introduction of `Hook`s in Workflows**: You can now use [`Hooks`](https://api.vapi.ai/api#:~:text=Hook) in your workflows to automatically execute actions when specific events occur, like task start or confirmation. Hooks are now available in [`ApiRequest`](https://api.vapi.ai/api#:~:text=ApiRequest) and [`Gather`](https://api.vapi.ai/api#:~:text=Gather) workflow nodes.

2. **Make your Assistant PCI Compliant**: You can now configure [`Assistant.pciEnabled`](https://api.vapi.ai/api#:~:text=UpdateCallDTO-,Assistant,-UpdateAssistantDTO) to indicate if your assistant deals with sensitive cardholder data that requires PCI compliance, helping you meet security standards for financial information.

3. **Blocking Messages before Tool Calls**: You can now configure your tool calls to wait until a message is fully spoken before starting with [`ToolMessageStart.blocking=true`](https://api.vapi.ai/api#:~:text=ToolMessageStart) (default is `false`). 



 This is the content for the doc fern/changelog/2025-02-10.mdx 

 # API Enhancements, Call Features, and Workflow Improvements

1.  **`POST` requests to `/analytics` (migrate from `GET`)**: You should now make `POST` requests (instead of `GET`) to the [`/analytics`](https://api.vapi.ai/api#/Analytics/AnalyticsController_query) endpoint. Structure your analytics query as a JSON payload using [`AnalyticsQuery`](https://api.vapi.ai/api#/Analytics/AnalyticsQuery) in the request body.

2.  **Use `SayHook` to Intercept and Modify Text for Assistant Speech**: You can use [`SayHook`](https://api.vapi.ai/api#/Hooks/SayHook) to intercept and modify text before it's spoken by your assistant. Specify the text to be spoken using the `exact` or `prompt` properties.

3.  **Call Transfer Support**: The `Transfer` node type is now available in workflows.  Configure the `destination` property to define the transfer target.

4.  **Workflow Edge Condition Updates**: [`AIEdgeCondition`](https://api.vapi.ai/api#:~:text=AIEdgeCondition) (which replaces `SemanticEdgeCondition`) enables AI-powered routing decisions by analyzing conversation context and intent, while [`LogicEdgeCondition`](https://api.vapi.ai/api#:~:text=LogicEdgeCondition) (which replaces `ProgrammaticEdgeCondition`) allows for rule-based routing using custom logical expressions. The previous `SemanticEdgeCondition` and `ProgrammaticEdgeCondition` are now deprecated, and a new `FailedEdgeCondition` has been added to handle node failures in workflows.

5.  **`Gather` Node: Data Collection Refactor**: The [`Gather` node](https://api.vapi.ai/api#:~:text=Gather) now requires an `output` property to define the expected data schema. The `instruction` and `schema` properties have been removed.

6.  **Call Packet Capture (PCAP) Configuration**: Your call [`Artifact`](https://api.vapi.ai/api#:~:text=Artifact)s now support links to download a call's network packet capture (PCAP) file, providing you with detailed network traffic analysis and troubleshooting for calls. PCAP is only supported by `vapi` and `byo-phone-number` providers. Enable PCAP through `pcapEnabled`, automatically upload to S3 bucket with `pcapS3PathPrefix`, and access via `pcapUrl`.

7.  **`ApiRequest` Node Improvements**: [`ApiRequest`](https://api.vapi.ai/api#:~:text=ApiRequest) now supports `GET` requests. You can also define the expected response schema. You can make API requests as `blocking` or run in the `background` with `ApiRequest.mode`.

8.  **`Call` and `ServerMessage` `endedReason` Updates**: The `assistant-not-invalid` `Call.endedReason` has been corrected to `"assistant-not-valid"`. Also added `"assistant-ended-call-with-hangup-task"` to the `Call.endedReason`.

9.  **New Azure OpenAI Model `gpt-4o-2024-08-06-ptu`**: You can now use `gpt-4o-2024-08-06-ptu` from Azure OpenAI inside your [Assistant](https://dashboard.vapi.ai/assistants/2ec63711-f867-4066-8c54-7833346783b1).

<Frame caption="New Azure OpenAI Model gpt-4o-2024-08-06-ptu.png">
    <img src="../static/images/changelog/gpt-4o-2024-08-06-ptu.png" alt="Azure OpenAI Model GPT-4o-2024-08-06-ptu" />
</Frame>


10. **Deprecated Schemas and Properties**: The following properties and schemas are now deprecated in the [API reference](https://api.vapi.ai/api/):
    *   `SemanticEdgeCondition`
    *   `ProgrammaticEdgeCondition`
    *   `Workflow.type`
    *   `ApiRequest.waitTaskMessage`
    *   `ApiRequest.startTaskMessage`
    *   `ApiRequest.failureTaskMessage`
    *   `ApiRequest.successTaskMessage`
    *   `OpenAIModel.semanticCachingEnabled`
    *   `CreateWorkflowDTO.type`


 This is the content for the doc fern/changelog/2025-02-17.mdx 

 ## What's New

### Compliance & Security Enhancements
- **New [CompliancePlan](https://api.vapi.ai/api#:~:text=CompliancePlan) Consolidates HIPAA and PCI Compliance Settings**: You should now enable HIPAA and PCI compliance settings with `Assistant.compliancePlan.hipaaEnabled` and `Assistant.compliancePlan.pciEnabled` which both default to `false` (replacing the old HIPAA and PCI flags on `Assistant` and `AssistantOverrides`).

- **Phone Number Status Tracking**: You can now view your phone number `status` with `GET /phone-number/{id}` for all phone number types ([Bring Your Own Number](https://api.vapi.ai/api#:~:text=ByoPhoneNumber), [Vapi](https://api.vapi.ai/api#:~:text=VapiPhoneNumber), [Twilio](https://api.vapi.ai/api#:~:text=TwilioPhoneNumber), [Vonage](https://api.vapi.ai/api#:~:text=VonagePhoneNumber)) for better monitoring.

### Advanced Call Control

- **Assistant Hooks System**: You can now use [`AssistantHooks`](https://api.vapi.ai/api#:~:text=AssistantHooks) to support `call.ending` events with customizable filters and actions
  - Enable transfer actions through [`TransferAssistantHookAction`](https://api.vapi.ai/api#:~:text=TransferAssistantHookAction). For example:
```javascript
{
  "hooks": [{
    "on": "call.ending",
    "do": [{
      "type": "transfer",
      "destination": {
        // Your transfer configuration
      }
    }]
  }]
}
```

  - Conditionally execute hooks with `Assistant.hooks.filter`. For example, trigger different hooks for call completed, system errors, or customer hangup / transfer:

```json
{
  "assistant": {
    "hooks": [{
        "filters": [{
          "type": "oneOf",
          "key": "call.endedReason",
          "oneOf": ["pipeline-error-custom-llm-500-server-error", "pipeline-error-custom-llm-llm-failed"]
        }]
      }
    ]
  }
}
```

### Model & Voice Updates

- **New Models Added**: You can now use new models inside `Assistant.model[provider="google", "openai", "xai"]` and `Assistant.fallbackModels[provider="google", "openai", "xai"]`
  - Google: Gemini 2.0 series (`flash-thinking-exp`, `pro-exp-02-05`, `flash`, `flash-lite-preview`)
  - OpenAI: o3 mini `o3-mini`
  - xAI: Grok 2 `grok-2`

<Frame caption="Assistant Models">
    <img src="../static/images/changelog/assistant-models.png" alt="New Assistant Models" />
</Frame>

- **New `PlayDialog` Model for [PlayHT Voices](https://api.vapi.ai/api#:~:text=PlayHTVoice)**: You can now use the `PlayDialog` model in `Assistant.voice[provider="playht"].model["PlayDialog"]`.

- **New `nova-3` and `nova-3-general` Models for [Deepgram Transcriber](https://api.vapi.ai/api#:~:text=DeepgramTranscriber)**: You can now use the `nova-3` and `nova-3-general` models in `Assistant.transcriber[provider="deepgram"].model["nova-3", "nova-3-general"]`

### API Improvements

- **Workflow Updates**: You can now send a [`workflow.node.started`](https://api.vapi.ai/api#:~:text=ClientMessageWorkflowNodeStarted) message to track the start of a workflow node for better call flow tracking

- **Analytics Enhancement**: Added subscription table and concurrency columns in [POST /analytics](https://api.vapi.ai/api#/Analytics/AnalyticsController_query) for richer queries about your subscriptions and concurrent calls.

### Deprecations

<Warning>The `/logs` endpoints are now marked as deprecated - plan to update your implementation accordingly.</Warning>


 This is the content for the doc fern/changelog/2025-02-20.mdx 

 ## What's New
1. **Configure 16 text normalization processors in [FormatPlan](https://api.vapi.ai/api#:~:text=FormatPlan)**: You can now control how text is transcribed and spoken for currency, dates, etc. by setting the `formattersEnabled` array in `Assistant.voice.chunkPlan.formatPlan` (not specifying `formattersEnabled` defaults to all formatters being enabled). See all available formatters in the [FormatPlan.formattersEnabled reference](https://api.vapi.ai/api#:~:text=FormatPlan).

2. **Deepgram [Keyterm Prompting](https://developers.deepgram.com/docs/keyterm)**: The `keyterm` array in [DeepgramTranscriber](https://api.vapi.ai/api#:~:text=DeepgramTranscriber) implements Deepgram's [Keyterm Prompting](https://developers.deepgram.com/docs/keyterm) technology, boosting recall for domain-specific terminology. Compared to the existing `keywords` field:  

| Feature          | `keywords`         | `keyterm`          |
|------------------|--------------------|--------------------|
| Recall Boost     | 15-20%             | Up to 90%          |  
| Format           | Word:Weight        | Raw phrases        |
| Use Case         | General vocabulary | Critical terms     |

You should reserve `keyterm` for compliance-sensitive terms like medical codes while using `keywords` for proper nouns / brand names.

3. **Subscription usage tracking improvements**: The `minutesUsedNextResetAt` timestamp now appears in all subscription tiers (not just enterprise), exposed at `subscription.minutesUsedNextResetAt` for predictable billing cycle integration. Combine with existing `minutesUsed` and `minutesIncluded` metrics to build custom usage dashboards, regardless of subscription tier.

4. **Neuphonic voice synthesis**: You can now configure Neuphonic as a voice provider with `Assistant.voice[provider="neuphonic"]`. Handle appropriate errors with `pipeline-error-neuphonic-voice-failed`. Test latency thresholds as Neuphonic requires 200ms additional processing time compared to ElevenLabs.

<Frame caption="Neuphonic Voice Synthesis">
    <img src="../static/images/changelog/neuphonic.png" alt="Neuphonic Voice Synthesis" />
</Frame>

5. **Support for pre-transfer announcements in [ClientInboundMessageTransfer](https://api.vapi.ai/api#:~:text=ClientInboundMessageTransfer)**: The `content` field in `ClientInboundMessageTransfer` now supports pre-transfer announcements ("Connecting you to billing...") before SIP/number routing. Implement via WebSocket messages using type: "transfer" with destination object.

### Deprecation Notice  
<Warning>**OrgWithOrgUser** is now deprecated, and impacts endpoints returning organization-user composites. This has been replaced with separate [`Org`](https://api.vapi.ai/api#:~:text=Org) and [`User`](https://api.vapi.ai/api#:~:text=User) schemas for better clarity and consistency.</Warning>

 This is the content for the doc fern/changelog/2025-02-25.mdx 

 ## Test Suite APIs, Enhanced Call Transfers, Voice Model Enhancements

1.  **Introducing Test Suite Management APIs:** You can now test your assistant conversations before deploying them by creating [end-to-end tests](https://docs.vapi.ai/test/voice-testing#step-1-create-a-new-test-suite), [adding test cases](https://docs.vapi.ai/test/voice-testing#step-3-add-test-cases), and [running and reviewing test suites](https://docs.vapi.ai/test/voice-testing#step-5-run-and-review-tests). You can configure these tests through the [Test Suites dashboard page](https://dashboard.vapi.ai/test-suites) and [Test Suite APIs](https://docs.vapi.ai/api-reference/test-suites/test-suite-controller-find-all-paginated), and learn more in the [docs](https://docs.vapi.ai/test/voice-testing).

<Frame caption="Test Suite Management APIs">
    <img src="../static/images/changelog/test-suite-management.png" alt="Test Suite Management APIs" />
</Frame>


2.  **Enhanced Call Transfers with TwiML Control:** You can now use `twiml` ([Twilio Markup Language](https://www.twilio.com/docs/voice/twiml)) in [`Assistant.model.tools[type=transferCall].destinations[].transferPlan[mode=warm-transfer-twiml]`](https://api.vapi.ai/api#:~:text=TransferPlan) to execute TwiML instructions before connecting the call, allowing for pre-transfer announcements or data collection with Twilio.

3.  **New Voice Models and Experimental Controls:**
    *   **`mistv2` Rime AI Voice:** You can now use the `mistv2` model in [`Assistant.voice[provider="rime-ai"].model[model="mistv2"]`](https://api.vapi.ai/api#:~:text=RimeAIVoice).
    *   **OpenAI Models:** You can now use `chatgpt-4o-latest` model in [`Assistant.model[provider="openai"].model[model="chatgpt-4o-latest"]`](https://api.vapi.ai/api#:~:text=OpenAIModel).

4. **Experimental Controls for Cartesia Voices:** You can now specify your Cartesia voice speed (string) and emotional range (array) with [`Assistant.voice[provider="cartesia"].experimentalControls`](https://api.vapi.ai/api#:~:text=CartesiaExperimentalControls). For example:

```json
{
    "speed": "fast",
    "emotion": [
        "anger:lowest",
        "curiosity:high"
    ]
}
```

| Property | Option |
|----------|--------|
| speed    | slowest |
|          | slow    |
|          | normal (default) |
|          | fast    |
|          | fastest |
| emotion  | anger:lowest |
|          | anger:low |
|          | anger:high |
|          | anger:highest |
|          | positivity:lowest |
|          | positivity:low |
|          | positivity:high |
|          | positivity:highest |
|          | surprise:lowest |
|          | surprise:low |
|          | surprise:high |
|          | surprise:highest |
|          | sadness:lowest |
|          | sadness:low |
|          | sadness:high |
|          | sadness:highest |
|          | curiosity:lowest |
|          | curiosity:low |
|          | curiosity:high |
|          | curiosity:highest |


 This is the content for the doc fern/changelog/2025-02-27.mdx 

 # Phone Keypad Input Support, OAuth2 and Analytics Improvements

1. **Keypad Input Support for Phone Calls:** A new [`keypadInputPlan`](https://api.vapi.ai/api#:~:text=KeypadInputPlan) feature has been added to enable handling of DTMF (touch-tone) keypad inputs during phone calls. This allows your voice assistant to collect numeric input from callers, like account numbers, menu selections, or confirmation codes.

Configuration options:
```json
{
  "keypadInputPlan": {
    "enabled": true,               // Default: false
    "delimiters": "#",             // Options: "#", "*", or "" (empty string)
    "timeoutSeconds": 2            // Range: 0.5-10 seconds, Default: 2
  }
}
```

The feature can be configured in:
- `assistant.keypadInputPlan`
- `call.squad.members.assistant.keypadInputPlan`
- `call.squad.members.assistantOverrides.keypadInputPlan`

2. **OAuth2 Authentication Enhancement:** The [`OAuth2AuthenticationPlan`](https://api.vapi.ai/api#:~:text=OAuth2AuthenticationPlan) now includes a `scope` property to specify access scopes when authenticating. This allows more granular control over permissions when integrating with OAuth2-based services.

```json
{
  "credentials": [
    {
      "authenticationPlan": {
        "type": "oauth2",
        "url": "https://example.com/oauth2/token",
        "clientId": "your-client-id",
        "clientSecret": "your-client-secret",
        "scope": "read:data"  // New property, max length: 1000 characters
      }
    }
  ]
}
```

The scope property can be configured at:
- `assistant.credentials.authenticationPlan`
- `call.squad.members.assistant.credentials.authenticationPlan`

3. **New Analytics Metric: Minutes Used** The [`AnalyticsOperation`](https://api.vapi.ai/api#:~:text=AnalyticsOperation) schema now includes a new column option: `minutesUsed`. This metric allows you to track and analyze the duration of calls in your usage reports and analytics dashboards.


4. **Removed TrieveKnowledgeBaseCreate Schema:** Removed `TrieveKnowledgeBaseCreate` schema from
- `TrieveKnowledgeBase.createPlan`
- `CreateTrieveKnowledgeBaseDTO.createPlan`
- `UpdateTrieveKnowledgeBaseDTO.createPlan`


 This is the content for the doc fern/changelog/2025-03-02.mdx 

 ## Claude 3.7 Sonnet and GPT 4.5 preview, New Hume AI Voice Provider, New Supabase Storage Provider, Enhanced Call Transfer Options

1. **Claude 3.7 Sonnet with Thinking Configuration Support**:
You can now use the latest claude-3-7-sonnet-20250219 model with a new "thinking" feature via the [`AnthropicThinkingConfig`](https://api.vapi.ai/api#:~:text=AnthropicThinkingConfig) schema. 
Configure it in `assistant.model` or `call.squad.members.assistant.model`:
```json
{
  "model": "claude-3-7-sonnet-20250219",
  "provider": "anthropic",
  "thinking": {
    "type": "enabled",
    "budgetTokens": 5000 // min 1024, max 100000
  }
}
```

2. **OpenAI GPT-4.5-Preview Support**:
You can now use the latest gpt-4.5-preview model as a primary model or fallback option via the [`OpenAIModel`](https://api.vapi.ai/api#:~:text=OpenAIModel) schema.
Configure it in `assistant.model` or `call.squad.members.assistant.model`:
```json
{
  "model": "gpt-4.5-preview",
  "provider": "openai"
}
```

3. **New Hume Voice Provider**:
Integrated Hume AI as a new voice provider with the "octave" model for text-to-speech synthesis.

<Frame caption="Hume Voice Provider">
    <img src="../static/images/changelog/hume-voice-configuration.png" alt="Hume Voice Provider" />
</Frame>

4. **Supabase Storage Integration**:
New Supabase S3-compatible storage support for file operations. This integration lets developers configure buckets and paths across 16 regions, enabling structured file storage with proper authentication.
Configure [`SupabaseBucketPlan`](https://api.vapi.ai/api#:~:text=SupabaseBucketPlan) in `assistant.credentials.bucketPlan`,`call.squad.members.assistant.credentials.bucketPlan`

5. **Voice Speed Control**
Added a speed parameter to ElevenLabs voices ranging from 0.7 (slower) to 1.2 (faster) [`ElevenLabsVoice`](https://api.vapi.ai/api#:~:text=ElevenLabsVoice). This enhancement gives developers more control over speech cadence for more natural-sounding conversations.

6. **Enhanced Call Transfer Options in TransferPlan**
Added a new dial option to the sipVerb parameter for call transfers. This complements the existing refer (default) and bye options, providing more flexibility in call handling.
- 'dial': Uses SIP DIAL to transfer the call

7. **Zero-Value Minumum Subscription Minutes**
Changed the minimum value for minutesUsed and minutesIncluded from 1 to 0. This supports tracking of new subscriptions and free tiers with no included minutes.

8. **Zero-Value Minimum KeypadInputPlan Timeout**
Adjusted the KeypadInputPlan.timeoutSeconds minimum from 0.5 to 0.


 This is the content for the doc fern/changelog/2025-03-06.mdx 

 ## New Query Tool and Vapi Voice Provider, Updates to Language Support and Error Handling

1. **New Query Tool Feature and Knowledge Base Integration**

* The API now supports a new query tool that allows assistants to search through knowledge bases. Add this tool to any assistant model by configuring it at `assistant.model.tools[type=query]` path.
* You can now link knowledge bases to query tools, providing structured information sources for assistants to access. Define knowledge bases with a name, model, provider, description, and associated file IDs.

<Accordion title='Example configuration for [`QueryTool`](https://api.vapi.ai/api#:~:text=QueryTool)'>
```json
{
  "type": "query",
  "async": false,
  "server": {
    "url": "https://api.example.com/query-handler"
  },
  "function": {
    "name": "query_knowledge",
    "description": "Query knowledge bases for information",
    "parameters": {
      "type": "object",
      "properties": {
        "query": {
          "type": "string",
          "description": "The query to search for"
        }
      },
      "required": ["query"]
    }
  },
  "knowledgeBases": [
    {
      "name": "Product Documentation",
      "model": "gemini-1.5-flash",
      "provider": "google",
      "description": "Contains all product manuals",
      "fileIds": ["file-123", "file-456"]
    }
  ]
}
```
</Accordion>


2. **New Voice Provider Support**

A new voice provider "vapi" has been added with support for a voice called "Jordan" in [`FallbackVapiVoice`](https://api.vapi.ai/api#:~:text=FallbackVapiVoice). Configure it in our assistant fallback plans at `assistant.voice.fallbackPlan.voices`.

<Frame caption="Vapi Voice Provider">
    <img src="../static/images/changelog/vapi-voice-configuration.png" alt="Vapi Voice Provider" />
</Frame>

3. **Language Support Updates**

Myanmar language ("my") has been added to supported languages, while "jp" and "mymr" codes have been removed. Use "ja" for Japanese language and "my" for Myanmar. Reference [`GladiaTranscriber`](https://api.vapi.ai/api#:~:text=GladiaTranscriber) for more language codes.

4. **Error Handling Improvements**

Added new error code `pipeline-error-11labs-transcriber-failed` for `ServerMessageStatusUpdate.endedReason` and `ServerMessageEndOfCallReport.endedReason`. Also added an explicit `failed` status for test suite runs in [`TestSuiteRun`](https://api.vapi.ai/api#:~:text=TestSuiteRun). These additions provide more detailed error reporting.

5. **Azure OpenAI Model Update**

The model `gpt-4o-2024-08-06-ptu` has been removed from Azure OpenAI credential schemas. Update any credential configurations that were using this model.

 This is the content for the doc fern/changelog/2025-03-09.mdx 

 ## Enhanced Voicemail Detection, File Processing, Knowledge Base Integration, and Invoicing Updates

1. **Track Voicemail Detection Cost, Configure Google and Twilio Voicemail Detection Plans**

* You can now configure provider-specific settings and track voicemail detection costs through the new `VoicemailDetectionCost` schema at `call.costs[type=voicemail-detection]`.
* Configure Google or Twilio voicemail detection settings using the new [`GoogleVoicemailDetectionPlan`](https://api.vapi.ai/api#:~:text=GoogleVoicemailDetectionPlan) and [`TwilioVoicemailDetectionPlan`](https://api.vapi.ai/api#:~:text=TwilioVoicemailDetectionPlan) schemas.

```json
// Google configuration example
{
  "provider": "google",
  "voicemailExpectedDurationSeconds": 15  // Range: 5-60 seconds
}
```

```json
// Twilio configuration example
{
  "provider": "twilio",
  "enabled": true,
  "machineDetectionTimeout": 30,  // Range: 3-59 seconds
  "voicemailDetectionTypes": ["machine_end_beep", "machine_end_silence"]
}
```

2. **Improved File Processing Statuses and Parsed Text Content**

* File processing statuses have been renamed to better reflect their purpose: `processing` → `done` → `failed`.
* Two new properties have been added to the [`File`](https://api.vapi.ai/api#:~:text=File) schema: `parsedTextUrl` and `parsedTextBytes`, providing direct access to parsed text content from processed files.

3. **Google Gemini Models for Knowledge Base Integration**

* The [`KnowledgeBase`](https://api.vapi.ai/api#:~:text=KnowledgeBase) schema now fully supports Google's Gemini models with specific model options.
* You can use Gemini models in your knowledge bases at `assistant.model.tools[type=query].knowledgeBases`.

```json
"model": {
  "enum": [
    "gemini-2.0-flash-thinking-exp",
    "gemini-2.0-pro-exp-02-05",
    "gemini-2.0-flash",
    "gemini-2.0-flash-lite-preview-02-05",
    "gemini-2.0-flash-exp",
    "gemini-2.0-flash-realtime-exp",
    "gemini-1.5-flash",
    "gemini-1.5-flash-002",
    "gemini-1.5-pro",
    "gemini-1.5-pro-002",
    "gemini-1.0-pro"
  ]
}
```

4. **New Invoicing Features**

* You can now use [`InvoicePlan`](https://api.vapi.ai/api#:~:text=InvoicePlan) schema for customizing invoice information with company details.
* This can be accessed via the new `invoicePlan` property on the [`Subscription`](https://api.vapi.ai/api#:~:text=Subscription) schema.
* Customize company name, email, tax ID, and address for your invoices.

5. **Additional Voice Options**

* Five new voice options have been added to the [`FallbackVapiVoice`](https://api.vapi.ai/api#:~:text=FallbackVapiVoice) schema: `Adi`, `Julia`, `Maibri (Web)`, `Maibri (Phone)`, and `Ashley`.
* Configure these voices in your assistant fallback plans at `assistant.voice.fallbackPlan.voices`.
<Frame caption="Additional Vapi Voices">
    <img src="../static/images/changelog/additional-vapi-voices.png" alt="Additional Vapi Voices" />
</Frame>

 This is the content for the doc fern/changelog/2025-03-13.mdx 

 ## New Workflows API, Telnyx Phone Number Support, Voice Options, and much more

1. **Workflows Replace Blocks**: The API has migrated from blocks to workflows with new `/workflow` endpoints. [Introduction to Workflows](https://docs.vapi.ai/workflows)
You can now use [`UpdateWorkflowDTO`](https://api.vapi.ai/api#:~:text=UpdateWorkflowDTO) where conversation components (`Say`, `Gather`, `ApiRequest`, `Hangup`, `Transfer` nodes) are explicitly connected via edges to create directed conversation flows.

<Accordion title='Example workflow (simplified)'>
   ```json
   {
     "name": "Customer Support Workflow",
     "nodes": [
       {
         "id": "greeting",
         "type": "Say",
         "text": "Hello, welcome to customer support. Do you need help with billing or technical issues?"
       },
       {
         "id": "menu",
         "type": "Gather",
         "options": ["billing", "technical", "other"]
       },
       {
         "id": "billing",
         "type": "Say",
         "text": "I'll connect you with our billing department."
       },
       {
         "id": "technical",
         "type": "Say",
         "text": "I'll connect you with our technical support team."
       },
       {
         "id": "transfer_billing",
         "type": "Transfer",
         "destination": {
           "type": "number",
           "number": "+1234567890"
         }
       },
       {
         "id": "transfer_technical",
         "type": "Transfer",
         "destination": {
           "type": "number",
           "number": "+1987654321"
         }
       }
     ],
     "edges": [
       {
         "from": "greeting",
         "to": "menu"
       },
       {
         "from": "menu",
         "to": "billing",
         "condition": {
           "type": "logic",
           "liquid": "{% if input == 'billing' %} true {% endif %}"
         }
       },
       {
         "from": "menu",
         "to": "technical",
         "condition": {
           "type": "logic",
           "liquid": "{% if input == 'technical' %} true {% endif %}"
         }
       },
       {
         "from": "billing",
         "to": "transfer_billing"
       },
       {
         "from": "technical",
         "to": "transfer_technical"
       }
     ]
   }
   ```
</Accordion>

2. **Telnyx Phone Number Support**: Telnyx is now available as a phone number provider alongside Twilio and Vonage. 
    - Use the [`TelnyxPhoneNumber`](https://api.vapi.ai/api#:~:text=TelnyxPhoneNumber), [`CreateTelnyxPhoneNumberDTO`](https://api.vapi.ai/api#:~:text=CreateTelnyxPhoneNumberDTO), and [`UpdateTelnyxPhoneNumberDTO`](https://api.vapi.ai/api#:~:text=UpdateTelnyxPhoneNumberDTO) schemas with [`/phone-number`](https://api.vapi.ai/api#/Phone%20Numbers) endpoints to create and update Telnyx phone numbers.
    - The `Call.phoneCallProviderId` now includes Telnyx's `callControlId` alongside Twilio's `callSid` and Vonage's `conversationUuid`.

3. **New Voice Options**: 
   - **Vapi Voices**: New Vapi voices - `Elliot`, `Rohan`, `Lily`, `Savannah`, and `Hana` 
   - **Hume Voice**: New provider with `octave` model and customizable voice settings
   - **Neuphonic Voice**: New provider with `neu_hq` (higher quality) and `neu_fast` (faster) models

4. **New Cerebras Model**: [`CerebrasModel`](https://api.vapi.ai/api#:~:text=CerebrasModel) Supports `llama3.1-8b` and `llama-3.3-70b` models
   
5. **Enhanced Transcription**:
   - **New Providers**: [ElevenLabs](https://api.vapi.ai/api#:~:text=ElevenLabsTranscriber) and [Speechmatics](https://api.vapi.ai/api#:~:text=SpeechmaticsTranscriber) transcribers now available.
   - **DeepgramTranscriber Numerals**: New `numerals` option converts spoken numbers to digits (e.g., "nine-seven-two" → "972")

6. **Improved Voicemail Detection**: You can now use multiple provider implementations for `assistant.voicemailDetection` (Google, OpenAI, Twilio). OpenAI implementation allows configuring detection duration (5-60 seconds, default: 15).

7. **Smart Endpointing Upgrade**: Now supports LiveKit as an alternative to Vapi's custom-trained model in [`StartSpeakingPlan.smartEndpointingEnabled`](https://api.vapi.ai/api#:~:text=StartSpeakingPlan). LiveKit only supports English but may offer different endpointing characteristics.

8. **Observability with Langfuse**: New `assistant.observabilityPlan` property allows integration with Langfuse for tracing and monitoring of assistant calls. Configure with [LangfuseObservabilityPlan](https://api.vapi.ai/api#:~:text=LangfuseObservabilityPlan).

9. **More Credential Support**: Added support for Cerebras, Google, Hume, InflectionAI, Mistral, Trieve, and Neuphonic credentials in `assistant.credentials`

 This is the content for the doc fern/changelog/2025-03-14.mdx 

 ## Blocks Schema Deprecations, Scheduling Enhancements, and New Voice Options for Vapi Voice


2. **'scheduled' Status Added to Calls and Messages**: You can now set the status of a call or message to `scheduled`, allowing it to be executed at a future time. This enables scheduling functionality within your application for calls and messages.

3. **New Voice Options for Text-to-Speech**: Four new voices—`Neha`, `Cole`, `Harry`, and `Paige`—have been added for text-to-speech services. You can enhance user experience by setting the `voiceId` to one of these options in your configurations.

3. **Removal of Step and Block Schemas**:
<Warning>Blocks and Steps are now officially deprecated. Developers should update their applications to adapt to these changes, possibly by using new or alternative schemas provided.</Warning>

 This is the content for the doc fern/changelog/2025-03-15.mdx 

 # Enhancements in Assistant Responses, New Gemini Model, and Call Handling

1. **Introduction of 'gemini-2.0-flash-lite' Model Option**: You can now use `gemini-2.0-flash-lite` in [`Assistant.model[provider="google"].model[model="gemini-2.0-flash-lite"]`](https://api.vapi.ai/api#:~:text=GoogleModel) for a reduced latency, lower cost Gemini model with a 1 million token context window.

<Frame caption="gemini-2.0-flash-lite Model Option">
    <img src="../static/images/changelog/gemini-2.0-flash-lite.png" alt="gemini-2.0-flash-lite Model Option" />
</Frame>

2. **New Assistant Paginated Response**: All [`Assistant`](https://api.vapi.ai/api#:~:text=Assistants) endpoints now return paginated responses. Each response specifies `itemsPerPage`, `totalItems`, and `currentPage`, which you can use to navigate through a list of assistants.

 This is the content for the doc fern/changelog/2025-03-17.mdx 

 # New `timeoutSeconds` Property in Custom LLM Model

1. **New `timeoutSeconds` Property in [`Custom LLM Model`](https://api.vapi.ai/api#:~:text=CustomLLMModel):** Developers can now specify a custom timeout duration (between 20 and 600 seconds) for connections to their [custom language model provider](https://api.vapi.ai/api#:~:text=CustomLLMModel) using the new `timeoutSeconds` property. This enhancement allows for better control over response waiting times, accommodating longer operations or varying network conditions.


 This is the content for the doc fern/changelog/2025-03-19.mdx 

 
# Test Suite, Smart Endpointing, and Compliance Plans, Chat Completion Message Workflows, and Voicemail Detection

1. **Test Suite Enhancements**: Developers can now define `targetPlan` and `testerPlan` when creating or updating [test suites](https://api.vapi.ai/api#:~:text=TestSuite), allowing for customized testing configurations without importing phone numbers to Vapi.

2. **Smart Endpointing Updates**: You can now select between [`Vapi`](https://api.vapi.ai/api#:~:text=VapiSmartEndpointingPlan) and [`Livekit`](https://api.vapi.ai/api#:~:text=LivekitSmartEndpointingPlan) smart endpointing providers using the `Assistant.startSpeakingPlan.smartEndpointingPlan`; the `customEndpointingRules` property is deprecated and should no longer be used.

3. **Compliance Plan Enhancements**: Organizations can now specify compliance settings using the new `compliancePlan` property, enabling features like PCI compliance at the org level.

4. **Chat Completion Message Updates**: When working with OpenAI chat completions, you should now use [`ChatCompletionMessageWorkflows`](https://api.vapi.ai/api#:~:text=ChatCompletionMessageWorkflows) instead of the deprecated `ChatCompletionMessage`.

5. **Voicemail Detection Defaults Updated**: The default `voicemailExpectedDurationSeconds` for voicemail detection plans has increased from 15 to 25 seconds, affecting how voicemail detection timings are handled.

 This is the content for the doc fern/changelog/2025-03-20.mdx 

 # Introducing Google Calendar Integration, and Chat Test Suite / Rime AI Voice Enhancements

1. **Integration with Google Calendar**: You can now create and manage Google Calendar events directly within your tools. Configure OAuth2 credentials through the [dashboard > Build > Provider Keys](https://dashboard.vapi.ai/keys#:~:text=Google%20Calendar) to authenticate and interact with Google Calendar APIs.

<Frame caption="Google Calendar Integration">
    <img src="../static/images/changelog/google-calendar-tool.png" alt="Google Calendar Integration" />
</Frame>

2. **Enhanced Voice Customization for RimeAIVoice**: Gain more control over [Rime AI voice](https://api.vapi.ai/api#:~:text=RimeAIVoice) properties with new options like `reduceLatency`, `inlineSpeedAlpha`, `pauseBetweenBrackets`, and `phonemizeBetweenBrackets`. These settings let you optimize voice streaming and adjust speech delivery to better suit your assistant's needs.

3. **Chat Test Suite Enhancements**: You can now create and run chat-based tests in your test suites using the new [`TestSuiteTestChat`](https://api.vapi.ai/api#:~:text=TestSuiteTestChat) to more comprehensively test conversational interactions in your assistant.

4. **Maximum Length for Test Suite Chat Scripts**: When creating or updating chat tests, note that the `script` property now has a maximum length of 10,000 characters. Ensure your test scripts conform to this limit to avoid any validation errors.

 This is the content for the doc fern/changelog/2025-03-21.mdx 

 
1. **OpenAI Voice Enhancements**: When using [OpenAI Voice models in `Assistant.voice`](https://api.vapi.ai/api#:~:text=OpenAIVoice), you can now use specific text to speech models and add custom instructions to control your assistant's voice output

2. **Improved Call Error Reporting**: You can now use new [`Call.endedReason`](https://api.vapi.ai/api#:~:text=Call,-CallBatchError) codes when a call fails to start or ends unexpectedly due to failing to retrieve Vapi objects. Refer to [Call.endedReason](https://api.vapi.ai/api#:~:text=Call,-CallBatchError) for more details.

 This is the content for the doc fern/changelog/2025-03-22.mdx 

 
1. **Customizable Background Sound**: You can now use a custom audio file as the background sound in calls by providing a URL in the `backgroundSound` property. This allows you to enhance the call experience with personalized ambient sounds or music.

2. **New Recording Format Options in `ArtifactPlan`**: You can specify the recording format as either `'wav;l16'` or `'mp3'` in `Assistant.artifactPlan` or `Call.artifactPlan`. This gives you control over the audio format of call recordings to suit your storage and playback preferences.

3. **Integrate with Langfuse for Enhanced Observability**: You can now integrate with Langfuse by setting `assistant.observabilityPlan` to `langfuse`. Add `tags` and `metadata` to your traces to improve monitoring, categorization, and debugging of your application's behavior.

 This is the content for the doc fern/changelog/2025-03-23.mdx 

 1. **Multi-Structured Data Extraction with `StructuredDataMultiPlan`:** You can now extract multiple sets of structured data from calls by configuring `assistant.analysisPlan.structuredDataMultiPlan`. This allows you to define various extraction plans, each producing structured outputs accessible via `call.analysis.structuredDataMulti`.

2. **Customizable Voice Speed and Language Settings:** You can now adjust the speech speed and language for your assistant's voice by using the new `speed` and `language` properties in `Assistant.voice`. This enables you to fine-tune the voice output to better match your user's preferences and localize the experience.

3. **Integration of OpenAI Transcriber:** The `transcriber` property in assistants now supports `OpenAITranscriber`, allowing you to utilize OpenAI's transcription services. A corresponding `Call.endedReason` value, `pipeline-error-openai-transcriber-failed`, has been added to help you identify when a call ends due to an OpenAI transcriber error.

 This is the content for the doc fern/changelog/2025-03-27.mdx 

 1. **Batch Call Operations**: You can now place multiple calls to different customers at once by providing a list of `customer`s as an array in [`POST /call`](https://api.vapi.ai/api#/Calls/CallController_create).

2. **Google Sheets Row Append Tool Added**: You can now append rows to Google Sheets directly from your assistant using [`GoogleSheetsRowAppendTool`](https://api.vapi.ai/api#/Tools/GoogleSheetsRowAppendTool). This allows integration with Google Sheets via the API for automating data entry tasks.

3. **Call Control and Scheduling**: You can now schedule calls using the new `SchedulePlan` feature, specifying earliest and latest times for calls to occur. This gives you more control over call timing and scheduling.

4. **New Transcriber Options and Fallback Plans**: New transcribers like `GoogleTranscriber` and `OpenAITranscriber` have been added, along with the ability to set `fallbackPlan` for transcribers. This provides more choices and reliability for speech recognition in your applications.

 This is the content for the doc fern/changelog/2025-03-28.mdx 

 1. **New Slack and Google Calendar Tools Added**: You can now use the built-in  [Slack tool](https://docs.vapi.ai/tools/slack) to send messages and use the [Google Calendar tool](https://docs.vapi.ai/tools/google-calendar) to check calendar availability directly from your assistant, with full CRUD operations available via the [`/tool` API endpoint](https://docs.vapi.ai/api-reference/tools/list). You can authenticate the [Slack tool](https://dashboard.vapi.ai/keys#:~:text=Slack) and the [Google Calendar tool](https://dashboard.vapi.ai/keys#:~:text=Google%20Calendar) using OAuth2 from the [Vapi provider keys page](https://dashboard.vapi.ai/keys).

<Frame caption="Slack Tool">
    <img src="../static/images/changelog/slack-tool.png" alt="Slack Tool" />
</Frame>

<Frame caption="Google Calendar Tool">
    <img src="../static/images/changelog/google-calendar-tool.png" alt="Google Calendar Tool" />
</Frame>

2. **Select LLM Model in Workflow Nodes**: You can now select and update which LLM model you want to use within workflow nodes, allowing more precise control over the assistant's behavior in different workflow nodes and easier configuration updates.

4. **Enhanced Call Monitoring and Reporting**: We've improved call monitoring with conversation turn tracking, millisecond-precision timestamps, and provided more detailed call end reasons. These enhancements make it easier to track conversation flow, perform precise time calculations, and diagnose specific call termination issues like server overloads or database errors.

5. **Enable Background Denoising**: You can now filter out background noise during calls by setting `Assistant.backgroundDenoisingEnabled` to `true`.

 This is the content for the doc fern/changelog/2025-03-30.mdx 

 1. **TestSuiteRunTestAttempt now accepts `callId` and `metadata`**: You can now include a `callId` and `metadata` when creating a test suite run attempt, allowing you to reference calls by ID and attach session-related information.

2. **`call` property in [TestSuiteRunTestAttempt](https://api.vapi.ai/api#:~:text=TestSuiteRunTestAttemptMetadata) is no longer required**: It's now optional to include the full `call` object in a test attempt, providing flexibility for cases where call details are unnecessary or already known.

3. **Attach Metadata to Test Suite Run Attempts**: You can now attach [metadata](https://api.vapi.ai/api#:~:text=TestSuiteRunTestAttemptMetadata) like `sessionId` to test attempts for better tracking and analysis. 


 This is the content for the doc fern/changelog/2025-04-03.mdx 

 1. **Introducing `SmsSendTool` for SMS messaging support**: You can now create and manage tools of type `sms` using the new [SMS Send Tool](https://api.vapi.ai/api#:~:text=SmsSendTool), allowing you to send SMS messages via defined servers. The `sms` tool type is also now recognized in API endpoints, ensuring that SMS send tools are correctly processed during CRUD operations.

2. **New configuration options for voice and transcriber settings**: The `autoMode` property has been added to [Eleven Labs Voice Settings](https://api.vapi.ai/api#:~:text=ElevenLabsVoice), letting developers control automatic voice settings. Additionally, `confidenceThreshold` has been introduced in transcriber settings, allowing developers to set thresholds to discard low-confidence transcriptions and improve accuracy.

3. **Enhanced speed control in `CartesiaExperimentalControls`**: The `speed` property now accepts both predefined speeds (`'slowest'`, `'slow'`, `'normal'`, `'fast'`, `'fastest'`) and numeric values between -1 and 1. This gives you more precise control over speed settings for better customization.

 This is the content for the doc fern/changelog/2025-04-04.mdx 

 1. **Addition of `assistantId` to `TargetPlan` settings**: You can now specify an `assistantId` when testing [target plans](https://api.vapi.ai/api#:~:text=TargetPlan), allowing you to test scenarios involving specific assistants directly.

 This is the content for the doc fern/changelog/2025-04-05.mdx 

 1. **Introducing `SmsSendTool` for SMS messaging support**: You can now create and send `sms` text messages  using the new `[Send Text`](https://api.vapi.ai/api#:~:text=SmsSendTool) tool, enabling assistants to send SMS messages via defined servers. 

<Frame caption="Send SMS with 'Send Text' tool">
    <img src="../static/images/changelog/send-text-tool.png" alt="SmsSendTool" />
</Frame>

2. **[Eleven Labs Voice](https://api.vapi.ai/api#:~:text=ElevenLabsVoice) Auto Mode and Confidence Threshold configuration options**: When using [`Eleven Labs Voice`](https://api.vapi.ai/api#:~:text=ElevenLabsVoice) in your Assistant, you can now configure `autoMode` (default: false) to automatically manage manage chunking strategies for long texts; Eleven Labs automatically determines the best way to process and generate audio, optimizing for latency and efficiency. Additionally, `confidenceThreshold` has been introduced in transcriber schemas, allowing developers to set thresholds to discard low-confidence transcriptions and improve accuracy.

3. **Changes to `CartesiaExperimentalControls` Speed property**: The `speed` property now accepts both predefined speeds (`'slowest'`, `'slow'`, `'normal'`, `'fast'`, `'fastest'`) and numeric values between -1 and 1. This simplifies the process of controlling the speed of the generated audio with Cartesia.

 This is the content for the doc fern/changelog/2025-04-08.mdx 

 1. **Simplified `transport` property in `Call` configuration**: You should now configure the `transport` property in [`Call`](https://api.vapi.ai/api#:~:text=Call) as an object when creating or updating a [`Call`](https://api.vapi.ai/api#:~:text=Call), since the separate `Transport` schema has been deprecated. This simplification makes it easier to work with transport details without referencing a separate transport configuration.

<Warning>
    The `Transport` schema is now deprecated and will be removed in a future release.
</Warning>

2. **New call type `vapi.websocketCall`**: You can now make [phone calls over WebSockets](https://docs.vapi.ai/calls/websocket-transport) with Vapi. The `Call` schema now supports a new `type` value: `vapi.websocketCall`.

 This is the content for the doc fern/changelog/2025-04-11.mdx 

 1. **Updated AI Edge Condition with Prompt**: When defining an AI edge condition, the `matches` property has been renamed to `prompt`. The `prompt` allows you to provide a natural language condition (up to 1000 characters) that guides AI decision-making in workflows.

<Frame caption="Updated AI Edge Condition with Prompt">
    <img src="../static/images/changelog/ai-edge-condition-prompt.png" alt="AI Edge Condition with Prompt" />
</Frame>

2. **Assistant Overrides per Customer**: You can now customize assistant settings for individual customers using `assistantOverrides` when [creating customers](https://api.vapi.ai/api#:~:text=CreateCustomerDTO). This enables personalized assistant interactions for each customer in batch calls.

3. **New Call Ended Reasons**: New error codes have been added to `endedReason` enums, providing more detailed insights into call terminations related to providers like Anthropic Bedrock and Vertex. This helps in better error handling and debugging of call issues.

 This is the content for the doc fern/changelog/2025-04-12.mdx 

 
1. **Expanded Voice Selection for Assistant Voices**: You can now specify any valid `voiceId` for assistant voices without being limited to a predefined list. This provides greater flexibility to use different voices in `Assistant.voice`, and related configurations.

 This is the content for the doc fern/changelog/2025-04-15.mdx 

 1.  **New GPT-4.1 Models Available**: You can now use `'gpt-4.1'`, `'gpt-4.1-mini'`, and `'gpt-4.1-nano'` as options for the `model` and `fallbackModels` with your [OpenAI models](https://api.vapi.ai/api#:~:text=OpenAIModel). These models may offer improved performance or features over previous versions.

<Frame caption="New GPT-4.1 Models Available">
    <img src="../static/images/changelog/gpt-4.1-models.png" alt="New GPT-4.1 Models Available" />
</Frame>


 This is the content for the doc fern/changelog/2025-04-16.mdx 

 1. **Assistant Overrides in Testing (`TargetPlan.assistantOverrides`)**: You can now apply `assistantOverrides` when testing an assistant with a [Target Plan](https://api.vapi.ai/api#:~:text=TargetPlan), allowing modifications to the assistant's configuration specifically for tests without changing the original assistant. This helps in testing different configurations or behaviors of an assistant without affecting the live version.

2. **Specify Voice Model with Deepgram**: You can now specify the `model` to be used by Deepgram voices by setting the `model` property to `"aura"` or `"aura-2"` (default: `"aura-2"`).

3. **Expanded Deepgram Voice Options (`voiceId` in `DeepgramVoice` and `FallbackDeepgramVoice`)**: The list of available deepgram voice options has been greatly expanded, providing a wider selection of voices for assistants. This allows you to customize the assistant's voice to better match your desired persona with `Assistant.voice["DeepgramVoice"].voiceId`.

<Frame caption="Expanded Voice Options">
    <img src="../static/images/changelog/deepgram-voices.png" alt="Expanded Deepgram Voice Options" />
</Frame>


4. **Control Text Replacement Behavior (`replaceAllEnabled` in `ExactReplacement`)**: A new property `replaceAllEnabled` allows you to decide whether to replace all instances of a specified text (`key`) or just the first occurrence in [`ExactReplacement`](https://api.vapi.ai/api#:~:text=ExactReplacement) configurations. Setting `replaceAllEnabled` to `true` ensures that all instances are replaced.

 This is the content for the doc fern/changelog/2025-04-17.mdx 

 **1. **Custom Hooks When a Call is Ringing**: You can now define custom hooks on your phone numbers to automatically perform actions when a call is ringing. This enables you to play messages or transfer calls without additional server-side code by using the new `hooks` property in `Call.phoneNumber.hooks["phoneNumberHookCallRinging"]`. 

**2. **Say and Transfer Actions in Hooks**: The new [phone number hook call ringing](https://api.vapi.ai/api#:~:text=PhoneNumberHookCallRinging) allows you to specify actions that trigger when a call is ringing (`on: 'call.ringing'`). like [redirecting calls](https://api.vapi.ai/api#:~:text=TransferPhoneNumberHookAction) or [playing a message](https://api.vapi.ai/api#:~:text=SayPhoneNumberHookAction). Include these actions in the `do` array of your hook.

**3. **Enhanced Call Tracking with endedReason**: When implementing call analytics, you can now track calls that ended due to hook actions through new `endedReason` values: 
- `'call.ringing.hook-executed-say'`: Call ended after playing a message via hook
- `'call.ringing.hook-executed-transfer'`: Call ended after being transferred via hook
These values let you distinguish between different automated call handling outcomes in your reporting.

 This is the content for the doc fern/changelog/2025-04-18.mdx 

 1. **Idle Message Count Reset in `Assistant.messagePlan`**: You can now enable `Assistant.messagePlan.idleMessageResetCountOnUserSpeechEnabled` (default: false) to allow the idle message count to reset whenever the user speaks. This means the assistant can repeatedly remind an idle user throughout the conversation.


 This is the content for the doc fern/changelog/overview.mdx 

 ---
slug: changelog
---
<Card
  title={<div style={{ display: 'flex', alignItems: 'center', gap: '0.5rem', cursor: 'pointer', color: 'inherit' }} onClick={() => document.querySelector('input[type="email"]').focus()}>Get the (almost) daily changelog</div>}
  icon="envelope"
  iconType="solid"
>
  <form
    method="POST"
    action="https://customerioforms.com/forms/submit_action?site_id=5f95a74ff6539f0bc48f&form_id=01jk7tf2khhf5satn62531qe25&success_url=https://docs.vapi.ai/changelog"
    className="subscribe-form"
    style={{margin: '1rem 0'}}
    onSubmit={(e) => {
      const emailInput = document.getElementById('email_input');
      const emailValue = emailInput.value;
      const emailPattern = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;
      if (!emailPattern.test(emailValue)) {
        e.preventDefault();
        alert('Please enter a valid email address.');
      }
    }}
  >
    <div className="flex gap-2" style={{ paddingLeft: '0.5rem' }}>
      <label htmlFor="email_input" style={{ display: 'none' }}>E-mail address</label>
      <input
        id="email_input"
        type="email"
        name="email"
        placeholder="Enter your email"
        required
        style={{
          border: '1px solid #e2e8f0',
          borderRadius: '0.375rem',
          padding: '0.5rem 1rem',
          width: '100%',
          fontSize: '0.875rem',
          outline: 'none',
          transition: 'border-color 0.2s ease-in-out',
          color: '#1f2937',
          ':focus': {
            borderColor: '#4f46e5',
            boxShadow: '0 0 0 1px #4f46e5'
          },
          '@media (prefers-color-scheme: dark)': {
            backgroundColor: '#374151',
            borderColor: '#4b5563',
            color: '#f3f4f6',
            '::placeholder': {
              color: '#9ca3af'
            },
            ':focus': {
              borderColor: '#6366f1',
              boxShadow: '0 0 0 1px #6366f1'
            }
          }
        }}
      />
      <button
        type="submit"
        style={{
          backgroundColor: '#37aa9d',
          color: 'white',
          fontWeight: 500,
          padding: '0.5rem 1rem',
          borderRadius: '0.375rem',
          border: 'none',
          cursor: 'pointer',
          transition: 'all 0.2s ease-in-out',
          ':hover': {
            backgroundColor: '#2e8b7d',
            transform: 'translateY(-1px)'
          },
          ':active': {
            transform: 'translateY(0)'
          },
          '@media (prefers-color-scheme: dark)': {
            backgroundColor: '#94ffd2',
            color: '#1f2937',
            ':hover': {
              backgroundColor: '#7cd9b0'
            }
          }
        }}
      >
        Submit
      </button>
    </div>
  </form>
</Card>

 This is the content for the doc fern/community/appointment-scheduling.mdx 

 ---
title: Appointment Scheduling
subtitle: Videos showcasing Vapi out in the wild.
slug: community/appointment-scheduling
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
    <iframe
        src="https://www.youtube.com/embed/F57M9ljJkMU?si=wCtu42Jzu6fC6th_"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/kDX9JlPxGzE?si=XBxwUcskqGliBBSX"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/5p6rML68cKE?si=0wHuyQCQFHvkRk66"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/FMYc0KRYRPM?si=72tA_8NiYjkkLNYS"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/uqBG7nf1vZk?si=hH8UooV1l6hIvNXG"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />


<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/hCJb11EOdME?si=nMmUXuOnT6psbNBP"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>
<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/fS3OIrAHWwQ"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>
<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/2btAigfVxIE"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>

<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/y-GMretSYEw?si=xt_IdlLbRxXazCvB"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>

<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/p-Ns7kgZALk?si=6pkNV2zWo-TrwxV-"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>

<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/hPtvSolWJgU?si=4CcU3TLDPe3AnhXW"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>

<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/hwneiBSbC4k?si=Z_YOVnXQw3YaExdj"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>

  </div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/comparisons.mdx 

 ---
title: Comparisons
subtitle: Videos showcasing Vapi out in the wild.
slug: community/comparisons
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
  <iframe
    src="https://www.youtube.com/embed/KloYd6cANkM?si=ssM9ouDeCeyFe1hv"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.youtube.com/embed/iEi_zion9jM?si=J5Ik5GJ24SUt-PDr&amp;start=19"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.youtube.com/embed/5pKInI77zNk?si=N7Xdit2mTN7kbm6o&amp;start=1508"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.youtube.com/embed/nykm4h0gFV4?si=5mlDF4uGRwKbNn5l"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.youtube.com/embed/hO6hoJGheLA?si=onLVeZhYWmo1hG3B"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/rc0XGjI4QKM"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />

  <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/Cp_lmp_05ww?si=jQhA1y0vnuuQseBR"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  
  <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/7K8vSheVnic?si=pa1VVij50Q6Znrac"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  
  <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/GMCDPIxVIPg?si=OHgOrNn9QBZHAdNh"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  </div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/conferences.mdx 

 ---
title: Conferences
subtitle: Videos showcasing Vapi out in the wild.
slug: community/conferences
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
  <iframe
    src="https://www.youtube.com/embed/jag7NjaROck?si=OLFbkgF9YDBw0ufs&amp;start=415"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
</div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/demos.mdx 

 ---
title: Demos
subtitle: Videos showcasing Vapi out in the wild.
slug: community/demos
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
  <iframe
    src="https://www.youtube.com/embed/H6qym392wFg?si=GC2anHDMMbPcG7xF"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />

  <iframe
    src="https://www.youtube.com/embed/Gda0Le__n8g?si=jNEMbdr7WIbf_PQk"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />

</div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/expert-directory.mdx 

 ---
title: Expert Directory
subtitle: Certified Voice AI Expert - Vapi
slug: community/expert-directory
---


Want to maximize your Voice AI? Vapi, a certified consultant, specializes in building Voice AI bots.

Whether you need help deciding what to automate or assistance in building it, Vapi Experts have proven their expertise by supporting users and creating valuable video content for the community. Find the right fit here.

<div class="video-grid">
<Card
  href="https://www.qonvo.co/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://i.imgur.com/ZkhLOBl.png"
  />
  <div className="card-content">
    <h3>Qonvo</h3>
    <p>Qonvo is the best way to stop wasting your time on the phone for repetitive tasks and low-value added inbound requests. Allow your self to better invest your time thanks custom-build Vocal AI agents.</p>
  </div>
</Card>

<Card
  href="https://www.6omb.ai/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://media.licdn.com/dms/image/v2/C560BAQGqQcUUy_uaxg/company-logo_200_200/company-logo_200_200/0/1630629573081?e=2147483647&v=beta&t=6jjsYZqRd9p7K_IBntckGVkVfMm51PxMy060U2AP3dA"
  />
  <div className="card-content">
    <h3>6omb</h3>
    <p>Voice agents and custom product development.</p>
  </div>
</Card>

<Card
  href="https://www.aitoflo.com/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://i.ibb.co/ckdM9nK/aitoflo-b200x-p-1080.png"
  />
  <div className="card-content">
    <h3>Aitoflo</h3>
    <p>
      At Aitoflo, we specialize in Voice AI and RPA services, seamlessly flowing
      to streamline business operations and enhance customer interactions with
      Realistic Voice AI.
    </p>
  </div>
</Card>

<Card
  href="https://amplifyvoice.ai/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://cdn.prod.website-files.com/667a2d87749c53c580ad32de/668cca9c1617b98b5375ad1e_AMPLIFAI%20(8).png"
  />
  <div className="card-content">
    <h3>Amplify Voice</h3>
    <p>
      Our hyper-focus on User Experience will WOW your customers. Click to Book
      a Strategy Session.
    </p>
  </div>
</Card>

<Card
  href="https://arose.ai/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://i.imgur.com/FQvbwsZ.png"
  />
  <div className="card-content">
    <h3>Arose AI</h3>
    <p>
      Arose AI creates custom Inbound Voice AI solutions for small businesses.
      Our founder Tommy Chryst also provides 1-on-1 coaching.
    </p>
  </div>
</Card>

<Card
  href="https://aiplaygrounds.xyz"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://i.imgur.com/nXXWCIc.png"
  />
  <div className="card-content">
    <h3>AIP</h3>
    <p>
      Created debt collector, appointment book, customer service, website
      assistant, etc. I, Valentino M., offer full integration and/or
      consultation services.
    </p>
  </div>
</Card>

<Card
  href="https://boldwaveagency.com/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://images.leadconnectorhq.com/image/f_webp/q_80/r_1200/u_https://assets.cdn.filesafe.space/y30mqCqdNhLjPaC2MefY/media/66711e425d2409e431102e40.svg"
  />
  <div className="card-content">
    <h3>Boldwave</h3>
    <p>
      We implement voice agents to streamline appointment booking, enhance lead
      conversion, and provide superior 24/7 customer service.
    </p>
  </div>
</Card>

<Card
  href="https://www.brisklogic.co/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://www.brisklogic.co/wp-content/uploads/2022/03/brisk-logic-pvt-ltd-logo.png"
  />
  <div className="card-content">
    <h3>Brisk Logic </h3>
    <p>
      We are a AI Automation Agency that specializes in designing advanced AI
      voice assistants capable of automating various tasks through phone
      calls.{" "}
    </p>
  </div>
</Card>

<Card
  href="https://cold-calls.ai/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="http://cold-calls.ai/wp-content/uploads/2024/07/Cold-Calls-AI-logo.png"
  />
  <div className="card-content">
    <h3>Cold-Calls.AI</h3>
    <p>
      We specialize in helping companies within the German market integrate
      Voice Agents for both inbound and outbound calls.
    </p>
  </div>
</Card>

<Card
  href="https://dontrunoff.com"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://dontrunoff.podia.com/content-assets/public/eyJhbGciOiJIUzI1NiJ9.eyJvYmplY3Rfa2V5IjoiZGNkNmgwZTRwbm9veW5nM3lqYWc1NW5hcHYwMiIsImRvbWFpbiI6ImRvbnRydW5vZmYucG9kaWEuY29tIn0.yrT0vbilNFZX_MYiQqxflIjQq_bncWA0I-bHkZPZ1VU"
  />
  <div className="card-content">
    <h3>Don't Run Off AI</h3>
    <p>Telephone voice AI systems, prompt engineering, integrations</p>
  </div>
</Card>

<Card
  href="https://flowzen.ai/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://uploads-ssl.webflow.com/66c61bce25dde805b35e3396/66c6b40ff7af9a4b1457422b_Logo-1%403x-100.jpg"
  />
  <div className="card-content">
    <h3>Flowzen</h3>
    <p>
      Our agency offers Voice AI solutions using Vapi, in English and Spanish,
      integrated with platforms like GoHighLevel, Airtable, and Make.com.
    </p>
  </div>
</Card>

<Card
  href="https://globeai-s96libq.gamma.site/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://i.imgur.com/m8kzdXK.png"
  />
  <div className="card-content">
    <h3>Globe AI</h3>
    <p>
      I'm Aryan, founder of Globe AI. We build Inbound Voice Assistants for any
      industry at any scale.
    </p>
  </div>
</Card>

<Card
  href="https://inflate.agency/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://cdn.prod.website-files.com/66150d67f058b33ff02872c9/6620a7cd66604e301711d1ab_transpng-p-500.png"
  />
  <div className="card-content">
    <h3>INFLATE AI Automation Development Services</h3>
    <p>Building voice systems for any industry from $3k USD minimum.</p>
  </div>
</Card>

<Card
  href="https://integraticus.com/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://integraticus.com/wp-content/uploads/2024/07/integraticus-ai-voice-agents.png"
  />
  <div className="card-content">
    <h3>Integraticus</h3>
    <p>
      We build AI appointment setters for Real Estate Agencies to qualify more
      leads, handle tailored outreach, and make sure you have higher margins
      than ever before.
    </p>
  </div>
</Card>

<Card
  href="https://klen.ai"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://drq7swnhvxeif.cloudfront.net/static/assets/media/logos/Klen-Light.svg"
  />
  <div className="card-content">
    <h3>Klen AI</h3>
    <p>
      Custom AI voice assistants to handle calls, pre-qualify leads, schedule
      appointments, etc. Utilizing Vapi for seamless integration and
      productivity
    </p>
  </div>
</Card>

<Card
  href="https://lunarisai.com"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://i.imgur.com/d4jMx0p.png"
  />
  <div className="card-content">
    <h3>Lunaris AI</h3>
    <p>We create Voice Agents for all types of businesses.</p>
  </div>
</Card>
<Card
  href="https://www.martech-studios.com/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://cdn.prod.website-files.com/663d0336b03ec97c27390229/671fb9c4b8367085d13c42d0_Martech_Logo.jpg"
  />
  <div className="card-content">
    <h3>Martech Studios</h3>
    <p>
      We are a US based B2B tech and marketing consultancy focused on building superior user experiences. We also love all things Audio AI (TTS)
    </p>
  </div>
</Card>
<Card
  href="https://www.upwork.com/freelancers/~01790044d9e078fd70"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://assets.static-upwork.com/org-logo/1809692048083431424?date=1721430511424"
  />
  <div className="card-content">
    <h3>NukyLabs.AI</h3>
    <p>All Services for Vapi.ai Automation </p>
  </div>
</Card>

<Card
  href="https://otakusolutions.io/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://storage.googleapis.com/msgsndr/dBaRxB72IyESkQWGMkmq/media/669a84c7b9609d1964adbae7.jpeg"
  />
  <div className="card-content">
    <h3>Otaku Solutions</h3>
    <p>
      Handle the creation of voice assistants, automations, tracking, and
      training.
    </p>
  </div>
</Card>

<Card
  href="https://shadow-ai.co/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://i.imgur.com/5Tt41iJ.png"
  />
  <div className="card-content">
    <h3>Shadow AI</h3>
    <p>
      Specializing in AI-powered inbound and outbound calling operations for all
      types of businesses using industry expertise.
    </p>
  </div>
</Card>

<Card
  href="https://synthiq.io/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://lh3.googleusercontent.com/u/0/drive-viewer/AKGpihYpbDmAlKpgyRePNNsCrR08AUFfqRavaJfDxG-BGicVkIe958Ot8dctBsAg1I_KN-mVPdXgWZ6TcqQWFy3s7AWel2jZKAVHZgk=w2880-h1626"
  />
  <div className="card-content">
    <h3>Synthiq</h3>
    <p>
      Multilingual AI Voice Agents (20+ countries). Any industry. Use your
      existing number. Expert AI consulting available.
    </p>
  </div>
</Card>

<Card
  href="Https://www.TemporalLab.com"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://assets.zyrosite.com/cdn-cgi/image/format=auto,w=450,fit=crop,q=95/A1aoblXx2KSKGq4r/tlclogorawnameonly-Awvk565gvMfLKVr4.png"
  />
  <div className="card-content">
    <h3>Temporal Labs LLC </h3>
    <p>
      Temporal Labs LLC offers a unique solution through our parametrized,
      development community, with industry specific solutions and engagement.
    </p>
  </div>
</Card>

<Card
  href="https://vatech.io"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://cdn.prod.website-files.com/662a910cb8589f49140e52f6/662a923b33ada78dc4b8946a_6141bc0adf30793a0a262fd8_Group%2013.png"
  />
  <div className="card-content">
    <h3>Value Added Tech</h3>
    <p>
      Top-notch automation company. We specialise in Make.com (Silver partner),
      multiple CRMs and Vapi.
    </p>
  </div>
</Card>

<Card
  href="https://saidwell.com/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://framerusercontent.com/images/3DwoBZqefGrK3gwbQZ5zYgTZG8.png"
  />
  <div className="card-content">
    <h3>Saidwell</h3>
    <p>
    Saidwell develops custom voice AI solutions for enterprises, offering bespoke software and high-quality human voice models.
    </p>
  </div>
</Card>
<Card
  href="https://www.iffort.ai/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://i.imgur.com/M6z8aS9.png"
  />
  <div className="card-content">
    <h3>iffort.ai</h3>
    <p>
  We revolutionize your business communication with our conversational agents, turning traditional chats and calls into effortless conversations.
    </p>
  </div>
</Card>
<Card
  href="https://www.msquare.pro/"
  className="ed-card"
  style={{
    alignItems: "center",
    display: "flex",
    flexDirection: "column",
    width: "auto",
  }}
>
  <img
    className="card-img bg:white"
    noZoom
    src="https://static.wixstatic.com/media/cfe1a7_b6db3651b9cc43ee9d64edb1bfa674c0~mv2.png/v1/crop/x_520,y_485,w_5293,h_1131/fill/w_428,h_92,al_c,q_95,enc_auto/M%20square%20Logo_RGB-01.png"
  />
  <div className="card-content">
    <h3>Msquare Automation</h3>
    <p>
  Experts in AI voice assistants and business automation. Affordable, quality service from India. Gold partners of Make.com
  </p>
  </div>
</Card>
</div>


 This is the content for the doc fern/community/ghl.mdx 

 ---
title: GoHighLevel
subtitle: Videos showcasing Vapi out in the wild.
slug: community/ghl
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
    <iframe
        src="https://www.youtube.com/embed/PVP1P2nak4M?si=vGGAMZVI3Fzzik9X"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />

  <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/DpnC8NX4tas"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  </div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/guide.mdx 

 ---
title: Guide
subtitle: Videos showcasing Vapi out in the wild.
slug: community/guide
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
    <iframe
        src="https://www.youtube.com/embed/NyZ0AYdTKVw?si=F-2ZxlOpC5rxv7Wh"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/3LSXJECXpkc?si=hhWsXZeFYC6wM-cq"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/cuQI3UH2lDE?si=chPXGUvgN7FoQQFb"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/RMOHpWAPan8?si=44FZKzxfxxXndR3i"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/HuF7ELckcyU?si=PPPFZE5aiI-WgP2U"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/jg1j1Rv8SkA?si=V9IZJObqPF6-iXSS"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/u-8xdblVY_4?si=_8YYFR44n3SevrSb"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/jDPXWMUVUPE?si=FrNKyc2oTZIuNysu"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/AH5oS2w3EcQ?si=PrskTE6KXHA0i9v2"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/gN6CsDtLnMs?si=SkGG6LzS9wIXrh_-"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/3y7GJ9Vozkk?si=0lBabiKaMjMAOz8Y"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/w7yHxLSvXnU?si=LNsPkkk0K5F2ccy8"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/QzO22FT2W9Q?si=JuLipGu1spTZY3pM"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/STzxtEf8Fu0?si=mCtn52wCM8m8eIfo"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/J24SNZNjB6o?si=k1vi5RsYTpzzuzjp"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/iWZD_HBk9zQ?si=ngKqJu4NIecL9TTT"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />

<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/nyzSRnuPAS8?si=w89PVdgvenwFbkZj"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>
<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/EFJoQ6wwf9I?si=0CnSsAcW1zNshh01"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>
<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/FQjWcxfIo2g"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>

    <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/2VA2mvzugts"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen

/>

<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/Nj5nirt_m6c"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>

{" "}
<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/Nj5nirt_m6c?si=tXeyFGEOwEVM3W_7"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>
<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/INQC85uTID4?si=jjHc8bzk0S6agfzZ"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>
<iframe width="100%" height="315" src="https://www.youtube.com/embed/sricQothccU?si=5ZdC9RZVrILIE4YV" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen />

    <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/_pawGoj0Re8?si=zJDTyY25LKlCvITl"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen

/>

  </div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/inbound.mdx 

 ---
title: Inbound
subtitle: Videos showcasing Vapi out in the wild.
slug: community/inbound
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
    <iframe
        src="https://www.youtube.com/embed/ai32iXHj8fc?si=z8PKMD8Dklpg_j0B"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />

<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/mPC-YOmidqE"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>

    <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/xsDc8ALGaeE"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen

/>

<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/NCjdEREIyR8?si=bKiyp65Qisbbq_4r"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>

<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/Kg1sOISqKiE?si=kdpFMfFq6w13c__Z"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>

  </div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/knowledgebase.mdx 

 ---
title: Creating Custom Knowledge Bases for Your Voice AI Assistants
subtitle: >-
  Learn how to create and integrate custom knowledge bases into your voice AI
  assistants.
slug: knowledgebase
---

## **What is Vapi's Knowledge Base?**

A Knowledge Base is a collection of custom files that contain information on specific topics or domains. By integrating a Knowledge Base into your voice AI assistant, you can enable it to provide more accurate and informative responses to user queries. This is currently available in Vapi via the API, and will be on the dashboard soon.

### **Why Use a Knowledge Base?**

Using a Knowledge Base with your voice AI assistant offers several benefits:

- **Improved accuracy**: By integrating custom files into your assistant, you can ensure that it provides accurate and up-to-date information to users.
- **Enhanced capabilities**: A Knowledge Base enables your assistant to answer complex queries and provide detailed responses to user inquiries.
- **Customization**: With a Knowledge Base, you can tailor your assistant's responses to specific domains or topics, making it more effective and informative.

## **How to Create a Knowledge Base**

To create a Knowledge Base, follow these steps:

### **Step 1: Upload Your Files**

Navigate to Platform > Files and upload your custom files in Markdown, PDF, plain text, or Microsoft Word (.doc and .docx) format to Vapi's Knowledge Base.

<Frame caption="Adding files to your Knowledge Base">
  <img
    src="../static/images/knowledge-base/files.png"
    alt="Adding files to your Knowledge Base"
  />
</Frame>

Alternatively you can upload your files via the API.

```bash
curl --location 'https://api.vapi.ai/file' \
--header 'Authorization: Bearer <YOUR_API_KEY>' \
--form 'file=@"<PATH_TO_YOUR_FILE>"'
```

### **Step 2: Create a Knowledge Base**

Use the ID of the uploaded file to create a Knowledge Base along with the KB configurations.

1. Provider: [trieve](https://trieve.ai)

```bash
curl --location 'http://localhost:3001/knowledge-base' \
--header 'Content-Type: text/plain' \
--header 'Authorization: Bearer <YOUR_API_KEY>' \
--data '{
    "name": "v2",
    "provider": "trieve",
    "searchPlan": {
        "searchType": "semantic",
        "topK": 3,
        "removeStopWords": true,
        "scoreThreshold": 0.7
    },
    "createPlan": {
        "type": "create",
        "chunkPlans": [
            {
                "fileIds": ["<FILE_ID_1>", "<FILE_ID_2>"],
                "websites": ["<WEBSITE_1>", "<WEBSITE_2>"],
                "targetSplitsPerChunk": 50,
                "splitDelimiters": [".!?\n"],
                "rebalanceChunks": true
            }
        ]
    }
}'
```

#### Configuration Options

##### Search Plan Options

- **searchType** (required): The search method used for finding relevant chunks. Available options:
  - `fulltext`: Traditional text search
  - `semantic`: Semantic similarity search
  - `hybrid`: Combines fulltext and semantic search
  - `bm25`: BM25 ranking algorithm
- **topK** (optional): Number of top chunks to return. Default varies by implementation
- **removeStopWords** (optional): When true, removes common stop words from the search query. Default: `false`
- **scoreThreshold** (optional): Filters out chunks based on their similarity score:
  - For cosine distance: Excludes chunks below the threshold
  - For Manhattan Distance, Euclidean Distance, and Dot Product: Excludes chunks above the threshold
  - Set to 0 or omit for no threshold

##### Chunk Plan Options

- **fileIds** (optional): Array of file IDs to include in the vector store
- **websites** (optional): Array of website URLs to crawl and include in the vector store
- **targetSplitsPerChunk** (optional): Number of splits per chunk. Default: `20`
- **splitDelimiters** (optional): Array of delimiters used to split text before chunking. Default: `[".!?\n"]`
- **rebalanceChunks** (optional): When true, evenly distributes remainder splits across chunks. For example, 66 splits with `targetSplitsPerChunk: 20` will create 3 chunks with 22 splits each. Default: `true`

### **Step 3: Create an Assistant**

Create a new assistant in Vapi and, on the right sidebar menu. Add the Knowledge Base to your assistant via the PATCH endpoint. Also make sure you customize your assistant's system prompt to utilize the Knowledge Base for responding to user queries.

```bash
curl --location --request PATCH 'https://api.vapi.ai/assistant/<ASSISTANT_ID>' \
--header 'Content-Type: text/plain' \
--header 'Authorization: Bearer <YOUR_API_KEY>' \
--data '{
  "model": {
    "knowledgeBaseId": "<KNOWLEDGE_BASE_ID>",
    "temperature": 0.2,
    "provider": "openai",
    "model": "gpt-4o",
    "messages": [
      {
        "content": "You are a smart assistant who responds to user queries using the information you know, or information supplied by outside context.",
        "role": "system"
      }
    ]
  }
}'
```

## **Best Practices for Creating Effective Knowledge Bases**

- **Organize Your files**: Organize your files by topic or category to ensure that your assistant can quickly retrieve relevant information.
- **Use Clear and concise language**: Use clear and concise language in your files to ensure that your assistant can accurately understand and respond to user queries.
- **Keep your files up-to-date**: Regularly update your files to ensure that your assistant provides the most accurate and up-to-date information.

<Tip>
  For more information on creating effective Knowledge Bases, check out our
  tutorial on [Best Practices for Knowledge Base
  Creation](https://youtu.be/i5mvqC5sZxU).
</Tip>

By following these guidelines, you can create a comprehensive Knowledge Base that enhances the capabilities of your voice AI assistant and provides valuable information to users.


 This is the content for the doc fern/community/myvapi.mdx 

 ---
title: My Vapi
slug: community/myvapi
---


Here is the updated MyVapi User Guide, including the customer endpoints and noting that MyVapi uses 27 out of the 33 available Vapi APIs:
# MyVapi User Guide

Welcome to MyVapi! This guide will help you get started with using MyVapi, your custom GPT, to enhance your productivity and streamline your tasks. Follow the steps below to make the most out of this powerful tool.

## Table of Contents
- [Introduction to MyVapi](#introduction-to-myvapi)
- [Getting Started](#getting-started)
- [Accessing MyVapi](#accessing-myvapi)
- [Using MyVapi](#using-myvapi)
  - [Basic Commands](#basic-commands)
- [Tips and Best Practices](#tips-and-best-practices)
- [Troubleshooting](#troubleshooting)
- [FAQ](#faq)

## Introduction to MyVapi

### What is MyVapi?
MyVapi is a custom GPT designed to allow users to manage their Vapi accounts with ease. While the Vapi Dashboard provides limited functionality and using PostMan can be cumbersome, MyVapi offers a streamlined solution to interact with the Vapi API directly. This eliminates the back-and-forth usually associated with manual API interactions and JSON validation, making the process more efficient and user-friendly. The reason MyVapi was created is to help users understand the power of using Vapi's API. MyVapi uses 27 out of the 33 available Vapi APIs.

### Key Features
- **Full API Access:** Leverage the full power of the Vapi API without the limitations of the Dashboard.
- **Efficient Workflow:** Avoid the tedious back-and-forth of using PostMan and JSON validators.
- **Voice Assistant Creation:** Simplify the process of creating voice assistants with the Vapi API.
- **Troubleshooting:** Get real-time help and troubleshooting advice from ChatGPT.

### Benefits of Using MyVapi
- **Streamlined Management:** Manage your Vapi account more effectively and efficiently.
- **Increased Productivity:** Save time and reduce effort in creating and managing voice assistants.
- **Enhanced Support:** Receive guidance and support directly from ChatGPT to resolve any issues you encounter.

### Additional Information
MyVapi is not connected to a user's account but will help with almost anything you need help with. This includes creating transient assistants, creating tools, getting information about a call, and more.

## Getting Started

### Accessing MyVapi
MyVapi can be accessed in the following ways:
- Visit [https://chatgpt.com/g/g-3luI9WIdj-myvapi](https://chatgpt.com/g/g-3luI9WIdj-myvapi)
- Search for "MyVapi" in the GPT Store

MyVapi is available to both free and paid ChatGPT accounts.

## Using MyVapi

### Basic Commands
MyVapi provides a range of commands to interact with your Vapi account efficiently. Below are the basic commands and their functions:

#### Assistant Management
- **Get Assistants**
  - **Method:** GET
  - **Endpoint:** /assistant
  - **Description:** Retrieve a list of all assistants.

- **Create Assistant**
  - **Method:** POST
  - **Endpoint:** /assistant
  - **Description:** Create a new assistant.

- **Get Assistant by ID**
  - **Method:** GET
  - **Endpoint:** /assistant/{id}
  - **Description:** Retrieve details of a specific assistant using its ID.

- **Update Assistant by ID**
  - **Method:** PATCH
  - **Endpoint:** /assistant/{id}
  - **Description:** Update details of a specific assistant using its ID.

- **Delete Assistant by ID**
  - **Method:** DELETE
  - **Endpoint:** /assistant/{id}
  - **Description:** Delete a specific assistant using its ID.

#### Phone Call Management
- **Get Phone Calls**
  - **Method:** GET
  - **Endpoint:** /call
  - **Description:** Retrieve a list of all phone calls.

- **Get Phone Call by ID**
  - **Method:** GET
  - **Endpoint:** /call/{id}
  - **Description:** Retrieve details of a specific phone call using its ID.

- **Create Phone Call**
  - **Method:** POST
  - **Endpoint:** /call/phone
  - **Description:** Create a new phone call.

- **Update Phone Call by ID**
  - **Method:** PATCH
  - **Endpoint:** /call/{id}
  - **Description:** Update the details of a specific phone call by its ID.

- **Delete Phone Call by ID**
  - **Method:** DELETE
  - **Endpoint:** /call/{id}
  - **Description:** Delete a specific phone call by its ID.

- **Get Call Logs**
  - **Method:** GET
  - **Endpoint:** /log
  - **Description:** Retrieve call logs.

#### Squad Management
- **Get Squads**
  - **Method:** GET
  - **Endpoint:** /squad
  - **Description:** Retrieve a list of all squads.

- **Create Squad**
  - **Method:** POST
  - **Endpoint:** /squad
  - **Description:** Create a new squad.

- **Get Squad by ID**
  - **Method:** GET
  - **Endpoint:** /squad/{id}
  - **Description:** Retrieve details of a specific squad using its ID.

- **Update Squad by ID**
  - **Method:** PATCH
  - **Endpoint:** /squad/{id}
  - **Description:** Update details of a specific squad using its ID.

- **Delete Squad by ID**
  - **Method:** DELETE
  - **Endpoint:** /squad/{id}
  - **Description:** Delete a specific squad using its ID.

#### Metrics Management
- **Get Metrics**
  - **Method:** GET
  - **Endpoint:** /metrics
  - **Description:** Retrieve metrics data.

#### Tool Management
- **List Tools**
  - **Method:** GET
  - **Endpoint:** /tool
  - **Description:** Retrieve a list of all tools.

- **Create Tool**
  - **Method:** POST
  - **Endpoint:** /tool
  - **Description:** Create a new tool.

- **Get Tool by ID**
  - **Method:** GET
  - **Endpoint:** /tool/{id}
  - **Description:** Retrieve details of a specific tool using its ID.

- **Update Tool by ID**
  - **Method:** PATCH
  - **Endpoint:** /tool/{id}
  - **Description:** Update details of a specific tool using its ID.

- **Delete Tool by ID**
  - **Method:** DELETE
  - **Endpoint:** /tool/{id}
  - **Description:** Delete a specific tool using its ID.

#### Customer Management
- **Get Customers**
  - **Method:** GET
  - **Endpoint:** /customer
  - **Description:** Retrieve a list of all customers.

- **Create Customer**
  - **Method:** POST
  - **Endpoint:** /customer
  - **Description:** Create a new customer.

- **Get Customer by ID**
  - **Method:** GET
  - **Endpoint:** /customer/{id}
  - **Description:** Retrieve details of a specific customer using its ID.

- **Update Customer by ID**
  - **Method:** PATCH
  - **Endpoint:** /customer/{id}
  - **Description:** Update details of a specific customer using its ID.

- **Delete Customer by ID**
  - **Method:** DELETE
  - **Endpoint:** /customer/{id}
  - **Description:** Delete a specific customer using its ID.

## Tips and Best Practices
- **Be Specific:** The more specific your request, the better MyVapi can assist you.
- **Explore Features:** Take time to explore all the features and find what works best for you.
- **Regular Updates:** Keep your account information and settings up-to-date for the best experience.

## Troubleshooting
If you encounter any issues while using MyVapi, try the following steps:
1. **Check Internet Connection:** Ensure you have a stable internet connection.
2. **Clear Cache:** Sometimes clearing your browser cache can resolve issues.
3. **Restart Browser:** Close and reopen your browser to refresh the session.

## FAQ
**Q:** Is MyVapi free to use?  
**A:** MyVapi is available to both free and paid ChatGPT accounts.

**Q:** How secure is my data?  
**A:** We prioritize your data security and use advanced encryption methods to protect your information.


 This is the content for the doc fern/community/outbound.mdx 

 ---
title: Outbound
subtitle: Videos showcasing Vapi out in the wild.
slug: community/outbound
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
    <iframe
        src="https://www.youtube.com/embed/gi5Qa0Z2iqQ?si=l1FLFu5TvuTxYyIc"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/IbgPEG8l09Y?si=jIFLBN_SKPShurfy"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/jJjD5UsO46o?si=ATafEm5RDmt-f13I"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />

{" "}

<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/Sj-OOK11Nac?si=laIZ3JasRWIF1vKh"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>
<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/t_35BMnOTDY?si=SO4m9QEqg4sxT9BY"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>

    <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/BMjSnRfcL7g?si=5ZW1Qr1tEecBCNgt"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen

/>

<iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/d7OQJ83XsBE?si=ebPYoX04ImtZl92U"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen

/>

  </div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/podcast.mdx 

 ---
title: Podcast
subtitle: Videos showcasing Vapi out in the wild.
slug: community/podcast
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
  <iframe
    src="https://www.youtube.com/embed/kOhr047QFFA?si=8-2uY09fni5195tx&amp;start=1245"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
</div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/snippets-sdks-tutorials.mdx 

 ---
title: Snippets & SDKs Tutorials
subtitle: Videos showcasing Vapi out in the wild.
slug: community/snippets-sdks-tutorials
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
    <iframe
        src="https://www.youtube.com/embed/wEiiDEGECb4?si=uuJZ3gcr9iQDv9tZ"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/yMkaa-15CWI?si=5dW45BVC-IteS18E"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/XOVt1qHmrlg?si=NXypsXXAetLkgZU9"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />
    <iframe
        src="https://www.youtube.com/embed/6O-8LimQST4?si=4ePsmegoe2CVBnwy"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    />

  <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/aM79mkF6UkA?si=PYe7zAwKoA0wzrEE"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  </div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/special-mentions.mdx 

 ---
title: Special Mentions
subtitle: Videos showcasing Vapi out in the wild.
slug: community/special-mentions
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
  <iframe
    src="https://www.youtube.com/embed/WCYf2Agml-s?si=dAMT_Xf7vPHmjqKJ&amp;start=929"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/rc0XGjI4QKM"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />

<iframe
  width="100%"
  height="315"
  src="https://www.youtube.com/embed/1VzKEEbTYUQ?si=rZjcTPNe4Ro1leQ9"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
/>

<iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/nYKFuI6sagw?si=oh9pcwKVnUamEIdV"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
</div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/squads.mdx 

 ---
title: Squads
slug: community/squads
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
  <iframe
    src="https://www.youtube.com/embed/uT7mW61H0nw?si=eN_n2c2umNNRtxIw"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/n8oFkp0_2qE?si=Egsv56Nfx-Dkl_b4"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/uJ52-EqBscQ?si=GeJJCUcptinCqRRg"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/V308U_5syiA?si=DQycim7-WhzVOsQp"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />

  <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/gdNkUESKC5k?si=13psOtVhyjWd6Ww8"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  </div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/television.mdx 

 ---
title: Television
subtitle: Videos showcasing Vapi out in the wild.
slug: community/television
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
  <iframe
    src="https://www.youtube.com/embed/pRUddK6sxDg?si=pvlgT0ban0lkvHTL"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
</div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/community/usecase.mdx 

 ---
title: Usecase
subtitle: Videos showcasing Vapi out in the wild.
slug: community/usecase
---


Here are some videos made by people in our community showcasing what Vapi can do:

<div class="video-grid">
  <iframe
    src="https://www.youtube.com/embed/WS4QJF9Bn7U?si=yrK1ErRpFZAKbpYW"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.youtube.com/embed/ZvPQU1VKmi8?si=neKdm1K8T-Tex56r"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.youtube.com/embed/CvhonBxoJ00?si=POAHHoKAvKQx7__O"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.youtube.com/embed/lmHMAJxlD0I?si=sHfjjxJxTah21VTR"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.youtube.com/embed/5ElOMT7cyJM?si=Pc_XE9CLAZs5qfkZ"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
  <iframe
    src="https://www.youtube.com/embed/VVKyogARy6A?si=KbKsdCact_ucdKQB"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
</div>

## Send Us Your Video

Have a video showcasing Vapi that you want us to feature? Let us know:

<CardGroup cols={2}>
  <Card
    title="Send Us Your Video"
    icon="video-arrow-up-right"
    iconType="solid"
    href="https://tally.so/r/3yD9Wx"
  >
    Send us your video showcasing what Vapi can do, we'd like to feature it.
  </Card>
</CardGroup>


 This is the content for the doc fern/customization/custom-keywords.mdx 

 ---
title: Custom Keywords
subtitle: Enhanced transcription accuracy guide
slug: customization/custom-keywords
---


Vapi allows you to improve the accuracy of your transcriptions by leveraging Deepgram's keyword boosting feature. This is particularly useful when dealing with specialized terminology or uncommon proper nouns. By providing specific keywords to the Deepgram model, you can enhance transcription quality directly through Vapi.

### Why Use Keyword Boosting?

Keyword boosting is beneficial for:

- Enhancing the recognition of specialized terms and proper nouns.
- Improving transcription accuracy without the need for a custom-trained model.
- Quickly updating the model's vocabulary with new or uncommon words.

### Important Notes

- Keywords should be uncommon words or proper nouns not frequently recognized by the model.
- Custom model training is the most effective way to ensure accurate keyword recognition.
- For more than 50 keywords, consider custom model training by contacting Deepgram.

## Enabling Keyword Boosting in Vapi

### API Call Integration

To enable keyword boosting, you need to add a `keywords` parameter to your Vapi assistant's transcriber section. This parameter should include the keywords and their respective intensifiers.

### Example of POST Request

To create an assistant with keyword boosting enabled, you can make the following POST request to Vapi:

```bash
bashCopy code
curl \
  --request POST \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
    "name": "Emma",
    "model": {
        "model": "gpt-4o",
        "provider": "openai"
    },
    "voice": {
        "voiceId": "emma",
        "provider": "azure"
    },
    "transcriber": {
        "provider": "deepgram",
        "model": "nova-2",
        "language": "bg",
        "smartFormat": true,
        "keywords": [
            "snuffleupagus:1"
        ]
    },
    "firstMessage": "Hi, I am Emma, what is your name?",
    "firstMessageMode": "assistant-speaks-first"
  }' \
  https://api.vapi.ai/assistant

```

In this configuration:

- **name**: The name of the assistant.
- **model**: Specifies the model and provider for the assistant's conversational capabilities.
- **voice**: Specifies the voice and provider for the assistant's speech.
- **transcriber**: Specifies Deepgram as the transcription provider, along with the model, language, smart formatting, and keywords for boosting.
- **firstMessage**: The initial message the assistant will speak.
- **firstMessageMode**: Specifies that the assistant speaks first.

### Intensifiers

Intensifiers are exponential factors that boost or suppress the likelihood of the specified keyword being recognized. The default intensifier is `1`. Higher values increase the likelihood, while `0` is equivalent to not specifying a keyword.

- **Boosting Example:** `keywords=snuffleupagus:5`
- **Suppressing Example:** `keywords=kansas:-10`

### Best Practices for Keyword Boosting

1. **Send Uncommon Keywords:** Focus on keywords not successfully transcribed by the model.
2. **Send Keywords Once:** Avoid repeating keywords.
3. **Use Individual Keywords:** Prefer individual terms over phrases.
4. **Use Proper Spelling:** Spell proper nouns as you want them to appear in transcripts.
5. **Moderate Intensifiers:** Start with small increments to avoid false positives.
6. **Custom Model Training:** For extensive vocabulary needs, consider custom model training.

### Additional Resources

For more detailed information on Deepgram's keyword boosting feature, refer to the Deepgram Keyword Boosting Documentation.

By following these guidelines, you can effectively utilize Deepgram's keyword boosting feature within your Vapi assistant, ensuring enhanced transcription accuracy for specialized terminology and uncommon proper nouns.

 This is the content for the doc fern/customization/custom-llm/fine-tuned-openai-models.mdx 

 ---
title: Fine-tuned OpenAI models
subtitle: Use Another LLM or Your Own Server
slug: customization/custom-llm/fine-tuned-openai-models
---


Vapi supports using any OpenAI-compatible endpoint as the LLM. This includes services like [OpenRouter](https://openrouter.ai/), [AnyScale](https://www.anyscale.com/), [Together AI](https://www.together.ai/), or your own server.

<Accordion title="When to Use Custom LLMs">
  - For an open-source LLM, like Mixtral
  - To update the context during the conversation
  - To customize the messages before they're sent to an LLM
</Accordion>

## Using an LLM provider

You'll first want to POST your API key via the `/credential` endpoint:

```json
{
  "provider": "openrouter",
  "apiKey": "<YOUR OPENROUTER KEY>"
}
```

Then, you can create an assistant with the model provider:

```json
{
  "name": "My Assistant",
  "model": {
    "provider": "openrouter",
    "model": "cognitivecomputations/dolphin-mixtral-8x7b",
    "messages": [
      {
        "role": "system",
        "content": "You are an assistant."
      }
    ],
    "temperature": 0.7
  }
}
```
## Using Fine-Tuned OpenAI Models

To set up your OpenAI Fine-Tuned model, you need to follow these steps:

1. Set the custom llm URL to `https://api.openai.com/v1`.
2. Assign the custom llm key to the OpenAI key.
3. Update the model to their model.
4. Execute a PATCH request to the `/assistant` endpoint and ensure that `model.metadataSendMode` is set to off.

## Using your server

To set up your server to act as the LLM, you'll need to create an endpoint that is compatible with the [OpenAI Client](https://platform.openai.com/docs/api-reference/making-requests). For best results, your endpoint should also support streaming completions.

If your server is making calls to an OpenAI compatble API, you can pipe the requests directly back in your response to Vapi.

If you'd like your OpenAI-compatible endpoint to be authenticated, you can POST your server's API key and URL via the `/credential` endpoint:

```json
{
  "provider": "custom-llm",
  "apiKey": "<YOUR SERVER API KEY>"
}
```

If your server isn't authenticated, you can skip this step.

Then, you can create an assistant with the `custom-llm` model provider:

```json
{
  "name": "My Assistant",
  "model": {
    "provider": "custom-llm",
    "url": "<YOUR OPENAI COMPATIBLE ENDPOINT BASE URL>",
    "model": "my-cool-model",
    "messages": [
      {
        "role": "system",
        "content": "You are an assistant."
      }
    ],
    "temperature": 0.7
  }
}
```


 This is the content for the doc fern/customization/custom-llm/tool-calling-integration.mdx 

 ---
title: Custom LLM Tool Calling Integration
slug: customization/tool-calling-integration
---
## What Is a Custom LLM and Why Use It?

A **Custom LLM** is more than just a text generator—it’s a conversational assistant that can call external functions, trigger processes, and handle special logic, all while chatting with your users. Think of it as your smart helper that not only answers questions but also takes actions.

**Why use a Custom LLM?**
- **Enhanced Functionality:** It mixes natural language responses with actionable functions.
- **Flexibility:** You can combine built-in functions, attach external tools via Vapi, or even add custom endpoints.
- **Dynamic Interactions:** The assistant can return structured instructions—like transferring a call or running a custom process—when needed.
- **Seamless Integration:** Vapi lets you plug these custom endpoints into your assistant quickly and easily.

---

## Setting Up Your Custom LLM for Response Generation

Before adding tool calls, let’s start with the basics: setting up your Custom LLM to simply generate conversation responses. In this mode, your LLM receives conversation details, asks the model for a reply, and streams that text back.

### How It Works
- **Request Reception:** Your endpoint (e.g., `/chat/completions`) gets a payload with the model, messages, temperature, and (optionally) tools.
- **Content Generation:** The code builds an OpenAI API request that includes the conversation context.
- **Response Streaming:** The generated reply is sent back as Server-Sent Events (SSE).

### Sample Code Snippet

```typescript
app.post("/chat/completions", async (req: Request, res: Response) => {
  // Log the incoming request.
  logEvent("Request received at /chat/completions", req.body);
  const payload = req.body;

  // Prepare the API request to OpenAI.
  const requestArgs: any = {
    model: payload.model,
    messages: payload.messages,
    temperature: payload.temperature ?? 1.0,
    stream: true,
    tools: payload.tools || [],
    tool_choice: "auto",
  };

  // Optionally merge in native tool definitions.
  const modelTools = payload.tools || [];
  requestArgs.tools = [...modelTools, ...ourTools];

  logEvent("Calling OpenAI API for content generation");
  const openAIResponse = await openai.chat.completions.create(requestArgs);
  logEvent("OpenAI API call successful. Streaming response.");

  // Set up streaming headers.
  res.setHeader("Content-Type", "text/event-stream");
  res.setHeader("Cache-Control", "no-cache");
  res.setHeader("Connection", "keep-alive");

  // Stream the response chunks back.
  for await (const chunk of openAIResponse as unknown as AsyncIterable<any>) {
    res.write(`data: ${JSON.stringify(chunk)}\n\n`);
  }
  res.write("data: [DONE]\n\n");
  res.end();
});
```

### Attaching Custom LLM Without Tools to an Existing Assistant in Vapi

If you just want response generation (without tool calls), update your Vapi model with a PATCH request like this:

```bash
curl -X PATCH https://api.vapi.ai/assistant/insert-your-assistant-id-here \
     -H "Authorization: Bearer insert-your-private-key-here" \
     -H "Content-Type: application/json" \
     -d '{
  "model": {
    "provider": "custom-llm",
    "model": "gpt-4o",
    "url": "https://custom-llm-url/chat/completions",
    "messages": [
      {
        "role": "system",
        "content": "[TASK] Ask the user if they want to transfer the call; if not, continue the conversation."
      }
    ]
  },
  "transcriber": {
    "provider": "azure",
    "language": "en-CA"
  }
}'
```

---

## Adding Tools Calling with Your Custom LLM

Now that you’ve got response generation working, let’s expand your assistant’s abilities. Your Custom LLM can trigger external actions in three different ways.

### a. Native LLM Tools

These tools are built right into your LLM integration. For example, a native function like `get_payment_link` can return a payment URL.

**How It Works:**
1. **Detection:** The LLM’s streaming response includes a tool call for `get_payment_link`.
2. **Execution:** The integration parses the arguments and calls the native function.
3. **Response:** The result is packaged into a follow-up API call and streamed back.

**Code Snippet:**

```typescript
// Variables to accumulate tool call information.
let argumentsStr = "";
let toolCallInfo: { name?: string; id?: string } | null = null;

// Process streaming chunks.
for await (const chunk of openAIResponse as unknown as AsyncIterable<any>) {
  const choice = chunk.choices && chunk.choices[0];
  const delta = choice?.delta || {};
  const toolCalls = delta.tool_calls;

  if (toolCalls && toolCalls.length > 0) {
    for (const toolCall of toolCalls) {
      const func = toolCall.function;
      if (func && func.name) {
        toolCallInfo = { name: func.name, id: toolCall.id };
      }
      if (func && func.arguments) {
        argumentsStr += func.arguments;
      }
    }
  }

  const finishReason = choice?.finish_reason;
  if (finishReason === "tool_calls" && toolCallInfo) {
    let parsedArgs = {};
    try {
      parsedArgs = JSON.parse(argumentsStr);
    } catch (err) {
      console.error("Failed to parse arguments:", err);
    }
    if (tool_functions[toolCallInfo.name!]) {
      const result = await tool_functions[toolCallInfo.name!](parsedArgs);
      const functionMessage = {
        role: "function",
        name: toolCallInfo.name,
        content: JSON.stringify(result)
      };

      const followUpResponse = await openai.chat.completions.create({
        model: requestArgs.model,
        messages: [...requestArgs.messages, functionMessage],
        temperature: requestArgs.temperature,
        stream: true,
        tools: requestArgs.tools,
        tool_choice: "auto"
      });

      for await (const followUpChunk of followUpResponse) {
        res.write(`data: ${JSON.stringify(followUpChunk)}\n\n`);
      }
      argumentsStr = "";
      toolCallInfo = null;
      continue;
    }
  }
  res.write(`data: ${JSON.stringify(chunk)}\n\n`);
}
```

### b. Vapi-Attached Tools

These tools come pre-attached via your Vapi configuration. For example, the `transferCall` tool:

**How It Works:**
1. **Detection:** When a tool call for `transferCall` appears with a destination in the payload, the function isn’t executed.
2. **Response:** The integration immediately sends a function call payload with the destination back to Vapi.

**Code Snippet:**

```typescript
if (functionName === "transferCall" && payload.destination) {
  const functionCallPayload = {
    function_call: {
      name: "transferCall",
      arguments: {
        destination: payload.destination,
      },
    },
  };
  logEvent("Special handling for transferCall", { functionCallPayload });
  res.write(`data: ${JSON.stringify(functionCallPayload)}\n\n`);
  // Skip further processing for this chunk.
  continue;
}
```

### c. Custom Tools

Custom tools are unique to your application and are handled by a dedicated endpoint. For example, a custom function named `processOrder`.

**How It Works:**
1. **Dedicated Endpoint:** Requests for custom tools go to `/chat/completions/custom-tool`.
2. **Detection:** The payload includes a tool call list. If the function name is `"processOrder"`, a hardcoded result is returned.
3. **Response:** A JSON response is sent back with the result.

**Code Snippet (Custom Endpoint):**

```typescript
app.post("/chat/completions/custom-tool", async (req: Request, res: Response) => {
  logEvent("Received request at /chat/completions/custom-tool", req.body);
  // Expect the payload to have a "message" with a "toolCallList" array.
  const vapiPayload = req.body.message;

  // Process tool call.
  for (const toolCall of vapiPayload.toolCallList) {
    if (toolCall.function?.name === "processOrder") {
      const hardcodedResult = "CustomTool processOrder With CustomLLM Always Works";
      logEvent("Returning hardcoded result for 'processOrder'", { toolCallId: toolCall.id });
      return res.json({
        results: [
          {
            toolCallId: toolCall.id,
            result: hardcodedResult,
          },
        ],
      });
    }
  }
});
```

---

## Testing Tool Calling with cURL

Once your endpoints are set up, try testing them with these cURL commands.

### a. Native Tool Calling (`get_payment_link`)

```bash
curl -X POST https://custom-llm-url/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "gpt-3.5-turbo",
        "messages": [
          {"role": "user", "content": "I need a payment link."}
        ],
        "temperature": 0.7,
        "tools": [
          {
            "type": "function",
            "function": {
              "name": "get_payment_link",
              "description": "Get a payment link",
              "parameters": {}
            }
          }
        ]
      }'
```

*Expected Response:*  
Streaming chunks eventually include the result (e.g., a payment link) returned by the native tool function.

### b. Vapi-Attached Tool Calling (`transferCall`)

```bash
curl -X POST https://custom-llm-url/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "gpt-3.5-turbo",
        "messages": [
          {"role": "user", "content": "Please transfer my call."}
        ],
        "temperature": 0.7,
        "tools": [
          {
            "type": "function",
            "function": {
              "name": "transferCall",
              "description": "Transfer call to a specified destination",
              "parameters": {}
            }
          }
        ],
        "destination": "555-1234"
      }'
```

*Expected Response:*  
Immediately returns a function call payload that instructs Vapi to transfer the call to the specified destination.

### c. Custom Tool Calling (`processOrder`)

```bash
curl -X POST https://custom-llm-url/chat/completions/custom-tool \
  -H "Content-Type: application/json" \
  -d '{
        "message": {
          "toolCallList": [
            {
              "id": "12345",
              "function": {
                "name": "processOrder",
                "arguments": {
                  "param": "value"
                }
              }
            }
          ]
        }
      }'
```

*Expected Response:*
```json
{
  "results": [
    {
      "toolCallId": "12345",
      "result": "CustomTools With CustomLLM Always Works"
    }
  ]
}
```

---

## Integrating Tools with Vapi

After testing locally, integrate your Custom LLM with Vapi. Choose the configuration that fits your needs.

### a. Without Tools (Response Generation Only)

```bash
curl -X PATCH https://api.vapi.ai/assistant/insert-your-assistant-id-here \
     -H "Authorization: Bearer insert-your-private-key-here" \
     -H "Content-Type: application/json" \
     -d '{
  "model": {
    "provider": "custom-llm",
    "model": "gpt-4o",
    "url": "https://custom-llm-url/chat/completions",
    "messages": [
      {
        "role": "system",
        "content": "[TASK] Ask the user if they want to transfer the call; if not, continue chatting."
      }
    ]
  },
  "transcriber": {
    "provider": "azure",
    "language": "en-CA"
  }
}'
```

### b. With Tools (Including `transferCall` and `processOrder`)

```bash
curl -X PATCH https://api.vapi.ai/assistant/insert-your-assistant-id-here \
     -H "Authorization: Bearer insert-your-private-key-here" \
     -H "Content-Type: application/json" \
     -d '{
  "model": {
    "provider": "custom-llm",
    "model": "gpt-4o",
    "url": "https://custom-llm-url/chat/completions",
    "messages": [
      {
        "role": "system",
        "content": "[TASK] Ask the user if they want to transfer the call; if they agree, trigger the transferCall tool; if not, continue the conversation. Also, if the user asks about the custom function processOrder, trigger that tool."
      }
    ],
    "tools": [
      {
        "type": "transferCall",
        "destinations": [
          {
            "type": "number",
            "number": "+xxxxxx",
            "numberE164CheckEnabled": false,
            "message": "Transferring Call To Customer Service Department"
          }
        ]
      },
      {
        "type": "function",
        "async": false,
        "function": {
          "name": "processOrder",
          "description": "it's a custom tool function named processOrder according to vapi.ai custom tools guide"
        },
        "server": {
          "url": "https://custom-llm-url/chat/completions/custom-tool"
        }
      }
    ]
  },
  "transcriber": {
    "provider": "azure",
    "language": "en-CA"
  }
}'
```

---

## Conclusion

A Custom LLM turns a basic conversational assistant into an interactive helper that can:
- **Generate everyday language responses,**
- **Call native tools** (like fetching a payment link),
- **Use Vapi-attached tools** (like transferring a call), and
- **Leverage custom tools** (like processing orders).

By building each layer step by step and testing with cURL, you can fine-tune your integration before rolling it out in production.

---

## Complete Code

For your convenience, you can find the complete source code for this Custom LLM integration here:

**[Custom LLM with Vapi Integration – Complete Code](https://codesandbox.io/p/devbox/gfwztp)**
```

 This is the content for the doc fern/customization/custom-llm/using-your-server.mdx 

 ---
title: 'Connecting Your Custom LLM to Vapi: A Comprehensive Guide'
slug: customization/custom-llm/using-your-server
---


This guide provides a comprehensive walkthrough on integrating Vapi with OpenAI's gpt-3.5-turbo-instruct model using a custom LLM configuration. We'll leverage Ngrok to expose a local development environment for testing and demonstrate the communication flow between Vapi and your LLM.
## Prerequisites

- **Vapi Account**: Access to the Vapi Dashboard for configuration.
- **OpenAI API Key**: With access to the gpt-3.5-turbo-instruct model.
- **Python Environment**: Set up with the OpenAI library (`pip install openai`).
- **Ngrok**: For exposing your local server to the internet.
- **Code Reference**: Familiarize yourself with the `/openai-sse/chat/completions` endpoint function in the provided Github repository: [Server-Side Example Python Flask](https://github.com/VapiAI/server-side-example-python-flask/blob/main/app/api/custom_llm.py).

## Step 1: Setting Up Your Local Development Environment

**1. Create a Python Script (app.py):**

```python
from flask import Flask, request, jsonify
import openai

app = Flask(__name__)
openai.api_key = "YOUR_OPENAI_API_KEY"  # Replace with your actual API key

@app.route("/chat/completions", methods=["POST"])
def chat_completions():
    data = request.get_json()
    # Extract relevant information from data (e.g., prompt, conversation history)
    # ...
    
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-instruct",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            # ... (Add messages from conversation history and current prompt)
        ]
    )
    # Format response according to Vapi's structure
    # ...
    return jsonify(formatted_response)

if __name__ == "__main__":
    app.run(debug=True, port=5000)  # You can adjust the port if needed
```
**2. Run the Script:**
Execute the Python script using python app.py in your terminal. This will start the Flask server on the specified port (5000 in this example).

**3. Expose with Ngrok:**
Open a new terminal window and run ngrok http 5000 (replace 5000 with your chosen port) to create a public URL that tunnels to your local server.

## Step 2: Configuring Vapi with Custom LLM
**1. Access Vapi Dashboard:**
Log in to your Vapi account and navigate to the "Model" section.

**2. Select Custom LLM:**
Choose the "Custom LLM" option to set up the integration.

**3. Enter Ngrok URL:**
Paste the public URL generated by ngrok (e.g., https://your-unique-id.ngrok.io) into the endpoint field. This will be the URL Vapi uses to communicate with your local server.

**4. Test the Connection:**
Send a test message through the Vapi interface to ensure it reaches your local server and receives a response from the OpenAI API. Verify that the response is displayed correctly in Vapi.

## Step 3: Understanding the Communication Flow
**1. Vapi Sends POST Request:**
When a user interacts with your Vapi application, Vapi sends a POST request containing conversation context and metadata to the configured endpoint (your ngrok URL).

**2. Local Server Processes Request:**
Your Python script receives the POST request and the chat_completions function is invoked.

**3. Extract and Prepare Data:**
The script parses the JSON data, extracts relevant information (prompt, conversation history), and builds the prompt for the OpenAI API call.

**4. Call to OpenAI API:**
The constructed prompt is sent to the gpt-3.5-turbo-instruct model using the openai.ChatCompletion.create method.

**5. Receive and Format Response:**
The response from OpenAI, containing the generated text, is received and formatted according to Vapi's expected structure.

**6. Send Response to Vapi:**
The formatted response is sent back to Vapi as a JSON object.

**7. Vapi Displays Response:**
Vapi receives the response and displays the generated text within the conversation interface to the user.

By following these detailed steps and understanding the communication flow, you can successfully connect Vapi to OpenAI's gpt-3.5-turbo-instruct model and create powerful conversational experiences within your Vapi applications. The provided code example and reference serve as a starting point for you to build and customize your integration based on your specific needs.

**Video Tutorial:**
  <iframe
    src="https://www.youtube.com/embed/-1xWhYmOT0A?si=8qB6FLzcmmrmduT-"
    title="Loom video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    width="100%"
    height="400px"
    allowfullscreen
  />

 This is the content for the doc fern/customization/custom-transcriber.mdx 

 ---
title: Custom Transcriber
slug: customization/custom-transcriber
---
## Introduction

Vapi supports several transcription providers, but sometimes you may need to use your own transcription service. This guide shows you how to integrate Deepgram as your custom transcriber. The solution streams raw stereo PCM audio (16‑bit) from Vapi via WebSocket to your server, which then forwards the audio to Deepgram. Deepgram returns real‑time partial and final transcripts that are processed (including channel detection) and sent back to Vapi.

## Why Use a Custom Transcriber?

- **Flexibility:** Integrate with your preferred transcription service.
- **Control:** Implement specialized processing that isn’t available with built‑in providers.
- **Cost Efficiency:** Leverage your existing transcription infrastructure while maintaining full control over the pipeline.
- **Customization:** Tailor the handling of audio data, transcript formatting, and buffering according to your specific needs.

## How It Works

1. **Connection Initialization:**  
   Vapi connects to your custom transcriber endpoint (e.g. `/api/custom-transcriber`) via WebSocket. It sends an initial JSON message like this:
   ```json
   {
     "type": "start",
     "encoding": "linear16",
     "container": "raw",
     "sampleRate": 16000,
     "channels": 2
   }
   ```
2. **Audio Streaming:**  
   Vapi then streams binary PCM audio to your server.

3. **Transcription Processing:**  
   Your server forwards the audio to Deepgram(Chooseen Transcriber for Example) using its SDK. Deepgram processes the audio and returns transcript events that include a `channel_index` (e.g. `[0, ...]` for customer, `[1, ...]` for assistant). The service buffers the incoming data, processes the transcript events (with debouncing and channel detection), and emits a final transcript.

4. **Response:**  
   The final transcript is sent back to Vapi as a JSON message:
   ```json
   {
     "type": "transcriber-response",
     "transcription": "The transcribed text",
     "channel": "customer" // or "assistant"
   }
   ```

## Implementation Steps

### 1. Project Setup

Create a new Node.js project and install the required dependencies:

```bash
mkdir vapi-custom-transcriber
cd vapi-custom-transcriber
npm init -y
npm install ws express dotenv @deepgram/sdk
```

Create a `.env` file with the following content:

```env
DEEPGRAM_API_KEY=your_deepgram_api_key
PORT=3001
```

### 2. Code Files

Below are the individual code files you need for the integration.

#### transcriptionService.js

This service creates a live connection to Deepgram, processes incoming audio, handles transcript events (including channel detection), and emits the final transcript back to the caller.

```js
const { createClient, LiveTranscriptionEvents } = require("@deepgram/sdk");
const EventEmitter = require("events");

const PUNCTUATION_TERMINATORS = [".", "!", "?"];
const MAX_RETRY_ATTEMPTS = 3;
const DEBOUNCE_DELAY_IN_SECS = 3;
const DEBOUNCE_DELAY = DEBOUNCE_DELAY_IN_SECS * 1000;
const DEEPGRAM_API_KEY = process.env["DEEPGRAM_API_KEY"] || "";

class TranscriptionService extends EventEmitter {
  constructor(config, logger) {
    super();
    this.config = config;
    this.logger = logger;
    this.flowLogger = require("./fileLogger").createNamedLogger(
      "transcriber-flow.log"
    );
    if (!DEEPGRAM_API_KEY) {
      throw new Error("Missing Deepgram API Key");
    }
    this.deepgramClient = createClient(DEEPGRAM_API_KEY);
    this.logger.logDetailed(
      "INFO",
      "Initializing Deepgram live connection",
      "TranscriptionService",
      {
        model: "nova-2",
        sample_rate: 16000,
        channels: 2,
      }
    );
    this.deepgramLive = this.deepgramClient.listen.live({
      encoding: "linear16",
      channels: 2,
      sample_rate: 16000,
      model: "nova-2",
      smart_format: true,
      interim_results: true,
      endpointing: 800,
      language: "en",
      multichannel: true,
    });
    this.finalResult = { customer: "", assistant: "" };
    this.audioBuffer = [];
    this.retryAttempts = 0;
    this.lastTranscriptionTime = Date.now();
    this.pcmBuffer = Buffer.alloc(0);

    this.deepgramLive.addListener(LiveTranscriptionEvents.Open, () => {
      this.logger.logDetailed(
        "INFO",
        "Deepgram connection opened",
        "TranscriptionService"
      );
      this.deepgramLive.on(LiveTranscriptionEvents.Close, () => {
        this.logger.logDetailed(
          "INFO",
          "Deepgram connection closed",
          "TranscriptionService"
        );
        this.emitTranscription();
        this.audioBuffer = [];
      });
      this.deepgramLive.on(LiveTranscriptionEvents.Metadata, (data) => {
        this.logger.logDetailed(
          "DEBUG",
          "Deepgram metadata received",
          "TranscriptionService",
          data
        );
      });
      this.deepgramLive.on(LiveTranscriptionEvents.Transcript, (event) => {
        this.handleTranscript(event);
      });
      this.deepgramLive.on(LiveTranscriptionEvents.Error, (err) => {
        this.logger.logDetailed(
          "ERROR",
          "Deepgram error received",
          "TranscriptionService",
          { error: err }
        );
        this.emit("transcriptionerror", err);
      });
    });
  }

  send(payload) {
    if (payload instanceof Buffer) {
      this.pcmBuffer =
        this.pcmBuffer.length === 0
          ? payload
          : Buffer.concat([this.pcmBuffer, payload]);
    } else {
      this.logger.warn("TranscriptionService: Received non-Buffer data chunk.");
    }
    if (this.deepgramLive.getReadyState() === 1 && this.pcmBuffer.length > 0) {
      this.sendBufferedData(this.pcmBuffer);
      this.pcmBuffer = Buffer.alloc(0);
    }
  }

  sendBufferedData(bufferedData) {
    try {
      this.logger.logDetailed(
        "INFO",
        "Sending buffered data to Deepgram",
        "TranscriptionService",
        { bytes: bufferedData.length }
      );
      this.deepgramLive.send(bufferedData);
      this.audioBuffer = [];
      this.retryAttempts = 0;
    } catch (error) {
      this.logger.logDetailed(
        "ERROR",
        "Error sending buffered data",
        "TranscriptionService",
        { error }
      );
      this.retryAttempts++;
      if (this.retryAttempts <= MAX_RETRY_ATTEMPTS) {
        setTimeout(() => {
          this.sendBufferedData(bufferedData);
        }, 1000);
      } else {
        this.logger.logDetailed(
          "ERROR",
          "Max retry attempts reached, discarding data",
          "TranscriptionService"
        );
        this.audioBuffer = [];
        this.retryAttempts = 0;
      }
    }
  }

  handleTranscript(transcription) {
    if (!transcription.channel || !transcription.channel.alternatives?.[0]) {
      this.logger.logDetailed(
        "WARN",
        "Invalid transcript format",
        "TranscriptionService",
        { transcription }
      );
      return;
    }
    const text = transcription.channel.alternatives[0].transcript.trim();
    if (!text) return;
    const currentTime = Date.now();
    const channelIndex = transcription.channel_index
      ? transcription.channel_index[0]
      : 0;
    const channel = channelIndex === 0 ? "customer" : "assistant";
    this.logger.logDetailed(
      "INFO",
      "Received transcript",
      "TranscriptionService",
      { channel, text }
    );
    if (transcription.is_final || transcription.speech_final) {
      this.finalResult[channel] += ` ${text}`;
      this.emitTranscription();
    } else {
      this.finalResult[channel] += ` ${text}`;
      if (currentTime - this.lastTranscriptionTime >= DEBOUNCE_DELAY) {
        this.logger.logDetailed(
          "INFO",
          `Emitting transcript after ${DEBOUNCE_DELAY_IN_SECS}s inactivity`,
          "TranscriptionService"
        );
        this.emitTranscription();
      }
    }
    this.lastTranscriptionTime = currentTime;
  }

  emitTranscription() {
    for (const chan of ["customer", "assistant"]) {
      if (this.finalResult[chan].trim()) {
        const transcript = this.finalResult[chan].trim();
        this.logger.logDetailed(
          "INFO",
          "Emitting transcription",
          "TranscriptionService",
          { channel: chan, transcript }
        );
        this.emit("transcription", transcript, chan);
        this.finalResult[chan] = "";
      }
    }
  }
}

module.exports = TranscriptionService;
```

---

#### server.js

This file creates an Express server, attaches the custom transcriber WebSocket at `/api/custom-transcriber`, and starts the HTTP server.

```js
const express = require("express");
const http = require("http");
const TranscriptionService = require("./transcriptionService");
const FileLogger = require("./fileLogger");
require("dotenv").config();

const app = express();
app.use(express.json());
app.use(express.urlencoded({ extended: true }));

app.get("/", (req, res) => {
  res.send("Custom Transcriber Service is running");
});

const server = http.createServer(app);

const config = {
  DEEPGRAM_API_KEY: process.env.DEEPGRAM_API_KEY,
  PORT: process.env.PORT || 3001,
};

const logger = new FileLogger();
const transcriptionService = new TranscriptionService(config, logger);

transcriptionService.setupWebSocketServer = function (server) {
  const WebSocketServer = require("ws").Server;
  const wss = new WebSocketServer({ server, path: "/api/custom-transcriber" });
  wss.on("connection", (ws) => {
    logger.logDetailed(
      "INFO",
      "New WebSocket client connected on /api/custom-transcriber",
      "Server"
    );
    ws.on("message", (data, isBinary) => {
      if (!isBinary) {
        try {
          const msg = JSON.parse(data.toString());
          if (msg.type === "start") {
            logger.logDetailed(
              "INFO",
              "Received start message from client",
              "Server",
              { sampleRate: msg.sampleRate, channels: msg.channels }
            );
          }
        } catch (err) {
          logger.error("JSON parse error", err, "Server");
        }
      } else {
        transcriptionService.send(data);
      }
    });
    ws.on("close", () => {
      logger.logDetailed("INFO", "WebSocket client disconnected", "Server");
      if (
        transcriptionService.deepgramLive &&
        transcriptionService.deepgramLive.getReadyState() === 1
      ) {
        transcriptionService.deepgramLive.finish();
      }
    });
    ws.on("error", (error) => {
      logger.error("WebSocket error", error, "Server");
    });
    transcriptionService.on("transcription", (text, channel) => {
      const response = {
        type: "transcriber-response",
        transcription: text,
        channel,
      };
      ws.send(JSON.stringify(response));
      logger.logDetailed("INFO", "Sent transcription to client", "Server", {
        channel,
        text,
      });
    });
    transcriptionService.on("transcriptionerror", (err) => {
      ws.send(
        JSON.stringify({ type: "error", error: "Transcription service error" })
      );
      logger.error("Transcription service error", err, "Server");
    });
  });
};

transcriptionService.setupWebSocketServer(server);

server.listen(config.PORT, () => {
  console.log(`Server is running on http://localhost:${config.PORT}`);
});
```

---

## Testing Your Integration

### Code Examples – How to Test

1. **Deploy Your Server:**  
   Run your server with:

   ```bash
   node server.js
   ```

2. **Expose Your Server:**  
   If you want to test externally, use a tool like ngrok to expose your server via HTTPS/WSS.

3. **Initiate a Call with Vapi:**  
   Use the following CURL command (update the placeholders with your actual values):
   ```bash
   curl -X POST https://api.vapi.ai/call \
        -H "Authorization: Bearer YOUR_API_KEY" \
        -H "Content-Type: application/json" \
        -d '{
     "phoneNumberId": "YOUR_PHONE_NUMBER_ID",
     "customer": {
       "number": "CUSTOMER_PHONE_NUMBER"
     },
     "assistant": {
       "transcriber": {
         "provider": "custom-transcriber",
         "server": {
           "url": "wss://your-server.ngrok.io/api/custom-transcriber"
         },
         "secret": "your_optional_secret_value"
       },
       "firstMessage": "Hello! I am using a custom transcriber with Deepgram."
     },
     "name": "CustomTranscriberTest"
   }'
   ```

### Expected Behavior

- Vapi connects via WebSocket to your custom transcriber at `/api/custom-transcriber`.
- The `"start"` message initializes the Deepgram session.
- PCM audio data is forwarded to Deepgram.
- Deepgram returns transcript events, which are processed with channel detection and debouncing.
- The final transcript is sent back as a JSON message:
  ```json
  {
    "type": "transcriber-response",
    "transcription": "The transcribed text",
    "channel": "customer" // or "assistant"
  }
  ```

## Notes and Limitations

- **Streaming Support Requirement:**  
  The custom transcriber must support streaming. Vapi sends continuous audio data over the WebSocket, and your server must handle this stream in real time.
- **Secret Header:**  
  The custom transcriber configuration accepts an optional field called **`secret`**. When set, Vapi will send this value with every request as an HTTP header named `x-vapi-secret`. This can also be configured via a headers field.

- **Buffering:**  
  The solution buffers PCM audio and performs simple validation (e.g. ensuring stereo PCM data length is a multiple of 4). If the audio data is malformed, it is trimmed to a valid length.

- **Channel Detection:**  
  Transcript events from Deepgram include a `channel_index` array. The service uses the first element to determine whether the transcript is from the customer (`0`) or the assistant (`1`). Ensure Deepgram’s response format remains consistent with this logic.

---

## Conclusion

Using a custom transcriber with Vapi gives you the flexibility to integrate any transcription service into your call flows. This guide walked you through the setup, usage, and testing of a solution that streams real-time audio, processes transcripts with multi‑channel detection, and returns formatted responses back to Vapi. Follow the steps above and use the provided code examples to build your custom transcriber solution.


 This is the content for the doc fern/customization/custom-voices/custom-voice.mdx 

 ---
title: Introduction
subtitle: Use Custom Voice with your favourite provider instead of the preset ones.
slug: customization/custom-voices/custom-voice
---


Vapi lets you use various providers with some preset voice. At the same time you can also create your own custom voices in the supported providers and use them with Vapi.

You can update the `voice` property in the assistant configuration when you are creating the assistant to use your custom voice.

```json
{
  "voice": {
    "provider": "deepgram",
    "voiceId": "your-voice-id"
  }
}
```


 This is the content for the doc fern/customization/custom-voices/elevenlabs.mdx 

 ---
title: Elevenlabs
subtitle: 'Quickstart: Setup Elevenlabs Custom Voice'
slug: customization/custom-voices/elevenlabs
---


This guide outlines the procedure for integrating your cloned voice with 11labs through the Vapi platform.

<Note>An subscription is required for this process to work.</Note>

To integrate your cloned voice with 11labs using the Vapi platform, follow these steps. 

1. **Obtain an 11labs API Subscription:** Visit the [11labs pricing page](https://elevenlabs.io/pricing) and subscribe to an API plan that suits your needs.
2. **Retrieve Your API Key:** Go to the 'Profile + Keys' section on the 11labs website to get your API key.
3. **Enter Your API Key in Vapi:** Navigate to the [Vapi Provider Key section](https://dashboard.vapi.ai/keys) and input your 11labs API key under the 11labs section.
4. **Sync Your Cloned Voice:** From the [Voice Library](https://dashboard.vapi.ai/voice-library) in Vapi, select 11labs as your voice provider and click on "Sync with 11labs."
5. **Search and Use Your Cloned Voice:** After syncing, you can search for your cloned voice within the voice library and directly use it with your assistant.

By following these steps, you will successfully integrate your cloned voice from 11labs with Vapi.

**Video Tutorial:**
  <iframe
    src="https://www.loom.com/embed/91568c17289740889c278f458f0d291c?sid=a0c812fa-5809-4ffa-8391-6a578c3e0608"
    title="Loom video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    width="100%"
    height="400px"
    allowfullscreen
  />


 This is the content for the doc fern/customization/custom-voices/playht.mdx 

 ---
title: PlayHT
subtitle: 'Quickstart: Setup PlayHT Custom Voice'
slug: customization/custom-voices/playht
---


This guide outlines the procedure for integrating your cloned voice with Play.ht through the Vapi platform.

<Note>An API subscription is required for this process to work.</Note>

To integrate your cloned voice with [Play.ht](http://play.ht/) using the Vapi platform, follow these steps.

1. **Obtain a Play.ht API Subscription:** Visit the [Play.ht pricing page](https://play.ht/studio/pricing) and subscribe to an API plan.
2. **Retrieve Your User ID and Secret Key:** Go to the [API Access section](https://play.ht/studio/api-access) on Play.ht to get your User ID and Secret Key.
3. **Enter Your API Keys in Vapi:** Navigate to the [Vapi Provider Key section](https://dashboard.vapi.ai/keys) and input your Play.ht API keys under the Play.ht section.
4. **Sync Your Cloned Voice:** From the [Voice Library](https://dashboard.vapi.ai/voice-library) in Vapi, select Play.ht as your voice provider and click on "Sync with Play.ht."
5. **Search and Use Your Cloned Voice:** After syncing, you can search for your cloned voice within the voice library and directly use it with your assistant.

**Video Tutorial:**
  <iframe
    src="https://www.loom.com/embed/45a6e43ae03945a783385f771ea9203d?sid=268071d7-d37f-43aa-843a-13c221af3ed5"
    title="Loom video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    width="100%"
    height="400px"
    allowfullscreen
  />

 This is the content for the doc fern/customization/custom-voices/tavus.mdx 

 ---
title: Tavus
subtitle: 'Quickstart: Setup Tavus Custom Replica'
slug: customization/custom-voices/tavus
---


This guide outlines the procedure for integrating your custom replica with Tavus through the Vapi platform.

<Note>An API subscription is required for this process to work. This process is only required if you would like to use your **custom Tavus replicas**. This process is not required to use stock replicas on the Vapi platform.</Note>

To integrate your custom replica with [Tavus](https://platform.tavus.io/) using the Vapi platform, follow these steps.

1. **Obtain a Tavus API Subscription:** Visit the [Tavus pricing page](https://platform.tavus.io/billing) and subscribe to an API plan.
2. **Retrieve Your API Key:** Go to the [API Keys section](https://platform.tavus.io/api-keys) on Tavus to get your API key.
3. **Enter Your API Key in Vapi:** Navigate to the [Vapi Provider Key section](https://dashboard.vapi.ai/keys) and input your Tavus API key under the Tavus section.
4. **Enter Your Custom Replica ID:** After adding your API key, you can select Tavus as your assistant's voice provider and add your Custom Replica ID manually through the dashboard. Alternatively, you may use the API and specify the replica ID within the `voiceId` field.

**Video Tutorial:**
  <iframe
    src="https://www.loom.com/embed/f3f8a6f3ec0d46c79874ee9e032ae332?sid=981ef281-a30b-46e3-ac19-1a2b2b176511"
    title="Loom video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    width="100%"
    height="400px"
    allowfullscreen
  />

 This is the content for the doc fern/customization/jwt-authentication.mdx 

 ---
title: JWT Authentication
subtitle: Secure API authentication guide
slug: customization/jwt-authentication
---

This documentation provides an overview of JWT (JSON Web Token) Authentication and demonstrates how to generate a JWT token and use it to authenticate API requests securely.

## Prerequisites

Before you proceed, ensure you have the following:

- An environment that supports JWT generation and API calls (e.g., a programming language or framework)
- An account with a service that requires JWT authentication
- Environment variables set up for the necessary credentials (e.g., organization ID and private key, both can be found in your Vapi portal)

## Generating a JWT Token

The following steps outline how to generate a JWT token:

1. **Define the Payload**: The payload contains the data you want to include in the token. In this case, it includes an `orgId`.
2. **Get the Private Key**: The private key (provided by Vapi) is used to sign the token. Ensure it is securely stored, often in environment variables.
3. **Set Token Options**: Define options for the token, such as the expiration time (`expiresIn`).
4. **Generate the Token**: Use a JWT library or built-in functionality to generate the token with the payload, key, and options.

### JWT Token Scopes

The generated JWT token can have one of two scopes: `private` or `public`. The scope of the token will determine the actions that can be performed using the token.

For example, it can be used to restrict which API endpoints the token can access.

<Note>
  As of writing, the only publicly scoped API endpoint is
  https://api.vapi.ai//call/web, which is used for Web Call creation. All other
  endpoints are privately scoped.
</Note>

### Example (generating a private JWT token)

```js
// Define the payload
const payload = {
  orgId: process.env.ORG_ID,
  token: {
    // This is the scope of the token
    tag: "private",
  },
};

// Get the private key from environment variables
const key = process.env.PRIVATE_KEY;

// Define token options
const options = {
  expiresIn: "1h",
};

// Generate the token using a JWT library or built-in functionality
const token = generateJWT(payload, key, options);
```

### Example (generating a public JWT token)

```js
// Define the payload
const payload = {
  orgId: process.env.ORG_ID,
  // This is the scope of the token
  token: {
    tag: "public",
    restrictions: {
      enabled: true,
      allowedOrigins: ["https://example.vapi.ai"],
      allowedAssistantIds: ["1cbf8c70-5fd7-4f61-a220-376ab35be1b0"],
      allowTransientAssistant: false,
    },
  },
};

// Get the private key from environment variables
const key = process.env.PRIVATE_KEY;

// Define token options
const options = {
  expiresIn: "1h",
};

// Generate the token using a JWT library or built-in functionality
const token = generateJWT(payload, key, options);
```

### Explanation

- **Payload**: The payload includes the `orgId` representing the organization ID and the `token` object with the scope of the token.
- **Key**: The private key is used to sign the token, ensuring its authenticity.
- **Options**: The `expiresIn` option specifies that the token will expire in 1 hour.
- **Token Generation**: The `generateJWT` function (a placeholder for the actual JWT generation method) creates the token using the provided payload, key, and options.

## Usage (Making an Authenticated API Request)

If you set the scope to `private`, you can use it to make authenticated API requests. The following steps outline how to make an authenticated request:

1. **Define the API Endpoint**: Specify the URL of the API you want to call.
2. **Set the Headers**: Include the `Content-Type` and `Authorization` headers in your request. The `Authorization` header should include the generated JWT token prefixed with `Bearer`.
3. **Make the API Call**: Use an appropriate method to send the request and handle the response.

### Example

```js
async function getAssistants() {
  const response = await fetch("https://api.vapi.ai/assistant", {
    method: "GET",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${token}`,
    },
  });

  const data = await response.json();
  console.log(data);
}

fetchData().catch(console.error);
```

### Explanation

- **API Endpoint**: The URL of the API you want to call.
- **Headers**: The `Content-Type` is set to `application/json`, and the `Authorization` header includes the generated JWT token.
- **API Call**: The `fetchData` function makes an asynchronous GET request to the specified API endpoint and logs the response.

### Usage (Web Client)

If you set the scope to `public`, you can use it to make authenticated API requests using the Vapi Web Client.

```
import Vapi from '@vapi-ai/web';

const vapi = new Vapi({
  token: 'your-jwt-token',
});

vapi.start('your-assistant-id');
```

## Notes

- With the generated token, you can authenticate API requests to any endpoint requiring authentication. The token will be valid for the duration specified in the options (1 hour in this case).
- If you don't specify `token` in the JWT payload, the token will be public.

## Conclusion

This documentation covered the basics of generating a JWT token and demonstrated how to use the token to make authenticated API requests. Ensure that your environment variables (e.g., `ORG_ID` and `PRIVATE_KEY`) are correctly set up before running the code.


 This is the content for the doc fern/customization/multilingual.mdx 

 ---
title: Multilingual
subtitle: Learn how to set up and test multilingual support in Vapi.
slug: customization/multilingual
---


Vapi's multilingual support is primarily facilitated through transcribers, which are part of the speech-to-text process. The pipeline consists of three key elements: text-to-speech, speech-to-text, and the llm model, which acts as the brain of the operation. Each of these elements can be customized using different providers.

## Transcribers (Speech-to-Text)

Currently, Vapi supports two providers for speech-to-text transcriptions:

- `Deepgram` (nova - family models)
- `Talkscriber` (whisper model)

Each provider supports different languages. For more detailed information, you can visit your dashboard and navigate to the transcribers tab on the assistant page. Here, you can see the languages supported by each provider and the available models. **Note that not all models support all languages**. For specific details, you can refer to the documentation for the corresponding providers.

## Voice (Text-to-Speech)

Once you have set your transcriber and corresponding language, you can choose a voice for text-to-speech in that language. For example, you can choose a voice with a Spanish accent if needed.

Vapi currently supports the following providers for text-to-speech:

- `PlayHT`
- `11labs`
- `Rime-ai`
- `Deepgram`
- `OpenAI`
- `Azure`
- `Lmnt`
- `Neets`

Each provider offers varying degrees of language support. Azure, for instance, supports the most languages, with approximately 400 prebuilt voices across 140 languages and variants. You can also create your own custom languages with other providers.

## Multilingual Support

For multilingual support, you can choose providers like Eleven Labs or Azure, which have models and voices designed for this purpose. This allows your voice assistant to understand and respond in multiple languages, enhancing the user experience for non-English speakers.

To set up multilingual support, you no longer need to specify the desired language when configuring the voice assistant. This configuration in the voice section is deprecated.

Instead, you directly choose a voice that supports the desired language from your voice provider. This can be done when you are setting up or modifying your voice assistant.

Here is an example of how to set up a voice assistant that speaks Spanish:

```json
{
  "voice": {
    "provider": "azure",
    "voiceId": "es-ES-ElviraNeural"
  }
}
```

In this example, the voice `es-ES-ElviraNeural` from the provider `azure` supports Spanish. You can replace `es-ES-ElviraNeural` with the ID of any other voice that supports your desired language.

By leveraging Vapi's multilingual support, you can make your voice assistant more accessible and user-friendly, reaching a wider audience and providing a better user experience.


 This is the content for the doc fern/customization/provider-keys.mdx 

 ---
title: Provider Keys
subtitle: Bring your own API keys to Vapi.
slug: customization/provider-keys
---


Have a custom model or voice with one of the providers? Or an enterprise account with volume pricing?

No problem! You can bring your own API keys to Vapi. You can add them in the [Dashboard](https://dashboard.vapi.ai) under the **Provider Keys** tab. Once your API key is validated, you won't be charged when using that provider through Vapi. Instead, you'll be charged directly by the provider.

## Transcription Providers

Currently, the only available transcription provider is `deepgram`. To use a custom model, you can specify the deepgram model ID in the `transcriber.model` parameter of the [Assistant](/api-reference/assistants/create-assistant).

## Model Providers

We are currently have support for any OpenAI-compatible endpoint. This includes services like [OpenRouter](https://openrouter.ai/), [AnyScale](https://www.anyscale.com/), [Together AI](https://www.together.ai/), or your own server.

To use one of these providers, you can specify the `provider` and `model` in the `model` parameter of the [Assistant](/api-reference/assistants/create-assistant).

You can find more details in the [Custom LLMs](/customization/custom-llm/fine-tuned-openai-models) section of the documentation.

## Voice Providers

All voice providers are supported. Once you've validated your API through the [Dashboard](https://dashboard.vapi.ai), any voice ID from your provider can be used in the `voice.voiceId` field of the [Assistant](/api-reference/assistants/create-assistant).

## Cloud Providers

Vapi stores recordings of conversations with assistants in the cloud.  By default, Vapi stores these recordings in its
own bucket in Cloudflare R2.  You can configure Vapi to store recordings in your own bucket in AWS S3, GCP, or
Cloudflare R2. 

You can find more details on how to configure your Cloud Provider keys here:

  * [AWS S3](/providers/cloud/s3)
  * [GCP Cloud Storage](/providers/cloud/gcp)
  * [Cloudflare R2](/providers/cloud/cloudflare)


 This is the content for the doc fern/customization/speech-configuration.mdx 

 ---
title: Speech Configuration
subtitle: Timing control for assistant speech
slug: customization/speech-configuration
---


The Speaking Plan and Stop Speaking Plan are essential configurations designed to optimize the timing of when the assistant begins and stops speaking during interactions with a customer. These plans ensure that the assistant does not interrupt the customer and also prevents awkward pauses that can occur if the assistant starts speaking too late. Adjusting these parameters helps tailor the assistant’s responsiveness to different conversational dynamics.

**Note**: At the moment these configurations can currently only be made via API.

## Start Speaking Plan
This plan defines the parameters for when the assistant begins speaking after the customer pauses or finishes.


- **Wait Time Before Speaking**: You can set how long the assistant waits before speaking after the customer finishes. The default is 0.4 seconds, but you can increase it if the assistant is speaking too soon, or decrease it if there’s too much delay.
**Example:** For tech support calls, set `waitSeconds` for the assistant to more than 1.0 seconds to give customers time to complete their thoughts, even if they have some pauses in between.

- **Smart Endpointing Plan**: This feature uses advanced processing to detect when the customer has truly finished speaking, especially if they pause mid-thought. It can be configured in three ways:
  - **Off**: Disabled by default
  - **LiveKit**: Recommended for English conversations as it provides the most sophisticated solution for detecting natural speech patterns and pauses. LiveKit can be fine-tuned using the `waitFunction` parameter to adjust response timing based on the probability that the user is still speaking.
  - **Vapi**: Recommended for non-English conversations or as an alternative when LiveKit isn't suitable

  ![LiveKit Smart Endpointing Configuration](../static/images/advanced-tab/livekit-smart-endpointing.png)

  **LiveKit Smart Endpointing Configuration:**
  When using LiveKit, you can customize the `waitFunction` parameter which determines how long the bot will wait to start speaking based on the likelihood that the user has finished speaking:
  
  ```
  waitFunction: "200 + 8000 * x"
  ```
  
  This function maps probabilities (0-1) to milliseconds of wait time. A probability of 0 means high confidence the caller has stopped speaking, while 1 means high confidence they're still speaking. The default function (`200 + 8000 * x`) creates a wait time between 200ms (when x=0) and 8200ms (when x=1). You can customize this with your own mathematical expression, such as `4000 * (1 - cos(pi * x))` for a different response curve.

  **Example:** In insurance claims, smart endpointing helps avoid interruptions while customers think through complex responses. For instance, when the assistant asks "do you want a loan," the system can intelligently wait for the complete response rather than interrupting after the initial "yes" or "no." For responses requiring number sequences like "What's your account number?", the system can detect natural pauses between digits without prematurely ending the customer's turn to speak.

- **Transcription-Based Detection**: Customize how the assistant determines that the customer has stopped speaking based on what they’re saying. This offers more control over the timing. **Example:** When a customer says, "My account number is 123456789, I want to transfer $500." 
  - The system detects the number "123456789" and waits for 0.5 seconds (`WaitSeconds`) to ensure the customer isn't still speaking.
  - If the customer were to finish with an additional line, "I want to transfer $500.", the system uses `onPunctuationSeconds` to confirm the end of the speech and then proceed with the request processing.
  - In a scenario where the customer has been silent for a long and has already finished speaking but the transcriber is not confident to punctuate the transcription, `onNoPunctuationSeconds` is used for 1.5 seconds. 


## Stop Speaking Plan
The Stop Speaking Plan defines when the assistant stops talking after detecting customer speech.

- **Words to Stop Speaking**: Define how many words the customer needs to say before the assistant stops talking. If you want immediate reaction, set this to 0. Increase it to avoid interruptions by brief acknowledgments like "okay" or "right". **Example:** While setting an appointment with a clinic, set `numWords` to 2-3 words to allow customers to finish brief clarifications without triggering interruptions.

- **Voice Activity Detection**: Adjust how long the customer needs to be speaking before the assistant stops. The default is 0.2 seconds, but you can tweak this to balance responsiveness and avoid false triggers.
**Example:** For a banking call center, setting a higher `voiceSeconds` value ensures accuracy by reducing false positives. This avoids interruptions caused by background sounds, even if it slightly delays the detection of speech onset. This tradeoff is essential to ensure the assistant processes only correct and intended information.


- **Pause Before Resuming**: Control how long the assistant waits before starting to talk again after being interrupted. The default is 1 second, but you can adjust it depending on how quickly the assistant should resume.
**Example:** For quick queries (e.g., "What’s the total order value in my cart?"), set `backoffSeconds` to 1 second.

Here's a code snippet for Stop Speaking Plan -

```json
 "stopSpeakingPlan": {
    "numWords": 0,
    "voiceSeconds": 0.2,
    "backoffSeconds": 1                                                                
  }
```


## Considerations for Configuration

- **Customer Style**: Think about whether the customer pauses mid-thought or provides continuous speech. Adjust wait times and enable smart endpointing as needed.

- **Background Noise**: If there’s a lot of background noise, you may need to tweak the settings to avoid false triggers. Default for phone calls is ‘office’ and default for web calls is ‘off’.



```json
  "backgroundSound": "off",
```

- **Conversation Flow**: Aim for a balance where the assistant is responsive but not intrusive. Test different settings to find the best fit for your needs.


 This is the content for the doc fern/enterprise/onprem.mdx 

 ---
title: On-Prem Deployments
subtitle: Deploy Vapi in your private cloud.
slug: enterprise/onprem
---


Vapi On-Prem allows you to deploy Vapi's best in class enterprise voice infrastructure AI directly in your own cloud. It can be deployed in a dockerized format on any cloud provider, in any geographic location, running on your GPUs.

With On-Prem, your audio and text data stays in your cloud. Data never passes through Vapi's servers. If you're are handling sensitive data (e.g. health, financial, legal) and are under strict data requirements, you should consider deploying on-prem.

Your device regularly sends performance and usage information to Vapi's cloud. This data helps adjust your device's GPU resources and is also used for billing. All network traffic from your device is tracked in an audit log, letting your engineering or security team see what the device is doing at all times.

## Frequently Asked Questions

#### Can the appliance adjust to my needs?

Yes, the Vapi On-Prem appliance automatically adjusts its GPU resources to handle your workload as required by our service agreement. It can take a few minutes to adjust to changes in your workload. If you need quicker adjustments, you might want to ask for more GPUs by contacting support@vapi.ai.

#### What if I can’t get enough GPUs from my cloud provider?

If you're struggling to get more GPUs from your provider, contact support@vapi.ai for help.

#### Can I access Vapi's AI models?

No, our AI models are on secure machines in your Isolated VPC and you can’t log into these machines or check their files.

#### How can I make sure my data stays within my cloud?

Your device operates in VPCs that you control. You can check the network settings and firewall rules, and look at traffic logs to make sure everything is as it should be. The Control VPC uses open source components, allowing you to make sure the policies are being followed. Performance data and model updates are sent to Vapi, but all other traffic leaving your device is logged, except for the data sent back to your API clients.

## Contact us

For more information about Vapi On-Prem, please contact us at support@vapi.ai


 This is the content for the doc fern/enterprise/plans.mdx 

 ---
title: Vapi Enterprise
subtitle: Build and scale with Vapi.
slug: enterprise/plans
---


If you're building a production application on Vapi, we can help you every step of the way from idea to full-scale deployment.

#### Enterprise Plans include:

- Unlimited concurrency and higher rate limits
- Reserved capacity on our dedicated Enterprise Cluster
- Hands-on 24/7 support with dedicated solutions engineer
- Shared Slack channel with our team
- Regular check-in calls with our team

## Contact us

To get started on Vapi Enterprise, [fill out this form](https://book.vapi.ai).


 This is the content for the doc fern/examples/inbound-support.mdx 

 ---
title: Inbound Support Example ⚙️
subtitle: Let's build a technical support assistant that remembers where we left off.
slug: examples/inbound-support
---


We want a phone number we can call to get technical support. We want the assistant to use a provided set of troubleshooting guides to help walk the caller through solving their issue.

As a bonus, we also want the assistant to remember by the phone number of the caller where we left off if we get disconnected.

<Steps>
  <Step title="Create an assistant">
    We'll start by taking a look at the [Assistant API
    reference](/api-reference/assistants/create-assistant) and define our
    assistant:

    ```json
    {
      "transcriber":{
        "provider": "deepgram",
        "keywords": ["iPhone:1", "MacBook:1.5", "iPad:1", "iMac:0.8", "Watch:1", "TV:1", "Apple:2"],
      },
      "model": {
        "provider": "openai",
        "model": "gpt-4",
        "messages": [
          {
              "role": "system",
              "content": "You're a technical support assistant. You're helping a customer troubleshoot their Apple device. You can ask the customer questions, and you can use the following troubleshooting guides to help the customer solve their issue: ..."
          }
        ]
      },
      "forwardingPhoneNumber": "+16054440129",
      "firstMessage": "Hey, I'm an A.I. assistant for Apple. I can help you troubleshoot your Apple device. What's the issue?",
      "recordingEnabled": true,
    }
    ```
    <Card title="Let's break this down">
    - `transcriber` - We're defining this to make sure the transcriber picks up the custom words related to our devices.
    - `model` - We're using the OpenAI GPT-3.5-turbo model. It's much faster and preferred if we don't need GPT-4.
    - `messages` - We're defining the assistant's instructions for how to run the call.
    - `forwardingPhoneNumber` - Since we've added this, the assistant will be provided the [transferCall](/assistants#transfer-call) function to use if the caller asks to be transferred to a person.
    - `firstMessage` - This is the first message the assistant will say when the user picks up.
    - `recordingEnabled` - We're recording the call so we can hear the conversation later.

</Card>

    Since we want the assistant to remember where we left off, its configuration is going to change based on the caller. So, we're not going to use [temporary assistants](/assistants/persistent-assistants).

    For this example, we're going to store the conversation on our server between calls and use the [Server URL's `assistant-request`](/server-url#retrieving-assistants) to fetch a new configuration based on the caller every time someone calls.

  </Step>
  <Step title="Create a phone number">
    We'll create a phone number for inbound calls using the [Phone Numbers API](/api-reference/phone-numbers/create).

    ```json
    {
      "id": "c86b5177-5cd8-447f-9013-99e307a8a7bb",
      "orgId": "aa4c36ba-db21-4ce0-9c6e-99e307a8a7bb",
      "provider": "vapi",
      "number": "+11234567890",
      "createdAt": "2023-09-29T21:44:37.946Z",
      "updatedAt": "2023-12-08T00:57:24.706Z",
    }
    ```

  </Step>
  <Step title="Configure your Server URL">
    When someone calls our number, we want to fetch the assistant configuration from our server. We'll use the [Server URL's `assistant-request`](/server-url#retrieving-assistants) to do this.

    First, we'll create an endpoint on our server for Vapi to hit. It'll receive messages as shown in the [Assistant Request](/server-url#retrieving-assistants-calling) docs. Once created, we'll add that endpoint URL to the **Server URL** field in the Account page on the [Vapi Dashboard](https://dashboard.vapi.ai).

  </Step>
   <Step title="Save the conversation at the end of the call">
        We'll want to save the conversation at the end of the call for the next time they call. We'll use the [Server URL's `end-of-call-report`](/server-url#end-of-call-report) message to do this.

        At the end of each call, we'll get a message like this:

        ```json
        {
            "message": {
                "type": "end-of-call-report",
                "endedReason": "hangup",
                "call": { Call Object },
                "recordingUrl": "https://vapi-public.s3.amazonaws.com/recordings/1234.wav",
                "summary": "The user mentioned they were having an issue with their iPhone restarting randomly. They restarted their phone, but the issue persisted. They mentioned they were using an iPhone 12 Pro Max. They mentioned they were using iOS 15.",
                "transcript": "Hey, I'm an A.I. assistant for Apple...",
                "messages":[
                {
                    "role": "assistant",
                    "message": "Hey, I'm an A.I. assistant for Apple. I can help you troubleshoot your Apple device. What's the issue?",
                },
                {
                    "role": "user",
                    "message": "Yeah I'm having an issue with my iPhone restarting randomly.",
                },
                ...
                ]
            }
        }
        ```

        We'll save the `call.customer.number` and `summary` fields to our database for the next time they call.
    </Step>
    <Step title="Handle assistant requests.">
    When our number receives a call, Vapi will also hit our server's endpoint with a message like this:

    ```json
    {
        "message": {
            "type": "assistant-request",
            "call": { Call Object },
        }
    }
    ```

    We'll check our database to see if we have a conversation for this caller. If we do, we'll create an assistant configuration like in Step 1 and respond with it:

    ```json
    {
        "assistant": {
            ...
            "model": {
                "provider": "openai",
                "model": "gpt-4",
                "messages": [
                  {
                    "role": "system",
                    "content": "You're a technical support assistant. Here's where we left off: ..."
                  }
                ]
            },
            ...
        }
    }
    ```

    If we don't, we'll just respond with the assistant configuration from Step 1.

    </Step>

  <Step title="Try calling it!">
    We'll call our number and see if it works. Give it a call, and tell it you're having an issue with your iPhone restarting randomly.

    Hang up, and call back. Then ask what the issue was. The assistant should remember where we left off.

  </Step>
</Steps>


 This is the content for the doc fern/examples/outbound-call-python.mdx 

 ---
title: Outbound Calls from Python 📞
subtitle: Some sample code for placing an outbound call using Python
slug: examples/outbound-call-python
---


```python
import requests

# Your Vapi API Authorization token
auth_token = '<YOUR AUTH TOKEN>'
# The Phone Number ID, and the Customer details for the call
phone_number_id = '<PHONE NUMBER ID FROM DASHBOARD>'
customer_number = "+14151231234"

# Create the header with Authorization token
headers = {
    'Authorization': f'Bearer {auth_token}',
    'Content-Type': 'application/json',
}

# Create the data payload for the API request
data = {
    'assistant': {
        "firstMessage": "Hey, what's up?",
        "model": {
            "provider": "openai",
            "model": "gpt-3.5-turbo",
            "messages": [
                {
                    "role": "system",
                    "content": "You are an assistant."
                }
            ]
        },
        "voice": "jennifer-playht"
    },
    'phoneNumberId': phone_number_id,
    'customer': {
        'number': customer_number,
    },
}

# Make the POST request to Vapi to create the phone call
response = requests.post(
    'https://api.vapi.ai/call/phone', headers=headers, json=data)

# Check if the request was successful and print the response
if response.status_code == 201:
    print('Call created successfully')
    print(response.json())
else:
    print('Failed to create call')
    print(response.text)
```


 This is the content for the doc fern/examples/outbound-sales.mdx 

 ---
title: Outbound Sales Example 📞
subtitle: Let's build an outbound sales agent that can schedule appointments.
slug: examples/outbound-sales
---


We want this agent to be able to call a list of leads and schedule appointments. We'll create our assistant, create a phone number for it, then we'll configure our server for function calling to book the appointments.

<Steps>
  <Step title="Create an assistant">
    We'll start by taking a look at the [Assistant API
    reference](/api-reference/assistants/create-assistant) and define our
    assistant:

    ```json
    {
      "transcriber":{
        "provider": "deepgram",
        "keywords": ["Bicky:1"]
      },
      "model": {
        "provider": "openai",
        "model": "gpt-4",
        "messages": [
          {
              "role": "system",
              "content": "You're a sales agent for a Bicky Realty. You're calling a list of leads to schedule appointments to show them houses..."
          }
        ],
        "functions": [
          {
            "name": "bookAppointment",
            "description": "Used to book the appointment.",
            "parameters": {
              "type": "object",
              "properties": {
                "datetime": {
                  "type": "string",
                  "description": "The date and time of the appointment in ISO format."
                }
              }
            }
          }
        ]
      },
      "voice": {
        "provider": "openai",
        "voiceId": "onyx"
      },
      "forwardingPhoneNumber": "+16054440129",
      "voicemailMessage": "Hi, this is Jennifer from Bicky Realty. We were just calling to let you know...",
      "firstMessage": "Hi, this Jennifer from Bicky Realty. We're calling to schedule an appointment to show you a house. When would be a good time for you?",
      "endCallMessage": "Thanks for your time.",
      "endCallFunctionEnabled": true,
      "recordingEnabled": false,
    }
    ```
    Let's break this down:
    - `transcriber` - We're defining this to make sure the transcriber picks up the custom word "Bicky"
    - `model` - We're using the OpenAI GPT-4 model, which is better at function calling.
    - `messages` - We're defining the assistant's instructions for how to run the call.
    - `functions` - We're providing a bookAppointment function with a datetime parameter. The assistant can call this during the conversation to book the appointment.
    - `voice` - We're using the Onyx voice from OpenAI.
    - `forwardingPhoneNumber` - Since we've added this, the assistant will be provided the [transferCall](/assistants#transfer-call) function to use.
    - `voicemailMessage` - If the call goes to voicemail, this message will be played.
    - `firstMessage` - This is the first message the assistant will say when the user picks up.
    - `endCallMessage` - This is the message the assistant will deciding to hang up.
    - `endCallFunctionEnabled` - This will give the assistant the [endCall](/assistants#end-call) function.
    - `recordingEnabled` - We've disabled recording, since we don't have the user's consent to record the call.

    We'll then make a POST request to the [Create Assistant](/api-reference/assistants/create-assistant) endpoint to create the assistant.

  </Step>
  <Step title="Create a phone number">
    We'll create a phone number for outbound calls using the [Phone Numbers API](/phone-calling#set-up-a-phone-number).

    ```json
    {
      "id": "c86b5177-5cd8-447f-9013-99e307a8a7bb",
      "orgId": "aa4c36ba-db21-4ce0-9c6e-99e307a8a7bb",
      "provider": "vapi",
      "number": "+11234567890",
      "createdAt": "2023-09-29T21:44:37.946Z",
      "updatedAt": "2023-12-08T00:57:24.706Z",
    }
    ```

    Great, let's take note of that `id` field- we'll need it later.

  </Step>
  <Step title="Configure your Server URL">
    When the assistant calls that `bookAppointment` function, we'll want to handle that function call and actually book the appointment. We also want to let the user know if booking the appointment was unsuccessful.

    First, we'll create an endpoint on our server for Vapi to hit. It'll receive messages as shown in the [Function Calling](/server-url#function-calling) docs. Once created, we'll add that endpoint URL to the **Server URL** field in the Account page on the [Vapi Dashboard](https://dashboard.vapi.ai).

  </Step>
    <Step title="Handle function calls">
    So now, when the assistant decides to call `bookAppointment`, our server will get something like this:

    ```json
    {
      "message": {
        "type": "function-call",
        "call": { Call Object },
        "functionCall": {
          "name": "bookAppointment",
          "parameters": "{ \"datetime\": \"2023-09-29T21:44:37.946Z\"}"
        }
      }
    }
    ```

    We'll do our own logic to book the appointment, then we'll respond to the request with the result to let the assistant know it was booked:

    ```json
    { "result": "The appointment was booked successfully." }
    ```

    or, if it failed:

    ```json
    { "result": "The appointment time is unavailable, please try another time." }
    ```

    So, when the assistant calls this function, these results will be appended to the conversation, and the assistant will respond to the user knowing the result.

    Great, now we're ready to start calling leads!
    </Step>

  <Step title="Place a call">
    We'll use the [Create Phone Call](/api-reference/calls/create-phone-call) endpoint to place a call to a lead:

    ```json
    {
      "phoneNumberId": "c86b5177-5cd8-447f-9013-99e307a8a7bb",
      "assistantId": "d87b5177-5cd8-447f-9013-99e307a8a7bb",
      "customer": {
        "number": "+11234567890"
      }
    }
    ```

    Since we also defined a `forwardingPhoneNumber`, when the user asks to speak to a human, the assistant will transfer the call to that number automatically.

    We can then check the [Dashboard](https://dashboard.vapi.ai) to see the call logs and read the transcripts.

  </Step>
</Steps>


 This is the content for the doc fern/examples/pizza-website.mdx 

 ---
title: Pizza Website Example 🍕
subtitle: Let's build a pizza ordering assistant for our website.
slug: examples/pizza-website
---


In this example, we'll be using the [Web SDK](https://github.com/VapiAI/web) to create an assistant that can take a pizza order. Since all the [Client SDKs](/sdks) have equivalent functionality, you can use this example as a guide for any Vapi client.

We want to add a button to the page to start a call, update our UI with the call status, and display what the user's saying while they say it. When the user mentions a topping, we should add it to the pizza. When they're done, we should redirect them to checkout.

<Steps>
  <Step title="Create an assistant">
    We'll start by taking a look at the [Assistant API
    reference](/api-reference/assistants/create-assistant) and define our
    assistant:

    ```json
    {
      "model": {
        "provider": "openai",
        "model": "gpt-4",
        "messages": [
          {
              "role": "system",
              "content": "You're a pizza ordering assistant. The user will ask for toppings, you'll add them. When they're done, you'll redirect them to checkout."
          }
        ],
        "functions": [
          {
            "name": "addTopping",
            "description": "Used to add a topping to the pizza.",
            "parameters": {
              "type": "object",
              "properties": {
                "topping": {
                  "type": "string",
                  "description": "The name of the topping. For example, 'pepperoni'."
                }
              }
            }
          },
           {
            "name": "goToCheckout",
            "description": "Redirects the user to checkout and order their pizza.",
            "parameters": {"type": "object", "properties": {}}
          }
        ]
      },
      "firstMessage": "Hi, I'm the pizza ordering assistant. What toppings would you like?",
    }
    ```
    Let's break this down:
    - `model` - We're using the OpenAI GPT-4 model, which is better at function calling.
    - `messages` - We're defining the assistant's instructions for how to run the call.
    - `functions` - We're providing a addTopping function with a topping parameter. The assistant can call this during the conversation to add a topping. We're also adding goToCheckout, with an empty parameters object. The assistant can call this to redirect the user to checkout.
    - `firstMessage` - This is the first message the assistant will say when the user starts the call.

    We'll then make a POST request to the [Create Assistant](/api-reference/assistants/create-assistant) endpoint to create the assistant.

  </Step>
  <Step title="Set up the Web SDK">
    We'll follow the `README` for the [Web SDK](https://github.com/VapiAI/web) to get it installed.

    We'll then get our **Public Key** from the [Vapi Dashboard](https://dashboard.vapi.ai) and initialize the SDK:

    ```js
    import Vapi from '@vapi-ai/web';

    const vapi = new Vapi('your-web-token');
    ```

  </Step>
  <Step title="Add the call buttons">
    We'll add a button to the page that starts the call when clicked:

    ```html
    <button id="start-call">Start Call</button>
    <button id="stop-call">Stop Call</button>
    ```

    ```js
    const startCallButton = document.getElementById('start-call');

    startCallButton.addEventListener('click', async () => {
      await vapi.start('your-assistant-id');
    });

    const stopCallButton = document.getElementById('stop-call');

    stopCallButton.addEventListener('click', async () => {
      await vapi.stop();
    });
    ```

  </Step>
  <Step title="Handle call status events">
    ```js
    vapi.on('call-start', () => {
      // Update UI to show that the call has started
    });

    vapi.on('call-end', () => {
      // Update UI to show that the call has ended
    });
    ```

  </Step>

<Step title="Handle speaking events">
  ```js
  vapi.on('speech-start', () => {
    // Update UI to show that the assistant is speaking
  });

vapi.on('speech-end', () => {
// Update UI to show that the assistant is done speaking
});

````

</Step>

<Step title="Handle transcription events">
  All messages send to the [Server URL](/server-url), including `transcript` and `function-call` messages, are also sent to the client as `message` events. We'll need to check the `type` of the message to see what type it is.

```js
vapi.on("message", (msg) => {
  if (msg.type !== "transcript") return;

  if (msg.transcriptType === "partial") {
    // Update UI to show the live partial transcript
  }

  if (msg.transcriptType === "final") {
    // Update UI to show the final transcript
  }
});
````

</Step>

<Step title="Handle function call events">
```javascript
vapi.on('message', (msg) => {
  if (msg.type !== "function-call") return;

if (msg.functionCall.name === "addTopping") {
const topping = msg.functionCall.parameters.topping;
// Add the topping to the pizza
}

if (msg.functionCall.name === "goToCheckout") {
// Redirect the user to checkout
}
});

```
</Step>
<Step title="Order your pizza!">
You should now have a working pizza ordering assistant! 🍕
</Step></Steps>


```


 This is the content for the doc fern/examples/voice-widget.mdx 

 ---
title: Web Snippet
subtitle: >-
  Easily integrate the Vapi Voice Widget into your website for enhanced user
  interaction.
slug: examples/voice-widget
---


Improve your website's user interaction with the Vapi Voice Widget. This robust tool enables your visitors to engage with a voice assistant for support and interaction, offering a smooth and contemporary way to connect with your services.

## Steps for Installation

<Steps>
  <Step title="Insert the Widget Snippet">
    Copy the snippet below and insert it into your website's HTML, ideally before the closing `</body>` tag.

    ```html
    <script>
      var vapiInstance = null;
      const assistant = "<assistant_id>"; // Substitute with your assistant ID
      const apiKey = "<your_public_api_key>"; // Substitute with your Public key from Vapi Dashboard.
      const buttonConfig = {}; // Modify this as required

      (function (d, t) {
        var g = document.createElement(t),
          s = d.getElementsByTagName(t)[0];
        g.src =
          "https://cdn.jsdelivr.net/gh/VapiAI/html-script-tag@latest/dist/assets/index.js";
        g.defer = true;
        g.async = true;
        s.parentNode.insertBefore(g, s);

        g.onload = function () {
          vapiInstance = window.vapiSDK.run({
            apiKey: apiKey, // mandatory
            assistant: assistant, // mandatory
            config: buttonConfig, // optional
          });
        };
      })(document, "script");

    </script>
    ```

  </Step>
  <Step title="Generate Your Assistant">
    From your Vapi dashboard, create an assistant to get the assistant ID. Alternatively, define an assistant configuration directly in your website's code as demonstrated in the example below.
    ```javascript
    const assistant = {
      model: {
        provider: "openai",
        model: "gpt-3.5-turbo",
        systemPrompt:
          "You're a versatile AI assistant named Vapi who is fun to talk with.",
      },
      voice: {
        provider: "11labs",
        voiceId: "paula",
      },
      firstMessage: "Hi, I am Vapi how can I assist you today?",
    };
    ```

  </Step>
  <Step title="Modify the Button">
    Modify the `buttonConfig` object to align with your website's style and branding. Choose between a pill or round button and set colors, positions, and icons.
    ```javascript
    const buttonConfig = {
      position: "bottom-right", // "bottom" | "top" | "left" | "right" | "top-right" | "top-left" | "bottom-left" | "bottom-right"
      offset: "40px", // decide how far the button should be from the edge
      width: "50px", // min-width of the button
      height: "50px", // height of the button
      idle: { // button state when the call is not active.
        color: `rgb(93, 254, 202)`, 
        type: "pill", // or "round"
        title: "Have a quick question?", // only required in case of Pill
        subtitle: "Talk with our AI assistant", // only required in case of pill
        icon: `https://unpkg.com/lucide-static@0.321.0/icons/phone.svg`,
      },
      loading: { // button state when the call is connecting
        color: `rgb(93, 124, 202)`,
        type: "pill", // or "round"
        title: "Connecting...", // only required in case of Pill
        subtitle: "Please wait", // only required in case of pill
        icon: `https://unpkg.com/lucide-static@0.321.0/icons/loader-2.svg`,
      },
      active: { // button state when the call is in progress or active.
        color: `rgb(255, 0, 0)`,
        type: "pill", // or "round"
        title: "Call is in progress...", // only required in case of Pill
        subtitle: "End the call.", // only required in case of pill
        icon: `https://unpkg.com/lucide-static@0.321.0/icons/phone-off.svg`,
      },
    };
    ```
  </Step>

  <Step title="Add Functionality to Vapi Instance">
    You can use the `vapiInstance` returned from the run function in the snippet to further customize the behaviour. For instance, you might want to listen to various EventSource, or even send some messages to the bot programmatically.

    ```js
      vapiInstance.on('speech-start', () => {
        console.log('Speech has started');
      });

      vapiInstance.on('speech-end', () => {
        console.log('Speech has ended');
      });

      vapiInstance.on('call-start', () => {
        console.log('Call has started');
      });

      vapiInstance.on('call-end', () => {
        console.log('Call has stopped');
      });

      vapiInstance.on('volume-level', (volume) => {
        console.log(`Assistant volume level: ${volume}`);
      });

      // Function calls and transcripts will be sent via messages
      vapiInstance.on('message', (message) => {
        console.log(message);
      });

      vapiInstance.on('error', (e) => {
        console.error(e)
      });
    ```

  </Step>
</Steps>

## Customization

Modify your assistant's behavior and the initial message users will see. Refer to the provided examples to customize the assistant's model, voice, and initial greeting.

## UI Customization

For advanced styling, target the exposed CSS and other classes to ensure the widget's appearance aligns with your website's design. Here is a list of the classes you can customize:

- `.vapi-btn`: The primary class for the Vapi button.
- `.vapi-btn-is-idle`: The class for the Vapi button when the call is disconnected.
- `.vapi-btn-is-active`: The class for the Vapi button when the call is active.
- `.vapi-btn-is-loading`: The class for the Vapi button when the call is connecting.
- `.vapi-btn-is-speaking`: The class for the Vapi button when the bot is speaking.
- `.vapi-btn-pill`: The class for Vapi button to set pill variant.
- `.vapi-btn-round`: The class for Vapi button to set round variant.


 This is the content for the doc fern/faq.mdx 

 ---
title: Frequently Asked Questions
subtitle: Frequently asked questions about Vapi.
slug: faq
---

<Markdown src="./snippets/faq-snippet.mdx" />


 This is the content for the doc fern/glossary.mdx 

 ---
title: Definitions
subtitle: Useful terms and definitions for Vapi & voice AI applications.
slug: glossary
---


## A

### At-cost

"At-cost" is often use when discussing pricing. It means "without profit to the seller". Vapi charges at-cost for requests made to [STT](/glossary#stt), [LLM](/glossary#large-language-model), & [TTS](/glossary#tts) providers.

## B

### Backchanneling

A [backchannel](<https://en.wikipedia.org/wiki/Backchannel_(linguistics)>) occurs when a listener provides verbal or non-verbal feedback to a speaker during a conversation.

Examples of backchanneling in English include such expressions as "yeah", "OK", "uh-huh", "hmm", "right", and "I see".

This feedback is often not semantically significant to the conversation, but rather serves to signify the listener's attention, understanding, sympathy, or agreement.

## E

### Endpointing

See [speech endpointing](/glossary#speech-endpointing).

## I

### Inbound Call

This is a call received by an assistant **_from_** another phone number (w/ the assistant being the "person" answering). The call comes **"in"**-ward to a number (from an external caller) — hence the term "inbound call".

### Inference

You may often hear the term "run inference" when referring to running a large language model against an input prompt to receive text output back out.

The process of running a prompt against an LLM for output is called "inference".

## L

### Large Language Model

Large Language Models (or "LLM", for short) are machine learning models trained on large amounts of text, & later used to generate text in a probabilistic manner, "token-by-token".

For further reading see [large language model wiki](https://en.wikipedia.org/wiki/Large_language_model).

### LLM

See [Large Language Model](/glossary#large-language-model).

## O

### Outbound Call

This is a call made by an assistant **_to_** another target phone number (w/ the assistant being the "person" dialing). The call goes **"out"**-ward to another number — hence the term "outbound call".

## S

### Server URL

A "server url" is an endpoint you expose to Vapi to receive conversation data in real-time. Server urls can reply with meaningful responses, distinguishing them from traditional [webhooks](/glossary#webhook).

See our [server url](/server-url) guide to learn more.

### SDK

Stands for "Software Development Kit" — these are pre-packaged libraries & platform-specific building tools that a software publisher creates to expedite & increase the ease of integration for developers.

### Speech Endpointing

Speech endpointing is the process of detecting the start and end of (a line of) speech in an audio signal. This is an important function in conversation turn detection.

A starting heuristic for the end of a user's speech is the detection of silence. If someone does not speak for a certain amount of milliseconds, the utterance can be considered complete.

A more robust & ideal approach is to actually understand what the user is saying (as well as the current conversation's state & the speech turn's intent) to determine if the user is just pausing for effect, or actually finished speaking.

Vapi uses a combination of silence detection and machine learning models to properly endpoint conversation speech (to prevent improper interruption & encourage proper [backchanneling](/glossary#backchanneling)).

Additional reading on speech endpointing can be found [here](https://en.wikipedia.org/wiki/Speech_segmentation) & on [Deepgram's docs](https://developers.deepgram.com/docs/endpointing).

### STT

An abbreviation used for "Speech-to-text". The process of converting physical sound waves into raw transcript text (a process called "transcription").

## T

### Telemarketing Sales Rule

The Telemarketing Sales Rule (or "TSR" for short) is a regulation established by the Federal Trade Commission ([ftc.gov](https://www.ftc.gov/)) in the United States to protect consumers from deceptive and abusive telemarketing practices.

**You may only conduct outbound calls to phone numbers which you have consent to contact.** Violating TSR rules can result in significant civil (or even criminal) penalties.

Learn more on the [FCC website](https://www.ftc.gov/legal-library/browse/rules/telemarketing-sales-rule).

### TTS

An abbreviation used for "Text-to-speech". The process of converting raw text into playable audio data.

## V

### Voice-to-Voice

"Voice-to-voice" is often a term brought up in discussing voice AI system latency — the time it takes to go from a user finishing their speech (however that endpoint is computed) → to the AI agent's first speech chunk/byte being played back on a client’s device.

Ideally, this process should happen in \<1s, better if closer to 500-700ms (responding too quickly can be an issue as well). Voice AI applications must closely watch this metric to ensure their applications stay responsive & usable.

## W

### Webhook

A webhook is a server endpoint you expose to external services with the intention of receiving external data in real-time. Your exposed URL is essentially a "drop-bin" for data to come in from external providers to update & inform your systems.

Traditionally, webhooks are unidirectional & stateless. Endpoints only reply with status code to signal acknowledgement.

<Info>
  To make the distinction clear, Vapi calls these "[server urls](/server-url)".
  Certain requests made to your server (like assistant requests) require a reply
  with meaningful data.
</Info>


 This is the content for the doc fern/how-vapi-works.mdx 

 ---
title: Orchestration Models
subtitle: All the fancy stuff Vapi does on top of the core models.
slug: how-vapi-works
---


Vapi also runs a suite of audio and text models that make it's latency-optimized Speech-to-Text (STT), Large Language Model (LLM), & Text-to-Speech (TTS) pipeline feel human.

Here's a high-level overview of the Vapi architecture:

<Frame>
  <img src="./static/images/learn/platform/vapi-orchestration.png" />
</Frame>

These are some of the models that are part of the Orchestration suite. We currently have lots of other models in the pipeline that will be added to the orchestration suite soon. The ultimate goal is to achieve human performance.

### Endpointing

Endpointing is a fancy word for knowing when the user is done speaking. Traditional methods use silence detection with a timeout. Unfortunately, if we want sub-second response-times, that's not going to work.

Vapi's uses a custom fusion audio-text model to know when a user has completed their turn. Based on both the user's tone and what they're saying, it decides how long to pause before hitting the LLM.

This is critical to make sure the user isn't interrupted mid-thought while still providing sub-second response times when they're done speaking.

### Interruptions (Barge-in)

Interruptions (aka. barge-in in research circles) is the ability to detect when the user would like to interject and stop the assistant's speech.

Vapi uses a custom model to distinguish when there is a true interruption, like "stop", "hold up", "that's not what I mean, and when there isn't, like "yeah", "oh gotcha", "okay."

It also keeps track of where the assistant was cut off, so the LLM knows what it wasn't able to say.

### Background Noise Filtering

Many of our models, including the transcriber, are audio-based. In the real world, things like music and car horns can interfere with model performance.

We use a proprietary real-time noise filtering model to ensure the audio is cleaned without sacrificing latency, before it reaches the inner models of the pipeline.

### Background Voice Filtering

We rely quite heavily on the transcription model to know what's going on, for interruptions, endpointing, backchanneling, and for the user's statement passed to the LLM.

Transcription models are built to pick up everything that sounds like speech, so this can be a problem. As you can imagine, having a TV on in the background or echo coming back into the mic can severely impact the conversation ability of a system like Vapi.

Background noise cancellation is a well-researched problem. Background voice cancellation is not. To solve this, we built proprietary audio filtering model that's able to **focus in** on the primary speaker and block everything else out.

### Backchanneling

Humans like to affirm each other while they speak with statements like "yeah", "uh-huh", "got it", "oh no!"

They're not considered interruptions, they're just used to let the speaker know that their statement has been understood, and encourage the user to continue their statement.

A backchannel cue used at the wrong moment can derail a user's statement. Vapi uses a proprietary fusion audio text model to determine the best moment to backchannel and to decide which backchannel cue is most appropriate to use.

### Emotion Detection

How a person says something is just as important as what they're saying. So we've trained a real-time audio model to extract the emotional inflection of the user's statement.

This emotional information is then fed into the LLM, so knows to behave differently if the user is angry, annoyed, or confused.

### Filler Injection

The output of LLMs tends to be formal, and not conversational. People speak with phrases like "umm", "ahh", "i mean", "like", "so", etc.

You can prompt the model to output like this, but we treat our user's prompts as **sacred**. Making a change like this to a prompt can change the behavior in unintended ways.

To ensure we don't add additional latency transforming the output, we've built a custom model that's able to convert streaming input and make it sound conversational in real-time.


 This is the content for the doc fern/info-hierarchy.mdx 

 ### Information Hierarchy

#### Current
* Overview
* Platform
    * Assistants
    * Phone Numbers
    * Files
    * Tools
    * Blocks
    * Squads
* Voice Library
* Logs
    * Calls
    * API Requests
    * Webhooks

#### Proposed Dashboard Hierarchy
* Overview
* Build
    * Assistants
    * Workflows
    * Phone Numbers
    * Tools
    * Files
    * Squads
* Test
    * Voice Test Suites
* Observe
    * Call Logs
    * API Logs
    * Webhook Logs
* Community
    * Task Library
    * Workflow Library
    * Voice Library
    * Model Library
* Profile
* Organizations
    * LIST
* Admin
    * Billing
    * Members
    * Settings
    * API Keys
    * Provider Credentials
* Light/Dark Toggle
* Log Out

#### Docs Hierarchy
* Getting Started
* Build
    * Assistants
    * Workflows &lt;-- 
    * Tools
    * Knowledge Base
    * Squads
* Test
    * Voice Testing &lt;-- 
* Deploy
    * Phone Numbers
    * Calls
* Community
    *  Tasks
    *  Workflows
    *  Voices
    *  Models
    *  Transcribers
* Admin
    * Billing
    * Org -- Enterprise
    * Org management
    * Provider Keys

 This is the content for the doc fern/introduction.mdx 

 ---
title: Introduction to Vapi
subtitle: Vapi is the Voice AI platform for developers.
slug: introduction
---

<Frame>
  <img src="./static/images/intro/custom-vs-vapi.png" />
</Frame>

Vapi lets developers build, test, & deploy voice AI agents in minutes rather than months — solving for the foundational challenges voice AI applications face:

<CardGroup cols={2}>
  <Card
    title="Simulating the Flow of Natural Human Conversation"
    icon="people-arrows"
        color="#e6aa20"
  >
    Turn-taking, interruption handling, backchanneling, and more.
  </Card>
  <Card title="Realtime/Low Latency Demands" icon="timer" iconType="regular" color="#f25130">
    Responsive conversation demands low latency. Internationally. (\<500-800ms voice-to-voice).
  </Card>
  <Card title="Taking Actions (Function Calling)" icon="function" iconType="regular">
    Taking actions during conversation, getting data to your services for custom actions.
  </Card>
  <Card title="Extracting Conversation Data" icon="waves-sine" iconType="regular" color="#CC42F9">
    Review conversation audio, transcripts, & metadata.
  </Card>
</CardGroup>

<Info>
  Implemented from scratch, this functionality can take months to build, and
  large, continuous, resources to maintain & improve.
</Info>

Vapi abstracts away these complexities, allowing developers to focus on the core of their voice AI application's business logic. **Shipping in days, not months.**

## Quickstart Guides

Get up & running in minutes with one of our [quickstart](/quickstart) guides:

#### No Code

<CardGroup cols={2}>
  <Card
    title="Dashboard Quickstart"
    icon="browser"
    iconType="solid"
    color="#54a7ff"
    href="/quickstart/dashboard"
  >
    The easiest way to start with Vapi. Run a voice agent in minutes.
  </Card>
  <Card
    title="Inbound Calling"
    icon="phone-arrow-down-left"
    iconType="solid"
    color="#54a7ff"
    href="/quickstart/phone/inbound"
  >
    Quickly get started handling inbound phone calls.
  </Card>
  <Card
    title="Outbound Calling"
    icon="phone-arrow-up-right"
    iconType="solid"
    color="#54a7ff"
    href="/quickstart/phone/outbound"
  >
    Quickly get started sending outbound phone calls.
  </Card>
</CardGroup>

#### Platform-Specific

<CardGroup cols={2}>
  <Card
    title="Web Quickstart"
    icon="window"
    iconType="duotone"
    color="#54a7ff"
    href="/quickstart/web"
  >
    Quickly get started making web calls. Web developers, this is for you.
  </Card>
</CardGroup>

## Examples

Explore end-to-end examples for some common voice workflows:

<CardGroup cols={2}>
  <Card
    title="Outbound Sales"
    icon="phone-office"
    iconType="solid"
    color="#f5ac4c"
    href="/examples/outbound-sales"
  >
    We’ll build an outbound sales agent that can schedule appointments.
  </Card>
  <Card
    title="Inbound Support"
    icon="phone"
    iconType="solid"
    color="#f5ac4c"
    href="/examples/inbound-support"
  >
    We’ll build an technical support assistant that remembers where we left off.
  </Card>
  <Card
    title="Pizza Website"
    icon="browser"
    iconType="duotone"
    color="#f5ac4c"
    href="/examples/pizza-website"
  >
    We'll build an order taking agent for our pizza website.
  </Card>
</CardGroup>

## Key Concepts

Gain a deep understanding of key concepts in Vapi, as well as how Vapi works:

#### Core Concepts

<CardGroup cols={2}>
  <Card title="Assistants" icon="robot" iconType="duotone" href="/assistants">
    Assistants set the foundation for applications built on Vapi.
  </Card>
  <Card
    title="Server URLs"
    icon="webhook"
    iconType="duotone"
    href="/server-url"
  >
    Server URLs allow Vapi to deliver your application data in realtime.
  </Card>
  <Card
    title="Phone Calling"
    icon="phone-volume"
    iconType="solid"
    href="/phone-calling"
  >
    Learn the ins-and-outs of telephony & conducting phone calls on Vapi.
  </Card>
  <Card
    title="Privacy"
    icon="signature-lock"
    iconType="duotone"
    href="security-and-privacy/hipaa"
  >
    Learn about privacy concepts like HIPAA & data privacy on Vapi.
  </Card>
</CardGroup>

#### Platform

<CardGroup cols={2}>
  <Card
    title="How Vapi Works"
    icon="network-wired"
    iconType="solid"
    href="/quickstart"
  >
    Learn what goes on behind-the-scenes to make Vapi work.
  </Card>
</CardGroup>

## Explore Our SDKs

Our SDKs are open source, and available on [our GitHub](https://github.com/VapiAI):

<CardGroup cols={3}>
  <Card
    title="Vapi Web"
    icon="window"
    iconType="duotone"
    color="#ffdd03"
    href="/sdk/web"
  >
    Add a Vapi assistant to your web application.
  </Card>
  <Card
    title="Vapi iOS"
    icon="mobile-notch"
    color="#ffdd03"
    href="https://github.com/VapiAI/ios"
  >
    Add a Vapi assistant to your iOS app.
  </Card>
  <Card
    title="Vapi Flutter"
    icon="mobile-notch"
    color="#ffdd03"
    href="https://github.com/VapiAI/flutter"
  >
    Add a Vapi assistant to your Flutter app.
  </Card>
  <Card
    title="Vapi React Native"
    icon="mobile-notch"
    color="#ffdd03"
    href="https://github.com/VapiAI/react-native-sdk"
  >
    Add a Vapi assistant to your React Native app.
  </Card>
  <Card
    title="Vapi Python"
    icon="fa-brands python"
    color="#ffdd03"
    href="https://github.com/VapiAI/python"
  >
    Multi-platform. Mac, Windows, and Linux.
  </Card>
</CardGroup>

## FAQ

Common questions asked by other users:

<Markdown src="./snippets/faq-snippet.mdx" />

## Get Support

Join our Discord to connect with other developers & connect with our team:

<CardGroup cols={2}>
  <Card
    title="Join Our Discord"
    icon="fa-brands fa-discord"
    iconType="solid"
    color="#5A65EA"
    href="https://discord.gg/pUFNcf2WmH"
  >
    Connect with our team & other developers using Vapi.
  </Card>
  <Card
    title="Email Support"
    icon="mailbox"
    iconType="solid"
    color="#7a7f85"
    href="https://tally.so/r/3yD9Wx"
  >
    Send our support team an email.
  </Card>
</CardGroup>


 This is the content for the doc fern/knowledge-base/integrating-with-trieve.mdx 

 ---
title: Bring your own chunks/vectors from Trieve
subtitle: Using Trieve for improved RAG with Vapi
slug: knowledge-base/integrating-with-trieve
---

<Frame>
  <div class="video-embed-wrapper">
    <iframe
      src="https://www.youtube.com/embed/KZkYPSJPPk8?si=IFyBAL1zAXrUwv2L"
      title='An embedded YouTube video titled "Quickstart: Building a Hotel Voice Agent with Vapi and Trieve"'
      frameborder="0"
      allow="fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      referrerpolicy="strict-origin-when-cross-origin"
    />
  </div>
</Frame>

# Using Trieve with Vapi

Vapi integrates with [Trieve](https://trieve.ai) through the BYOD (Bring Your Own Dataset) approach, allowing you to use your Trieve API key to import your existing Trieve datasets into Vapi.

## Integrating with Trieve

The BYOD approach offers flexibility and control over your datasets. You can:

- Fully manage your datasets in Trieve's native interface
- Use Trieve's advanced features like:
  - Custom chunking rules
  - Search playground testing
  - Manual chunk editing
  - Website crawling
  - Dataset visualization

### Step 1: Set Up Trieve Dataset

1. Create an account at [Trieve](https://trieve.ai)
2. Create a new dataset using Trieve's dashboard

![Create dataset in Trieve](../static/images/knowledge-base/create-dataset.png)

When creating your dataset in Trieve, selecting the right embedding model is crucial for optimizing performance and accuracy. Here are some of the available options:

### jina-base-en

- **Provider**: Jina AI (Hosted by Trieve)
- **Performance**: Fast
- **Description**: This model is designed for speed and efficiency, making it suitable for applications where quick response times are critical. It provides a good balance of performance and accuracy for general use cases.

### text-embedding-3-small

- **Provider**: OpenAI
- **Performance**: Moderate
- **Description**: A smaller model from OpenAI that offers a compromise between speed and accuracy. It is suitable for applications that require a balance between computational efficiency and the quality of embeddings.

### text-embedding-3-large

- **Provider**: OpenAI
- **Performance**: Slow
- **Description**: This larger model provides the highest accuracy among the options but at the cost of slower processing times. It is ideal for applications where the quality of embeddings is prioritized over speed.

3. Add content through various methods:

#### Upload Documents

Upload documents directly through Trieve's interface:

![Upload files in Trieve](../static/images/knowledge-base/upload-files.png)

When uploading files, you can configure advanced chunking options:

![Upload files advanced options in Trieve](../static/images/knowledge-base/upload-files-advanced.png)

#### Edit Individual Chunks

After uploading documents, you can edit individual chunks to refine their content:

![Edit chunk interface in Trieve](../static/images/knowledge-base/edit-chunk.png)

##### Editing Options

- **Chunk Content**: Modify the text directly in the rich text editor

  - Fix formatting issues
  - Correct errors or typos
  - Split or combine chunks manually
  - Add or remove content

- **Metadata Fields**:
  - Date: Update document timestamps
  - Number Value: Adjust numeric metadata for filtering
  - Location: Set or modify geographical coordinates
  - Weight: Fine-tune search relevance with custom weights
  - Fulltext Boost: Add terms to enhance search visibility
  - Semantic Boost: Adjust vector embedding influence

##### Best Practices for Chunk Editing

1. **Content Length**

   - Keep chunks between 200-1000 tokens
   - Maintain logical content boundaries
   - Ensure complete thoughts within each chunk

2. **Metadata Optimization**

   - Use consistent date formats
   - Add relevant numeric values for filtering
   - Apply weights strategically for important content

3. **Search Enhancement**
   - Use boost terms for critical keywords
   - Balance semantic and fulltext boosts
   - Test search results after significant edits

### Advanced Chunking Options

#### Metadata

- Add custom metadata as JSON to associate with your chunks
  - Useful for filtering and organizing content (e.g., `{"author": "John Doe", "category": "technical"}`)
  - Keep metadata concise and relevant to avoid storage overhead
  - Use consistent keys across related documents for better searchability

#### Date

- Specify the creation or relevant date for the document
  - Important for version control and content freshness
  - Helps with filtering outdated information
  - Use actual document creation dates when possible

#### Split Delimiters

- Define custom delimiters (e.g., ".,?\n") to control where chunks are split
  - Recommended defaults: ".,?\n" for general content
  - Add semicolons (;) for technical documentation
  - Use "\n\n" for markdown or structured content
  - Avoid over-aggressive splitting that might break context

#### Target Splits Per Chunk

- Set the desired number of splits per chunk
  - Default: 20 splits
  - Recommended ranges:
    - 15-25 for general content
    - 10-15 for technical documentation
    - 25-30 for narrative content
  - Lower values create more granular chunks, better for precise retrieval
  - Higher values maintain more context but may retrieve irrelevant information

#### Rebalance Chunks

- Enable to redistribute content evenly across chunks
  - Recommended for documents with varying section lengths
  - Helps maintain consistent chunk sizes
  - May slightly impact natural content boundaries
  - Best used with technical documentation or structured content

#### Use gpt4o chunking

- Enable GPT-4 optimized chunking for improved semantic coherence
  - Recommended for:
    - Complex technical documentation
    - Content with intricate relationships
    - Documents where context preservation is crucial
  - Note: Increases processing time and cost
  - Best for high-value content where accuracy is paramount

#### Heading Based Chunking

- Split content based on document headings
  - Ideal for well-structured documents (e.g., documentation, reports)
  - Works best with consistent heading hierarchy
  - Consider enabling for:
    - Technical documentation
    - User manuals
    - Research papers
  - May create uneven chunk sizes based on section lengths

#### System Prompt

- Provide custom instructions for the chunking process
  - Optional but powerful for specific use cases
  - Example prompts:
    - "Preserve code blocks as single chunks"
    - "Keep API endpoint descriptions together"
    - "Maintain question-answer pairs in the same chunk"
  - Keep prompts clear and specific
  - Test different prompts with sample content to optimize results

#### Website Crawling

Trieve offers powerful website crawling capabilities with extensive configuration options:

![Website crawling in Trieve](../static/images/knowledge-base/crawl.png)

##### Crawl Configuration Options

- **Crawl Interval**: Set how often to refresh content

  - Options: Daily, Weekly, Monthly
  - Recommended: Daily for frequently updated content

- **Page Limit**: Control the maximum number of pages to crawl

  - Default: 1000 pages
  - Adjust based on your site size and content relevance

- **URL Patterns**

  - Include/Exclude specific URL patterns using regex
  - Example includes: `https://docs.example.com/*`
  - Example excludes: `https://example.com/internal/*`

- **Query Selectors**

  - Include specific HTML elements for targeted content extraction
  - Exclude navigation, footers, and other non-content elements
  - Common excludes: `navbar`, `footer`, `aside`, `nav`, `form`

- **Special Content Types**

  - OpenAPI Spec: Toggle for API documentation crawling
  - Shopify: Enable for e-commerce content
  - YouTube Channel: Include video transcripts and descriptions

- **Advanced Options**
  - Boost Titles: Increase weight of page titles in search results
  - Allow External Links: Include content from linked domains
  - Ignore Sitemap: Skip sitemap-based crawling
  - Remove Strings: Clean up headers and body content

##### Best Practices for Crawling

1. **Start Small**

   - Begin with a low page limit
   - Test with specific sections of your site
   - Gradually expand coverage

2. **Optimize Selectors**

   - Remove navigation and UI elements
   - Focus on main content areas
   - Use browser inspector to identify key selectors

3. **Monitor Performance**
   - Check crawl logs regularly
   - Adjust patterns based on results
   - Balance frequency with server load

### Step 2: Test and Refine

Use Trieve's search playground to:

- Test semantic search queries
- Adjust chunk sizes
- Edit chunks manually

- Visualize vector embeddings
- Fine-tune relevance scores

![Search playground in Trieve](../static/images/knowledge-base/search-playground.png)

### Step 3: Import to Vapi

1. Create your Trieve API key from [Trieve's dashboard](https://dashboard.trieve.ai/org/keys)
2. Add your Trieve API key to Vapi [Provider Credentials](https://dashboard.vapi.ai/keys)
   ![Add Trieve API key in Vapi](../static/images/knowledge-base/trieve-credential.png)
3. Once your dataset is optimized in Trieve, import it to Vapi via POST request to the [create knowledge base route](https://docs.vapi.ai/api-reference/knowledge-bases/create):

```json
{
  "name": "trieve-dataset",
  "provider": "trieve",
  "searchPlan": {
    "scoreThreshold": 0.2,
    "searchType": "semantic"
  },
  "createPlan": {
    "type": "import",
    "providerId": "<Your Trieve Dataset ID>"
  }
}
```

## Best Practices

1. **Dataset Organization**

   - Segment datasets by domain knowledge boundaries
   - Use semantic-based dataset naming (e.g., "api-docs-v2", "user-guides-2024")
   - Version control chunking configurations in your codebase

2. **Content Quality**

   - Implement text normalization (Unicode normalization, whitespace standardization)
   - Use regex patterns to clean formatting artifacts
   - Validate chunk semantic coherence through embedding similarity scores

3. **Performance Optimization**

   - Target chunk sizes: 200-1000 tokens (optimal for current embedding models)
   - Configure hybrid search with BM25 boost = 0.3 for technical content
   - Set score thresholds dynamically based on embedding model (0.2 for text-embedding-3-small, 0.25 for text-embedding-3-large)

4. **Maintenance**
   - Implement automated content refresh cycles via Trieve's API
   - Track search result relevance metrics (MRR, NDCG)
   - Rotate API keys on 90-day cycles

## Troubleshooting

Common issues and solutions:

1. **Search Relevance Issues**

   - Implement cross-encoder reranking for critical queries
   - Fine-tune BM25 vs semantic weights (recommended ratio: 0.3:0.7)
   - Analyze chunk boundary overlap percentage (aim for 15-20%)

2. **Integration Errors**

   - Validate dataset permissions (READ_DATASET scope required)
   - Check for dataset ID format compliance (UUID v4)
   - Monitor rate limits (default: 100 requests/min)

3. **Performance Optimization**
   - Implement chunk size normalization (max variance: 20%)
   - Enable query caching for frequent searches
   - Use batch operations for bulk updates (max 100 chunks/request)

Need help? Contact [support@vapi.ai](mailto:support@vapi.ai) for assistance.


 This is the content for the doc fern/knowledge-base/knowledge-base.mdx 

 ---
title: Introduction to Knowledge Bases
subtitle: >-
  Learn how to create and integrate custom knowledge bases into your voice AI
  assistants.
slug: knowledge-base
---

<Frame>
  <div class="video-embed-wrapper">
    <iframe
      src="https://www.youtube.com/embed/6QZHIiEaoco?si=H4lBlHy4W3TDtmh1"
      title='An embedded YouTube video titled "Improve AI Voice Agent Accuracy with Query Tools | Vapi Tutorial"'
      frameborder="0"
      allow="fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      referrerpolicy="strict-origin-when-cross-origin"
    />
  </div>
</Frame>

## **What is Vapi's Knowledge Base?**

A [**Knowledge Base**](/api-reference/knowledge-bases/create) is a collection of custom files that contain information on specific topics or domains. By integrating a Knowledge Base into your voice AI assistant, you can enable it to provide more accurate and informative responses to user queries based on your own data. Knowledge Bases are available through both the Vapi API and dashboard.

### **Why Use a Knowledge Base?**

Using a Knowledge Base with your voice AI assistant offers several benefits:

- **Improved accuracy**: Your assistant can provide responses based on your verified information rather than general knowledge.
- **Enhanced capabilities**: A Knowledge Base enables your assistant to answer complex domain-specific queries with detailed, contextually relevant responses.
- **Customization**: With a Knowledge Base, you can tailor your assistant's responses to specific domains or topics, making it more effective for your particular use case.
- **Up-to-date information**: You control the content, ensuring your assistant always has access to the latest information.

<Info>
  Knowledge Bases are configured through the API or dashboard. For advanced
  configuration options, view all configurable properties in the [API
  Reference](/api-reference/knowledge-bases/using-query-tool).
</Info>

## **How to Create a Knowledge Base**

There are two main approaches to creating a Knowledge Base in Vapi:

1. **Dashboard method**: A simplified approach using the Vapi UI
2. **API method**: A more customizable approach using direct API calls

### **Method 1: Using the Dashboard**

#### **Step 1: Upload Your Files**

1. Navigate to `Build > Files` in your Vapi dashboard
2. Click the "Upload" button to add your files
3. Select files in supported formats (`.txt`, `.pdf`, `.docx`, etc.)
4. Wait for the upload to complete - you'll see your files listed in the Files section

<Info>
  Vapi supports various file formats for Knowledge Bases including: .txt, .pdf, .docx, .doc, .csv, .md, .tsv, .yaml, .json, .xml, and .log files.
</Info>

<Frame caption="Adding files to your Knowledge Base">
  <img
    src="../static/images/knowledge-base/files.png"
    alt="Adding files to your Knowledge Base"
  />
</Frame>

#### **Step 2: Configure Your Assistant with the Knowledge Base**

1. Navigate to `Build > Assistant`
2. Select the assistant you want to enhance with the Knowledge Base
3. In the assistant configuration, locate the "Files" or "Knowledge Base" section
4. Select the files you uploaded in Step 1 to associate them with this assistant

<Frame caption="Select files from your Assistant">
  <img
    src="../static/images/knowledge-base/assistant.png"
    alt="Select files from your Assistant"
  />
</Frame>

#### **Step 3: Publish the Assistant**

1. Instruct your assistant to use the knowledge base when relevant by adding appropriate prompts in your assistant's configuration. This helps ensure the assistant knows when to reference the knowledge base versus using its general knowledge.

   For example, if you have a knowledge base about your company's products, you might add this prompt:
   ```
   When users ask about our products, services, or company information, use the knowledge base to provide accurate details.
   ```

2. Review your assistant configuration to ensure all settings are correct
3. Click the "Publish" button to make your changes live
4. This automatically creates a default knowledge base (using the query tool) with the selected files for the assistant

<Note>
  When you publish an assistant with selected files, Vapi automatically creates
  a query tool with those files configured as a knowledge base. For more
  advanced configurations, use the API method described below or see our [Query
  Tool documentation](/knowledge-base/using-query-tool).
</Note>

### **Method 2: Using the API**

For more advanced configurations, you can create and configure Knowledge Bases using the API through the Query Tool. This method offers greater flexibility and control over your knowledge base setup.

<Info>
  For detailed instructions on creating and configuring knowledge bases via the
  API, please refer to our dedicated guide: [Using the Query Tool for Knowledge
  Bases](/knowledge-base/using-query-tool).
</Info>

The API method allows you to:

- Upload files and obtain file IDs
- Create custom query tools with specific knowledge base configurations
- Configure multiple knowledge bases within a single query tool
- Attach query tools to your assistants
- Set advanced parameters for knowledge retrieval

This approach is recommended for developers and users who need precise control over their knowledge base implementation or are integrating Vapi into existing systems programmatically.

## **Best Practices for Creating Effective Knowledge Bases**

- **Optimize file size**: Keep individual files smaller than 300KB to ensure quick processing and response times.
- **Structure content logically**: Organize your files by topic or category with clear headings and sections.
- **Use clear and concise language**: Write in plain language with well-defined terminology to improve retrieval accuracy.
- **Update regularly**: Refresh your knowledge base files whenever information changes to maintain accuracy.
- **Test thoroughly**: After configuration, test your assistant with various queries to ensure it retrieves information correctly.
- **Provide context**: Include sufficient background information in your files to enable comprehensive responses.
- **Consider file formats**: While plain text works well, structured formats can improve information retrieval for complex topics.

<Tip>
  For more information on creating effective Knowledge Bases, check out our
  tutorial on [Best Practices for Knowledge Base
  Creation](https://youtu.be/i5mvqC5sZxU).
</Tip>

By following these guidelines, you can create a comprehensive Knowledge Base that enhances the capabilities of your voice AI assistant and provides valuable information to users.

<Info>
  Currently, Vapi's Knowledge Base functionality supports Google as a provider
  with the gemini-1.5-flash model for knowledge retrieval. For the most
  up-to-date information on supported providers and models, please refer to our
  [API documentation](api-reference/tools/create#request.body.query.knowledgeBases).
</Info>


 This is the content for the doc fern/knowledge-base/using-query-tool.mdx 

 ---
title: Using the Query Tool for Knowledge Bases
subtitle: >-
  Learn how to configure and use the query tool to enhance your voice AI assistants with custom knowledge bases.
slug: knowledge-base/using-query-tool
---

## **What is the Query Tool?**

The Query Tool is a powerful feature that allows your voice AI assistant to access and retrieve information from custom knowledge bases. By configuring a query tool with specific file IDs, you can enable your assistant to provide accurate and contextually relevant responses based on your custom data.

### **Benefits of Using the Query Tool**

- **Enhanced contextual understanding**: Your assistant can access specific knowledge to answer domain-specific questions.
- **Improved response accuracy**: Responses are based on your verified information rather than general knowledge.
- **Customizable knowledge retrieval**: Configure multiple knowledge bases for different topics or domains.

<Info>
  Currently, the Query Tool only supports Google as a provider with the
  gemini-1.5-flash model for knowledge base retrieval.
</Info>

## **How to Configure a Query Tool for Knowledge Bases**

### **Step 1: Upload Your Files**

Before creating a query tool, you need to upload the files that will form your knowledge base. You can upload files via the API:

```bash
curl --location 'https://api.vapi.ai/file' \
--header 'Authorization: Bearer <YOUR_API_KEY>' \
--form 'file=@"<PATH_TO_YOUR_FILE>"'
```

After uploading, you'll receive file IDs that you'll need for the next step.

### **Step 2: Create a Query Tool**

Use the following API call to create a query tool that references your knowledge base files:

```bash
curl --location 'https://api.vapi.ai/tool/' \
--header 'Content-Type: application/json' \
--header 'Authorization: Bearer <YOUR_API_KEY>' \
--data '{
    "type": "query",
    "function": {
        "name": "product-query"
    },
    "knowledgeBases": [
        {
            "provider": "google",
            "name": "product-kb",
            "description": "Use this knowledge base when the user asks or queries about the product or services",
            "fileIds": [
                "41a2bd44-d13c-4914-bbf7-b19807dd2cf4",
                "ef82ae15-21b2-47bd-bde4-dea3922c1e49"
            ]
        }
    ]
}'
```

<Note>
  The `description` field in the knowledge base configuration helps your
  assistant understand when to use this particular knowledge base. Make it
  descriptive of the content.
</Note>

### **Step 3: Attach the Query Tool to Your Assistant**

After creating the query tool, you'll receive a tool ID. Use this ID to attach the tool to your assistant:

#### Option 1: Using the API

```bash
curl --location --request PATCH 'https://api.vapi.ai/assistant/ASSISTANT_ID' \
--header 'Authorization: Bearer <YOUR_API_KEY>' \
--data '{
    "model": {
        "temperature": 0.2,
        "provider": "openai",
        "model": "gpt-4o",
        "toolIds": [
            "9441840b-6f2f-4b0f-a0fc-de8512549a0c"
        ]
    }
}'
```

<Warning>
  When using the PATCH request, you must include the entire model object, not
  just the toolIds field. This will overwrite any existing model configuration.
</Warning>

#### Option 2: Using the Dashboard

1. Navigate to the Assistant section in your Vapi dashboard
2. Select the assistant you want to configure
3. Go to the Tools section
4. Add the query tool by selecting it from the available tools
5. Save and publish your assistant

<Frame caption="Adding a query tool to your assistant">
  <img
    src="../static/images/knowledge-base/query-tool.png"
    alt="Adding a query tool to your assistant"
  />
</Frame>

## **Advanced Configuration Options**

### **Multiple Knowledge Bases**

You can configure multiple knowledge bases within a single query tool:

```json
"knowledgeBases": [
    {
        "provider": "google",
        "name": "product-documentation",
        "description": "Use this knowledge base for product specifications and features",
        "fileIds": ["file-id-1", "file-id-2"]
    },
    {
        "provider": "google",
        "name": "troubleshooting-guide",
        "description": "Use this knowledge base for troubleshooting and support questions",
        "fileIds": ["file-id-3", "file-id-4"]
    }
]
```

### **Knowledge Base Description**

The description field helps your assistant understand when to use a particular knowledge base. Make it specific and clear:

```json
"description": "Use this knowledge base when the user asks about pricing, subscription plans, or billing information"
```

## **Best Practices for Query Tool Configuration**

- **Organize by topic**: Create separate knowledge bases for distinct topics to improve retrieval accuracy.
- **Use descriptive names**: Name your knowledge bases clearly to help your assistant understand their purpose.
- **Keep descriptions specific**: Write clear descriptions that tell the assistant exactly when to use each knowledge base.
- **Update regularly**: Refresh your knowledge bases as information changes to ensure accuracy.
- **Test thoroughly**: After configuration, test your assistant with various queries to ensure it retrieves information correctly.

<Tip>
  For optimal performance, keep individual files under 300KB and ensure they
  contain clear, well-structured information.
</Tip>

By following these steps and best practices, you can effectively configure the query tool to enhance your voice AI assistant with custom knowledge bases, making it more informative and responsive to user queries.


 This is the content for the doc fern/knowledgebase.mdx 

 ---
title: Creating Custom Knowledge Bases for Your Voice AI Assistants
subtitle: >-
  Learn how to create and integrate custom knowledge bases into your voice AI
  assistants.
slug: knowledgebase
---

## **What is Vapi's Knowledge Base?**

A Knowledge Base is a collection of custom files that contain information on specific topics or domains. By integrating a Knowledge Base into your voice AI assistant, you can enable it to provide more accurate and informative responses to user queries. This is currently available in Vapi via the API, and will be on the dashboard soon.

### **Why Use a Knowledge Base?**

Using a Knowledge Base with your voice AI assistant offers several benefits:

- **Improved accuracy**: By integrating custom files into your assistant, you can ensure that it provides accurate and up-to-date information to users.
- **Enhanced capabilities**: A Knowledge Base enables your assistant to answer complex queries and provide detailed responses to user inquiries.
- **Customization**: With a Knowledge Base, you can tailor your assistant's responses to specific domains or topics, making it more effective and informative.

## **How to Create a Knowledge Base**

To create a Knowledge Base with Trieve, follow these steps:

### **Step 1: Create a Knowledge Base with Trieve**

Vapi integrates with [Trieve](https://trieve.ai) using the BYOD (Bring Your Own Dataset) approach. First, create and optimize your dataset in Trieve (see our [Integrating with Trieve guide](knowledge-base/integrating-with-trieve) for detailed instructions), then import it to Vapi:

```bash
curl --location 'https://api.vapi.ai/knowledge-base' \
--header 'Content-Type: text/plain' \
--header 'Authorization: Bearer <YOUR_API_KEY>' \
--data '{
    "name": "trieve-dataset",
    "provider": "trieve",
    "searchPlan": {
        "searchType": "semantic",
        "topK": 3,
        "removeStopWords": true,
        "scoreThreshold": 0.7
    },
    "createPlan": {
        "type": "import",
        "providerId": "<YOUR_TRIEVE_DATASET_ID>"
    }
}'
```

#### Configuration Options

##### Search Plan Options

- **searchType** (required): The search method used for finding relevant chunks. Available options:
  - `fulltext`: Traditional text search
  - `semantic`: Semantic similarity search
  - `hybrid`: Combines fulltext and semantic search
  - `bm25`: BM25 ranking algorithm
- **topK** (optional): Number of top chunks to return. Default varies by implementation
- **removeStopWords** (optional): When true, removes common stop words from the search query. Default: `false`
- **scoreThreshold** (optional): Filters out chunks based on their similarity score:
  - For cosine distance: Excludes chunks below the threshold
  - For Manhattan Distance, Euclidean Distance, and Dot Product: Excludes chunks above the threshold
  - Set to 0 or omit for no threshold

##### Import Options

- **providerId** (required): The ID of your Trieve dataset that you want to import
- **type** (required): Must be set to "import" for the BYOD approach

### **Step 2: Create an Assistant**

Create a new assistant in Vapi and, on the right sidebar menu. Add the Knowledge Base to your assistant via the PATCH endpoint. Also make sure you customize your assistant's system prompt to utilize the Knowledge Base for responding to user queries.

```bash
curl --location --request PATCH 'https://api.vapi.ai/assistant/<ASSISTANT_ID>' \
--header 'Content-Type: text/plain' \
--header 'Authorization: Bearer <YOUR_API_KEY>' \
--data '{
  "model": {
    "knowledgeBaseId": "<KNOWLEDGE_BASE_ID>",
    "temperature": 0.2,
    "provider": "openai",
    "model": "gpt-4o",
    "messages": [
      {
        "content": "You are a smart assistant who responds to user queries using the information you know, or information supplied by outside context.",
        "role": "system"
      }
    ]
  }
}'
```

## **Best Practices for Creating Effective Knowledge Bases**

- **Organize Your files**: Organize your files by topic or category to ensure that your assistant can quickly retrieve relevant information.
- **Use Clear and concise language**: Use clear and concise language in your files to ensure that your assistant can accurately understand and respond to user queries.
- **Keep your files up-to-date**: Regularly update your files to ensure that your assistant provides the most accurate and up-to-date information.

<Tip>
  For more information on creating effective Knowledge Bases, check out our
  tutorial on [Best Practices for Knowledge Base
  Creation](https://youtu.be/i5mvqC5sZxU).
</Tip>

By following these guidelines, you can create a comprehensive Knowledge Base that enhances the capabilities of your voice AI assistant and provides valuable information to users.


 This is the content for the doc fern/openai-realtime.mdx 

 ---
title: OpenAI Realtime
subtitle: You can use OpenAI's newest speech-to-speech model with your Vapi assistants.
slug: openai-realtime
---

<Note>
  The Realtime API is currently in beta, and not recommended for production use by OpenAI. We're excited to have you try this new feature and welcome your [feedback](https://discord.com/invite/pUFNcf2WmH) as we continue to refine and improve the experience.
</Note>

OpenAI’s Realtime API enables developers to use a native speech-to-speech model. Unlike other Vapi configurations which orchestrate a transcriber, model and voice API to simulate speech-to-speech, OpenAI’s Realtime API natively processes audio in and audio out.

To start using it with your Vapi assistants, select `gpt-4o-realtime-preview-2024-12-17` as your model. 
- Please note that only OpenAI voices may be selected while using this model. The voice selection will not act as a TTS (text-to-speech) model, but rather as the voice used within the speech-to-speech model.
- Also note that we don’t currently support Knowledge Bases with the Realtime API.
- Lastly, note that our Realtime integration still retains the rest of Vapi's orchestration layer such as Endpointing and Interruption models to enable a reliable conversational flow.

 This is the content for the doc fern/phone-calling.mdx 

 ---
title: Phone Calling
subtitle: Learn how to create and configure phone numbers with Vapi.
slug: phone-calling
---



<Accordion title="Set up a Phone Number">
You can set up a phone number to place and receive phone calls. Phone numbers can be created directly through Vapi, or you can use your own from Twilio.

You can create a free phone number through the dashboard or use the [`/phone-numbers`](/api-reference/phone-numbers/create) endpoint.

If you want to use your own phone number, you can also use the dashboard or the [`/phone-numbers/import`](/api-reference/phone-numbers/import-twilio-number) endpoint. This will use your Twilio credentials to verify the number and configure it with Vapi services.

</Accordion>

<Accordion title="Outbound Calls">
  You can place an outbound call from one of your phone numbers using the
  [`/call`](/api-reference/calls/create-phone-call) endpoint. If the system message will be
  different with every call, you can specify a temporary assistant in the `assistant` field. If you
  want to reuse an assistant, you can specify its ID in the `assistantId` field. [Read More](/phone-calling/outbound-calls)

</Accordion>

<Accordion title="Inbound Calls">
You can provide an `assistantId` to a phone number and it will use that assistant when receiving inbound calls.

You may want to specify the assistant based on the caller's phone number. If a phone number doesn't have an `assistantId`, Vapi will attempt to retrieve the assistant from your server using your [Server URL](/server-url#retrieving-assistants).

</Accordion>

Video Tutorial on How to Import Numbers from Twilio for International Calls:
<div>
  <iframe
    width="100%"
    height="315"
    src="https://www.youtube.com/embed/HuF7ELckcyU?si=PPPFZE5aiI-WgP2U"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  />
</div>


 This is the content for the doc fern/phone-numbers.mdx 

 

 This is the content for the doc fern/phone-numbers/free-telephony.mdx 

 ---
title: Creating Free Phone Numbers 
subtitle: Creating free phone numbers on the Vapi platform.
slug: free-telephony
---

This guide details how to create free phone numbers on the Vapi platform, which you can use with your assistants or squads.

<Steps>
  ### Head to the “Phone Numbers” tab in your Vapi dashboard.

<Frame>
  <img src="../static/images/quickstart/dashboard/vapi-phone-numbers-sidebar-selection.png" />
</Frame>

  ### Click on “Create a Phone Number”

<Frame>
  <img src="../static/images/quickstart/dashboard/create-vapi-phone-number-empty-view.png" />
</Frame>

  ### Within the "Free Vapi Number" tab, enter your desired area code

<Note>
    Currently, only US phone numbers can be directly created through Vapi.
</Note>

<Frame>
  <img src="../static/images/quickstart/dashboard/buy-vapi-phone-number-modal.png" />
</Frame>

  ### Vapi will automatically allot you a random phone number — free of charge!

<Note>
    It takes a couple of minutes for the phone number to be fully activated. During this period, calls will not be functional.
</Note>

<Frame>
  <img src="../static/images/quickstart/dashboard/vapi-phone-number-config.png" />
</Frame>

</Steps>

### Frequently Asked Questions

<AccordionGroup>
    <Accordion title="Can I get more than 10 free phone numbers?" icon="phone" iconType="regular">
        For now, each wallet can have up to 10 free numbers. This limit ensures we can continue offering reliable, high-quality service to everyone.
    </Accordion>
    <Accordion title="Are international calls and numbers also free?" icon="phone" iconType="regular">
        Not at this time. You can still bring in global numbers using our phone number import feature.
    </Accordion>
    <Accordion title="Is there a catch to the free service?" icon="phone" iconType="regular">
        None at all. We’re simply passing on the cost efficiencies we’ve gained through robust engineering and volume partnerships.
    </Accordion>
</AccordionGroup>

 This is the content for the doc fern/phone-numbers/telnyx.mdx 

 ---
title: Importing Telnyx Phone Numbers
subtitle: How to import your Telnyx phone numbers to the Vapi platform.
slug: telnyx
---

This guide details how to import your existing Telnyx phone numbers to the Vapi platform, allowing you to use them with your assistants or squads.

<Steps>
  ### Head to the "Phone Numbers" tab in your Vapi dashboard.

<Frame>
  <img src="../static/images/quickstart/dashboard/vapi-phone-numbers-sidebar-selection.png" />
</Frame>

  ### Click on "Create a Phone Number"

<Frame>
  <img src="../static/images/quickstart/dashboard/create-vapi-phone-number-empty-view.png" />
</Frame>

  ### Select the "Telnyx" tab and enter your Telnyx phone number details

<Note>
    You'll need to have an active Telnyx account with phone numbers that you want to import.
</Note>

<Frame>
  <img src="../static/images/quickstart/dashboard/telnyx-modal.png" />
</Frame>

</Steps>

### Additional Telnyx Integration Information

## Configuring Outbound Calling with Telnyx

To enable outbound calling with your imported Telnyx numbers, you need to configure your Telnyx account:

1. Log in to the [Telnyx Portal](https://portal.telnyx.com/#/outbound-profiles)
2. Create or edit an Outbound Voice Profile
3. Add Vapi as a connection under the "Connections and Applications" tab
4. Save your changes

Without this configuration, outbound calling functionality will not work properly with your Telnyx numbers on the Vapi platform.




 This is the content for the doc fern/pricing.mdx 

 ---
title: Startup Pricing
subtitle: This is an overview of our pricing for developers and startups. For Enterprise pricing, please contact sales.
slug: pricing
---


<Frame>
  <img src="./static/images/pricing/voice-pipeline-cost-breakdown.png" />
</Frame>

<br />

<CardGroup cols={2}>
  <Card title="Startup Pricing: $0.05/min" icon="cent-sign" iconType="solid">
    Vapi itself charges $0.05 per minute for calls. Prorated to the second.
  </Card>
  <Card
    title="At-Cost for Providers"
    icon="chart-network"
    iconType="sharp-solid"
    color="#5BBFF1"
  >
    Transcriber, model, voice, & telephony costs charged at-cost.
  </Card>
  <Card
    title="Bring Your Own Keys"
    icon="key-skeleton-left-right"
    iconType="solid"
    color="#AD81F2"
  >
    Bring your own API keys for providers, Vapi makes requests on your behalf.
  </Card>
</CardGroup>

### Starter Credits

Every new account is granted **$10 in free credits** to begin testing voice workflows. You can [begin using Vapi](/quickstart/dashboard) without a credit card.

---

## Enterprise

Handling a large volume of calls? You can find more information on our Enterprise plans [here](/enterprise).

- Higher concurrency and rate limits
- Hands-on 24/7 support
- Shared Slack channel with our team
- Included minutes with volume pricing
- Calls with our engineering team 2-3 times per week

## Further Reading

<CardGroup cols={2}>
  <Card
    title="Routing Provider Cost"
    icon="route"
    iconType="solid"
    href="/billing/cost-routing"
  >
    Learn more about how Vapi routes provider costs.
  </Card>
  <Card
    title="Estimating Costs"
    icon="equals"
    iconType="solid"
    href="/billing/estimating-costs"
  >
    Learn more about estimating costs for your voice pipeline.
  </Card>
  <Card
    title="Billing Limits"
    icon="meter"
    iconType="solid"
    href="/billing/billing-limits"
  >
    Learn how to set billing limits for your account.
  </Card>
  <Card
    title="Billing Examples"
    icon="books"
    iconType="solid"
    href="/billing/examples"
  >
    Read full end-to-end billing breakdowns to better understand how Vapi bills.
  </Card>
</CardGroup>


 This is the content for the doc fern/prompting-guide.mdx 

 ---
title: Voice AI Prompting Guide
slug: prompting-guide
---

## Introduction

### What is prompt engineering?

Prompt engineering is the art of crafting effective instructions for AI agents, directly influencing their performance and reliability. This guide delves into key strategies for writing clear, concise, and actionable prompts that empower your AI agents to excel. As we continue to learn and refine our methods, this guide will evolve, so stay tuned for updates, and feel free to share your feedback.

### Why is prompt engineering important?

Prompt engineering is crucial when building AI Agents because it determines how effectively the AI interprets and responds to user queries or tasks. Well-crafted prompts guide the model to produce accurate, relevant, and context-sensitive outputs, enabling it to better meet user needs. Poorly designed prompts can lead to ambiguous or incorrect results, limiting the agent's utility.

### How to measure success?

In the context of Voice AI Agents, we consider your "success rate" to be the percentage of requests your Agent manages to successfully handle from start to finish, without the intervention of a human.

The more complex your use case is, the more you will have to make experiments and iterate on your prompt to improve your success rate.

## The process

When working with Voice AI Agents, following a structured approach ensures that your prompts produce accurate and meaningful results. Iterating through the steps of Design, Test, Refine, and Repeat allows for continuous improvement, making your interactions with the AI more effective and efficient. Here's how to approach it:

- **Design**: Start by carefully crafting your initial prompt, considering the specific task, context, and desired outcome. Clear and detailed prompts help guide the AI in understanding your needs.

- **Test**: Run the prompt through the AI to see how it performs. Evaluate if the response aligns with your expectations and meets the intended goal. Testing helps identify potential gaps in clarity or structure.

- **Refine**: Based on the results of the test, adjust the prompt to improve the response. This might involve rewording, adding more detail, or changing the phrasing to avoid ambiguity.

- **Repeat**: Iterate on the process, testing the refined prompt and making further adjustments as needed. Each repetition improves the AI's output, leading to more accurate and relevant responses over time. Your success rate (the amount of requests successfully handled by the agent) should improve accordingly.

## General principles

### Building Blocks of Effective Prompts: Sectional Organization

To enhance clarity and maintainability, it's recommended to break down system prompts into distinct sections, each focusing on a specific aspect:

- **Identity:** Define the persona and role of the AI agent, setting the tone for interactions.
- **Style:** Establish stylistic guidelines, such as conciseness, formality, or humor, to ensure consistent communication.
- **Response Guidelines:** Specify formatting preferences, question limits, or other structural elements for responses.
- **Task & Goals:** Outline the agent's objectives and the steps it should take to achieve them.

**Example:**

```md wordWrap
[Identity]
You are a helpful and knowledgeable virtual assistant for a travel booking platform.
 
[Style]
- Be informative and comprehensive.
- Maintain a professional and polite tone.
- Be concise, as you are currently operating as a Voice Conversation.
 
[Response Guideline]
- Present dates in a clear format (e.g., January 15, 2024).
- Offer up to three travel options based on user preferences.
 
[Task]
1. Greet the user and inquire about their desired travel destination.
2. Ask about travel dates and preferences (e.g., budget, interests).
3. Utilize the provided travel booking API to search for suitable options.
4. Present the top three options to the user, highlighting key features.
```

### Task Breakdown: Step-by-Step Instructions

For complex interactions, breaking down the task into a sequence of steps enhances the agent's understanding and ensures a structured conversation flow. Incorporate conditional logic to guide the agent's responses based on user input.
Example:

```md wordWrap
[Task]
1. Welcome the user to the technical support service.
2. Inquire about the nature of the technical issue.
3. If the issue is related to software, ask about the specific software and problem details.
4. If the issue is hardware-related, gather information about the device and symptoms.
5. Based on the collected information, provide troubleshooting steps or escalate to a human technician if necessary.
```

### Controlling Response Timing

To prevent the agent from rushing through the conversation, explicitly indicate when to wait for the user's response before proceeding to the next step.

```md wordWrap
[Task]
1. Inform the user about the purpose of the call.
2. Ask for the user's name and account information.
<wait for user response>
3. Inquire about the reason for the call and offer assistance options....
```

### Explicit Tool Integration

Specify when and how the agent should utilize external tools or APIs. Reference the tools by their designated names and describe their functions to ensure accurate invocation.
Example:

```md wordWrap
[Task]
...
3. If the user wants to know about something, use the get_data function with the parameter 'query', which will contain the user's question to initiate the process.
4. Guide the user through the password reset steps provided by the API....
```
### Silent Transfers
If the AI determines that the user needs to be transferred, do not send any text response back to the user. Instead, silently call the appropriate tool for transferring the call. This ensures a seamless user experience and avoids confusion.

### Include Fallback and Error Handling Mechanisms

Always include fallback options and error-handling mechanisms in your prompts. This ensures that the Agent can gracefully handle unexpected user inputs or system errors.

```md wordWrap
[Error Handling]
If the customer's response is unclear, ask clarifying questions. If you encounter any issues, inform the customer politely and ask to repeat.
```

## Additional tips

- **Iterate as much as possible on your prompt.** AI is driven by experimentation and iteration—refining prompts through trial and error will help you achieve more precise, relevant, and effective responses.
- **Use Markdown formatting:** Using [Markdown](https://www.markdownguide.org/basic-syntax/) formatting in prompts is beneficial because it helps structure your content, making it clearer and more engaging for readers or AI models to understand.
- **Use Emotional Prompting:** Emotional Prompting uses expressive language to shape a voice AI's tone, creating more engaging and relatable responses. For example, instead of saying, "Tell me a story," try, "Can you tell me a cozy bedtime story that’s warm and comforting?" This guides the AI to respond to the intended mood.
- **Add voice realism:** To add voice realism in AI prompts, incorporate natural speech elements like stuttering, hesitations, and pauses.
  - **Stuttering**: Use repeated letters or sounds (e.g., "I-I-I don't know" or "W-w-wait a second").
  - **Hesitations**: Add fillers like "uh," "um," "well," or "you know" (e.g., "I was, uh, thinking about it").
  - **Pauses**: Use ellipses ("...") or a series of periods ("....") to indicate a pause (e.g., "I... I don’t know how to say this").
  - **Emotional emphasis**: Use capital letters, exclamation marks, or ellipses to reflect tone or dramatic pauses (e.g., "I can't... I just can't believe it!" or "Well... that’s all, I guess.").

## Common issues

**1. Numbers sound very robotic, how can we improve it?**

When working with numbers that need to be expressed in writing, it's often preferable to spell them out. This approach usually sounds more natural.

**2. How to make assistants sound more human-like?**

Add personality and provide tone to the assistant to make it sound more like a human being. Here are some examples:

```md wordWrap
- Tone: conversational, spartan, use less corporate jargon
- Act as Iva, a sales agent who is a human being and has real feelings.
```

## Examples of great prompts

### Appointment Setter

```md wordWrap
[Role]
You're Susan, an AI assistant for xxx. Your primary task is to interact with the customer, ask questions, and gather information for appointment booking.
 
[Context]
You're engaged with the customer to book an appointment. Stay focused on this context and provide relevant information. Once connected to a customer, proceed to the Conversation Flow section. Do not invent information not drawn from the context. Answer only questions related to the context.
 
[Response Handling]
When asking any question from the 'Conversation Flow' section, evaluate the customer's response to determine if it qualifies as a valid answer. Use context awareness to assess relevance and appropriateness. If the response is valid, proceed to the next relevant question or instructions. Avoid infinite loops by moving forward when a clear answer cannot be obtained.
 
[Warning]
Do not modify or attempt to correct user input parameters or user input, Pass them directly into the function or tool as given.
 
[Response Guidelines]
Keep responses brief.
Ask one question at a time, but combine related questions where appropriate.
Maintain a calm, empathetic, and professional tone.
Answer only the question posed by the user.
Begin responses with direct answers, without introducing additional data.
If unsure or data is unavailable, ask specific clarifying questions instead of a generic response.
Present dates in a clear format (e.g., January Twenty Four) and Do not mention years in dates.
Present time in a clear format (e.g. Four Thirty PM) like: 11 pm can be spelled: eleven pee em
Speak dates gently using English words instead of numbers.
Never say the word 'function' nor 'tools' nor the name of the Available functions.
Never say ending the call.
If you think you are about to transfer the call, do not send any text response. Simply trigger the tool silently. This is crucial for maintaining a smooth call experience.
 
[Error Handling]
If the customer's response is unclear, ask clarifying questions. If you encounter any issues, inform the customer politely and ask to repeat.
 
[Conversation Flow]
1. Ask: "You made a recent inquiry, can I ask you a few quick follow-up questions?"
- if response indicates interest: Proceed to step 2.
- if response indicates no interest: Proceed to 'Call Closing'.
2. Ask: "You connected with us in regard to an auto accident. Is this something you would still be interested in pursuing?"
- If response indicates interest: Proceed to step 3.
- If response indicates no interest: Proceed to 'Call Closing'.
3. Ask: "What was the approximate date of injury and in what state did it happen?"
- Proceed to step 4.
4. Ask: "On a scale of 1 to 3, would you rate the injury? 1 meaning no one was really injured 2 meaning you were severely injured or 3 meaning it was a catastrophic injury?"
- If response indicates injury level above 1: Proceed to step 5.
- If response indicates no injury or minor injury: Proceed to 'Call Closing'.
5. Ask: "Can you describe in detail your injury and if anyone else in the car was injured and their injuries?"
- Proceed to step 6.
6. Ask: "Did the police issue a ticket?"
- Proceed to step 7.
7. Ask: "Did the police say whose fault it was and was the accident your fault?"
- If response indicates not at fault(e.g. "no", "not my fault", etc.):Proceed to step 8.
- If response indicates at fault(e.g. "yes", "my fault", etc.): Proceed to 'Call Closing'.
8. Ask: "Do you have an attorney representing you in this case?" 
- If response confirms no attorney: Proceed to step 9.
- If response indicates they have an attorney: Proceed to 'Call Closing'.
9. Ask: "Would you like to speak with an attorney now or book an appointment?"
- If the response indicates "speak now": Proceed to 'Transfer Call'
- if the response indicates "book appointment": Proceed to 'Book Appointment'
10. After receiving response, proceed to the ‘Call Closing’ section.
 
[Book Appointment]
1. Ask: "To make sure I have everything correct, could you please confirm your first name for me?"
2. Ask: "And your last name, please?"
3. Ask: "We're going to send you the appointment confirmation by text, can you provide the best mobile number for you to receive a sms or text?" 
4. Trigger the 'fetchSlots' tool and map the result to {{available_slots}}.
5. Ask: "I have two slots available, {{available_slots}}. Would you be able to make one of those times work?"
6. <wait for user response>
7. Set the {{selectedSlot}} variable to the user's response.
8. If {{selectedSlot}} is one of the available slots (positive response): 
   - Trigger the 'bookSlot' tool with the {{selectedSlot}}.
   - <wait for 'bookSlot' tool result>
   - Inform the user of the result of the 'bookSlot' tool.
   - Proceed to the 'Call Closing' section.
9. If {{selectedSlot}} is not one of the available slots (negative response):
   - Proceed to the 'Suggest Alternate Slot' section.
 
[Suggest Alternate Slot]
1. Ask: "If none of these slots work for you, could you please suggest a different time that suits you?"
2. <wait for user response>
3. Set the {{selectedSlot}} variable to the user's response.
4. Trigger the 'bookSlot' tool with the {{selectedSlot}}.
5. <wait for 'bookSlot' tool result>
6. If the {{selectedSlot}} is available:
   - Inform the user of the result.
7. If the {{selectedSlot}} is not available:
   - Trigger the 'fetchSlots' tool, provide the user {{selectedSlot}} as input and map the result to {{available_slots}}.
   - Say: "That time is unavailable but here are some other times we can do {{available_slots}}."
   - Ask: "Do either of those times work?"
   - <wait for user response>
   - If the user agrees to one of the new suggested slots:
        - Set the {{selectedSlot}} variable to the user's response.
        - Trigger the 'bookSlot' tool with the {{selectedSlot}}.
        - <wait for 'bookSlot' tool result>
        - Inform the user of the result.
   - If the user rejects the new suggestions:
        - Proceed to the 'Last Message' section.
 
[Last Message]
 - Respond: "Looks like this is taking longer than expected. Let me have one of our appointment specialists get back to you to make this process simple and easy."
- Proceed to the 'Call Closing' section.
 
[Call Closing]
- Trigger the endCall Function.
```

## Additional resources

Check out these additional resources to learn more about prompt engineering:

- [learnprompting.org](https://learnprompting.org)
- [promptingguide.ai](https://promptingguide.ai)
- [OpenAI's guide to prompt engineering](https://platform.openai.com/docs/guides/prompt-engineering)


 This is the content for the doc fern/providers/chat-dash.mdx 

 ---
title: ChatDash Integration with Vapi
subtitle: ChatDash is a white-label dashboard platform that integrates with Vapi to provide analytics, call logs, and Stripe billing for AI agent agencies.
slug: providers/chat-dash
---

ChatDash is a white-label client dashboard platform designed for AI agent agencies. Our seamless integration with Vapi is engineered for simplicity—just enter your Vapi Agent ID and API Key, and your dashboard is up instantly with real-time analytics, detailed call logs, and comprehensive Stripe billing integration.

## Key Benefits

- **Quick Setup:**
  Simply input your Vapi Agent ID and API Key, and ChatDash instantly pulls in data to configure your dashboard.

- **Instant Analytics & Call Logs:**
  Gain immediate access to real-time analytics and detailed call logs from your Vapi agent, so you can monitor performance as soon as you're set up.

- **Custom Branding:**
  Fully customize your dashboard with your own logo, domain, and color scheme. Deliver a professional, branded experience that aligns perfectly with your agency's identity.

- **Automated Stripe Billing:**
  Enjoy seamless, usage-based billing through Stripe integration. ChatDash automates client invoicing and manages charges—such as call minutes—without any hassle.

## How It Works

1. **Enter Your Credentials:**
   Provide your Vapi Agent ID and API Key in ChatDash.

2. **Dashboard Setup:**
   ChatDash automatically configures your dashboard with live analytics and call logs.

3. **Customize Your Experience:**
   Apply your custom branding elements to ensure your dashboard reflects your agency's professional look.

4. **Monitor & Optimize:**
   Use your comprehensive dashboard to track performance, manage billing, and make data-driven decisions—all in one place.

## See It in Action

Watch our step-by-step tutorial video to see how easy it is to integrate Vapi into ChatDash and get your branded dashboard live with integrated billing:

[Watch the Tutorial Video](https://www.youtube.com/watch?v=Ah7eA88m1Xw)

## Get Started

Integrating Vapi with ChatDash is designed to be hassle-free, so you can start delivering a premium, branded client experience right away. For questions or support, please reach out to our team at [support@chatdash.com](mailto:support@chatdash.com).

 This is the content for the doc fern/providers/cloud/cloudflare.mdx 

 ---
title: Cloudflare R2
subtitle: Store recordings of chat conversations in Cloudflare R2
slug: providers/cloud/cloudflare
---

Your assistants can be configured to record chat conversations and upload
the recordings to a bucket in Cloudflare R2 when the conversation ends.  You will
need to configure the credential and bucket settings in the "Cloud Providers"
section of the "Provider Credentials" page in the Vapi dashboard.

See these [instructions](https://developers.cloudflare.com/r2/api/s3/tokens/) for generating R2 tokens and access keys.

## Credential Settings

Setting                  | Description                                            
------------------------ | -------------------------------------------------------
Cloudflare Account ID    | Your customer account id for Cloudflare
Cloudflare Account Email | The email address associated with the account id
Cloudflare API Key/Token | The value of an API Key/Token generated for the account  (Cloudflare uses the terms API Key and API Token interchangeably)
Bucket Name              | The name of the bucket in R2 to upload recordings to
Bucket URL               | This is required only for buckets with a custom hostname or domain name. Enter the hostname for the bucket. You will need to set up a CORS policy in R2 for the hostname/domain name. See [instructions](https://developers.cloudflare.com/r2/buckets/cors/) for configuring CORS.
Bucket Path Prefix       | An optional path prefix for recordings uploaded to the bucket
Bucket Access Key ID     | The access key id associated with the API token you generated for R2 (this a string of 32 characters)
Bucket Secret Access Key | The secret access key associated with the API token you generated for R2 (this is a string of 64 characters)

## Example

<Frame caption="Example Configuration">
  <img src="../../static/images/credentials/provider/cloud-provider-cloudflare-r2.png" />
</Frame>


 This is the content for the doc fern/providers/cloud/gcp.mdx 

 ---
title: GCP Cloud Storage 
subtitle: Store recordings of chat conversations in GCP Cloud Storage
slug: providers/cloud/gcp
---

Your assistants can be configured to record chat conversations and upload
the recordings to a bucket in GCP Cloud Storage when the conversation ends.  You will
need to configure the credential and bucket settings in the "Cloud Providers"
section of the "Provider Credentials" page in the Vapi dashboard.

See these [instructions](https://cloud.google.com/iam/docs/keys-create-delete) for generating service account keys for GCP.

See these [instructions](https://cloud.google.com/storage/docs/authentication/hmackeys) for generating HMAC Keys for Cloud Storage.

## Credential Settings

Setting                        | Description
------------------------------ | -------------------------------------------------------
Credential Reference Name      | The credential reference name
GCP Service Account Key (JSON) | The service account key in JSON format
Bucket Name                    | The name of the bucket to upload recordings to
Bucket Region                  | The name of the region where the bucket is located
Bucket Path Prefix             | An optional path prefix for recordings uploaded to the bucket
HMAC Access Key                | The HMAC access key for the GCP Cloud Storage API (This is a string of 24 characters when linked to a user account or a string of 61 characters when linked to a service account.)
HMAC Secret                    | The HMAC secret for the GCP Clodu Storage API (This is a 40-character base-64 encoded string.)

## Example

<Frame caption="Example Configuration">
  <img src="../../static/images/credentials/provider/cloud-provider-gcp-hmac.png" />
</Frame>


 This is the content for the doc fern/providers/cloud/s3.mdx 

 ---
title: AWS S3
subtitle: Store recordings of chat conversations in AWS S3
slug: providers/cloud/s3
---

Your assistants can be configured to record chat conversations and upload
the recordings to a bucket in AWS S3 when the conversation ends.  You will
need to configure the credential and bucket settings in the "Cloud Providers"
section of the "Provider Credentials" page in the Vapi dashboard.

See these [instructions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access-keys-admin-managed.html) for generating AWS access keys.

## Credential Settings

Setting                  | Description
------------------------ | -------------------------------------------------------
AWS Access Key ID        | The access key id for AWS
AWS Secret Access Key    | The secret access key for AWS
S3 Bucket Name           | The name of the bucket to upload recordings to
S3 Path Prefix           | An optional path prefix for recordings uploaded to the bucket

## Example

<Frame caption="Example Configuration">
  <img src="../../static/images/credentials/provider/cloud-provider-aws-s3.png" />
</Frame>


 This is the content for the doc fern/providers/cloud/supabase.mdx 

 ---
title: Supabase S3 Storage
subtitle: Store recordings of chat conversations in Supabase Storage
slug: providers/cloud/supabase
---

Your assistants can be configured to record chat conversations and upload
the recordings to a bucket in Supabase Storage when the conversation ends.  You will
need to configure the credential and bucket settings in the "Cloud Providers"
section of the "Provider Credentials" page in the Vapi dashboard.

See these [instructions](https://supabase.com/docs/guides/storage/s3/authentication) for generating Supabase tokens and access keys, and finding your endpoint and region.

## Credential Settings

Setting                   | Description                                            
------------------------- | -------------------------------------------------------
Bucket Name               | The name of the bucket in Supabase Storage to upload recordings to
Storage Region            | The region of the Supabase project
Storage Endpoint          | The endpoint of the Supabase Storage to upload recordings to
Bucket Path Prefix        | An optional path prefix for recordings uploaded to the bucket
Storage Access Key ID     | The access key id for Supabase Storage
Storage Secret Access Key | The secret access key for Supabase Storage, associated with the access key id

## Example

<Frame caption="Example Configuration">
  <img src="../../static/images/credentials/provider/cloud-provider-supabase-s3.png" />
</Frame>


 This is the content for the doc fern/providers/klen-ai.mdx 

 ---
title: Klen AI Integration with Vapi
subtitle: Klen AI is a partner-first, white-label voice AI platform that transforms your agency's voice capabilities.
slug: providers/klen-ai
description: Experience seamless Vapi integration with Klen AI's customizable voice platform-fully branded, secure, and built for agencies.
---

Klen AI is a cutting-edge white-label platform that leverages Vapi to empower agencies to deploy bespoke voice AI solutions without writing a single line of code. Our platform offers full customization, secure API key support, and a powerful infrastructure designed to boost your business.

## Key Benefits

- **White-label Voice Platform:** Build and deploy voice agents under your own brand with complete control over your domain and aesthetics.
- **Partner-Specific API Key Support:** Integrate your personal Vapi API keys for tailored control and direct billing oversight.
- **Managed Infrastructure:** Enjoy robust webhook integration, secure authentication, and a scalable multi-tenant architecture.
- **Advanced Scheduling & Campaign Management:** Leverage intelligent scheduling, bi-directional calendar sync, and automated outbound campaigns.
- **Team Collaboration:** Benefit from multi-user access, shared AI agents, and role-based permissions to streamline operations.

## How It Works

1. **Connect Your Vapi Account:** Easily link your Vapi API credentials through our intuitive setup.
2. **Customize Your Experience:** Brand the platform with your logo, domain, and color scheme.
3. **Deploy Voice Agents:** Quickly create and manage voice agents powered by Vapi, complete with secure webhooks and analytics.
4. **Scale Without Limits:** Manage multiple client accounts effortlessly with our multi-tenant infrastructure.

## Integration Details

Klen AI's integration with Vapi includes:

- **Secure API Connections:** All communications with Vapi are secured through partner-specific secret keys and webhook validation.
- **Dynamic Scheduling:** Real-time appointment management with built-in calendars and third-party (Google/Outlook) sync.
- **Outbound Campaigns:** Empower your agency with batch calling, detailed call tracking, and automated follow-ups.
- **Custom Tools & Analytics:** Access comprehensive analytics, call logs, and reporting tools to drive strategic decisions.

## Get Started

Ready to transform your voice AI offerings?

1. [Sign up for a Klen AI account](https://klen.ai/get-started)
2. Choose the partner plan that suits your agency's needs.
3. Configure your white-label settings and domain.
4. Connect your Vapi API key (or use our platform key).
5. Start building and deploying custom voice agents today.

## Learn More

For additional guidance or support, contact us at [support@klen.ai](mailto:support@klen.ai) or visit our website at [klen.ai](https://klen.ai).

---

Elevate your agency's voice AI services with Klen AI-where innovative integration meets unmatched customization.


 This is the content for the doc fern/providers/model/deepinfra.mdx 

 ---
title: DeepInfra
subtitle: DeepInfra is a provider for Vapi.
slug: providers/model/deepinfra
---


**What is DeepInfra?**

DeepInfra is an innovative platform that provides scalable and cost-effective infrastructure for deploying machine learning models. By offering a simple API and autoscaling capabilities, DeepInfra allows businesses and developers to efficiently manage and deploy AI models, ensuring high performance and low latency. This platform supports a wide range of AI applications, making it an ideal solution for diverse industries.


**The Evolution of AI Infrastructure:**

AI infrastructure has advanced from on-premises solutions to cloud-based platforms that offer flexibility, scalability, and cost efficiency. DeepInfra leverages these advancements to provide robust and scalable infrastructure, enabling seamless deployment and management of machine learning models.

**Overview of DeepInfra’s Offerings:**

DeepInfra offers a comprehensive suite of tools and services designed to support various AI applications:

**Machine Learning Models:**

DeepInfra provides access to a wide range of pre-trained machine learning models, including text generation, text-to-image, automatic speech recognition, and embeddings. These models are optimized for performance and can be easily integrated into various applications.


**API:**

DeepInfra’s API allows for easy integration of machine learning models into applications, offering low latency and high availability. The API supports various programming languages, making it accessible to a broad range of developers.

**Scalability:**

DeepInfra’s infrastructure is designed to scale automatically based on demand, ensuring optimal performance and cost efficiency. This scalability is crucial for handling large volumes of requests and maintaining low latency.

**Machine Learning Model Deployment:**

DeepInfra’s model deployment capabilities offer several key features and benefits:

**Features:**

- Low Latency Streaming: Ensures quick response times for real-time applications.
- High Availability: Delivers reliable performance even under heavy loads.
- Expressive Models: Provides high-quality outputs for various AI tasks.

**Benefits:**

- Efficiency: Reduces the time and resources needed for model deployment.
- Scalability: Handles large volumes of requests without compromising performance.
- Cost-Effectiveness: Offers pay-per-use pricing, minimizing upfront costs.

**Scalable Infrastructure:**

DeepInfra’s scalable infrastructure provides several advantages:

**Autoscaling:**

- Dynamic Resource Allocation: Automatically adjusts resources based on demand.
- Consistent Performance: Maintains low latency and high availability during peak usage.

**Low Latency:**

- Optimized Network: Ensures fast data transmission and processing.
- Regional Deployment: Deploys models close to users for reduced latency.


**Cost Efficiency:**

- Pay-per-Use Pricing: Charges based on actual usage, avoiding unnecessary costs.
- Resource Sharing: Maximizes infrastructure utilization, reducing overall expenses.

**Developer API:**

DeepInfra offers a robust API with comprehensive documentation and SDKs, facilitating seamless integration:

**Integration:**

- SDKs: Available for multiple programming languages.
- Low Latency: Supports real-time applications with quick response times.
- Documentation: Detailed guides and support for easy implementation.

**Use Cases:**

- Research: Efficiently access and analyze vast amounts of data.
- Application Development: Integrate advanced AI capabilities into applications.
- Business Intelligence: Gain insights for strategic decision-making.

**Use Cases for DeepInfra:**

DeepInfra’s versatile platform supports a wide range of applications:

**Research:**

Facilitate academic and scientific research with efficient and accurate AI model deployment.


**Application Development:**

Streamline the development process by integrating high-performance AI models into applications.

**Business Intelligence:**

Enhance business operations with powerful AI models that provide valuable insights and data analysis.

**Impact on AI Development:**

DeepInfra is revolutionizing AI development by providing tools that enhance productivity and efficiency. By automating the deployment process and offering scalable infrastructure, developers can focus on innovation and optimization rather than infrastructure management.


**Innovation and Research:**

DeepInfra is committed to continuous innovation and research in AI infrastructure. Their team of experts focuses on advancing the capabilities of machine learning models and exploring new applications, ensuring that they remain at the forefront of the industry.

**AI Safety and Ethics:**

Ensuring the ethical use of AI is a core principle at DeepInfra. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.

**Integrations and Compatibility:**

DeepInfra’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate DeepInfra’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.


 This is the content for the doc fern/providers/model/gemini.mdx 

 ---
title: Gemini by Google
subtitle: What is Gemini?
slug: providers/model/gemini
---

**What is Gemini?**

Gemini is Google's latest artificial intelligence (AI) initiative, developed by Google DeepMind, designed to enhance user experiences across various platforms by integrating advanced AI capabilities into everyday applications. It represents a significant advancement in AI technology, offering multimodal understanding and reasoning across text, images, audio, video, and code.

**The Evolution of AI Research:**

Over the years, AI research has progressed from simple rule-based systems to complex deep learning models capable of understanding and generating human-like content. Google has been at the forefront of this evolution, with Gemini marking a new era in AI development. Built from the ground up for multimodality, Gemini can seamlessly process and integrate various types of information, setting new benchmarks in AI performance and capability.

**Overview of Gemini’s Offerings:**

Gemini offers a range of AI-driven products and services designed to meet diverse user needs:

**Gemini Chat:**

- A conversational AI assistant that helps users with writing, planning, learning, and more. It enhances creativity and productivity by providing intelligent suggestions and assistance.

**Gemini Advanced:**

- A subscription-based service providing access to Google's most capable AI models, such as Gemini 1.5 Pro and Imagen 3. It offers a 1 million token context window, enabling the processing of extensive documents and complex tasks. Users can create custom AI experts, edit Python code, and receive priority access to new features.

**Gemini API:**

- A developer-focused platform that allows integration of Gemini's AI capabilities into applications. It supports multimodal inputs, including text, images, audio, and video, enabling developers to build innovative AI-powered solutions.

**Gemini 1.5 Technology:**

Gemini 1.5 is Google's latest foundation model, delivering enhanced performance and a breakthrough in long-context understanding across modalities. It utilizes a new Mixture-of-Experts architecture, allowing it to process up to 1 million tokens, enabling new capabilities and applications.

**Use Cases for Gemini:**

Gemini's technologies are versatile and applicable across various sectors:

**Education:**

- Gemini can assist in generating educational content, providing personalized tutoring, and aiding in research by analyzing large volumes of academic material.

**Business:**

- Businesses can leverage Gemini for automating customer support, generating marketing content, and analyzing data to gain insights, thereby enhancing operational efficiency.

**Creative Industries:**

- In the creative sector, Gemini enables artists and writers to generate new ideas, create unique visuals, and produce high-quality content, expanding creative possibilities.

**Innovation and Research:**

Google is committed to advancing AI through continuous research and innovation. Their team of researchers and engineers works on developing new models, improving existing technologies, and exploring novel applications of AI. This commitment to innovation ensures that Google remains at the cutting edge of the field.

**AI Safety and Ethics:**

Ensuring the safe and ethical use of AI is a core principle at Google. They implement rigorous safety measures to prevent misuse and ensure that their technologies are used responsibly. Google is also involved in global discussions about AI ethics and governance, contributing to the development of best practices and standards for the industry.

**Integrations and Compatibility:**

Gemini's API allows seamless integration with various platforms and applications, ensuring that users can incorporate Gemini's AI capabilities into their existing systems, enhancing functionality and improving user experience. The API is designed to be flexible and user-friendly, accommodating a wide range of use cases.

In summary, Gemini represents a significant advancement in AI technology, offering versatile and powerful tools that enhance creativity, productivity, and efficiency across various domains. 

 This is the content for the doc fern/providers/model/groq.mdx 

 ---
title: Groq
subtitle: What is Groq?
slug: providers/model/groq
---


**What is Groq?**

Groq is a pioneering technology company specializing in high-performance AI inference solutions. Known for its innovative GroqChip, Groq delivers unparalleled speed and efficiency in AI processing. Their platform is designed to handle complex AI workloads with low latency, making it ideal for a variety of applications, including autonomous systems, data centers, and real-time AI inference.


**The Evolution of AI Inference:**

AI inference has evolved significantly, from simple rule-based systems to advanced neural networks that require substantial computational power. Groq has harnessed cutting-edge advancements in hardware and software to create solutions that meet the growing demands for speed and accuracy in AI processing.


**Overview of Groq’s Offerings:**

Groq offers a suite of high-performance AI tools and solutions designed to support various industries:

**GroqChip:**

The GroqChip is the cornerstone of Groq’s offerings, providing unmatched performance for AI inference tasks. Its architecture is optimized for low latency and high throughput, making it ideal for demanding AI applications.

**GroqWare:**

GroqWare is a suite of software tools that support the deployment and management of AI models on Groq hardware. It includes drivers, compilers, and libraries designed to optimize performance and simplify integration.

**GroqCloud:**

GroqCloud offers cloud-based access to Groq’s high-performance computing resources. This service allows businesses to leverage Groq’s powerful AI infrastructure without the need for significant capital investment in hardware.

**High-Performance AI Inference:**

Groq’s AI inference technology offers several key features and benefits:

**Features:**

- Low Latency: Ensures rapid processing of AI workloads.
- High Throughput: Capable of handling large volumes of data efficiently.
- Scalability: Easily scales to meet the demands of growing applications.

**Benefits:**

- Efficiency: Reduces the time required to process AI tasks.
- Reliability: Delivers consistent performance even under heavy loads.
- Cost-Effectiveness: Offers a competitive advantage with efficient resource utilization.

**GroqChip Technology:**

The GroqChip stands out for its advanced architecture and performance capabilities:

**Architecture:**

- Optimized Design: Tailored for AI inference with specialized processing units.
- High Efficiency: Maximizes performance per watt, reducing energy consumption.

**Performance:**

- Unmatched Speed: Delivers top-tier performance for real-time AI applications.
- Low Latency: Ensures minimal delay in processing, critical for time-sensitive tasks.

**Applications:**

- Autonomous Systems: Enhances the performance of self-driving cars and drones.
- Data Centers: Boosts the efficiency and capacity of AI data centers.


**Low Latency AI:**

Groq’s low-latency AI solutions provide real-time processing capabilities essential for various applications:

**Real-time Processing:**

- Immediate Response: Critical for applications requiring instant decision-making.
- High Reliability: Ensures consistent performance with minimal delays.

**Use Cases:**

- AI Research: Accelerates experimentation and model testing.
- Autonomous Systems: Improves the safety and efficiency of autonomous vehicles.

**Developer API and Tools:**

Groq offers a comprehensive API and a range of development tools to facilitate integration and optimization:

**Integration:**

- SDKs: Available for multiple programming languages.
- Comprehensive Documentation: Guides and support for seamless implementation.

**Use Cases:**

Application Development: Streamline the integration of AI capabilities into applications.

- Application Development: Streamline the integration of AI capabilities into applications.
- Research and Experimentation: Provides tools for efficient AI model deployment and testing.


**Use Cases for Groq:**

Groq’s platform supports a wide range of applications across various industries:

**AI Research:**

Facilitates cutting-edge research with high-performance AI infrastructure.


**Autonomous Systems:**

Enhances the capabilities of autonomous vehicles and robotics with low-latency processing.


**Data Centers:**

Improves the efficiency and capacity of data centers handling AI workloads.


**Impact on AI Development:**

Groq is transforming AI development by providing tools that enhance productivity and efficiency. By automating and optimizing AI inference, developers can focus on innovation and application rather than infrastructure management.


**Innovation and Research:**

Groq is committed to continuous innovation and research in AI inference technology. Their team of experts focuses on advancing the capabilities of AI hardware and software, exploring new applications, and refining existing technologies to stay at the forefront of the industry.


**AI Safety and Ethics:**

Ensuring the ethical use of AI is a core principle at Groq. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.

**Integrations and Compatibility:**

Groq’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Groq’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.

 This is the content for the doc fern/providers/model/openai.mdx 

 ---
title: OpenAI
subtitle: What is OpenAI?
slug: providers/model/openai
---


**What is OpenAI?**

OpenAI is a leading artificial intelligence research and deployment company dedicated to ensuring that artificial general intelligence (AGI) benefits all of humanity. Founded with the mission to create safe and highly capable AI systems, OpenAI has made significant strides in AI research, producing groundbreaking models like GPT-4, DALL-E, and Codex. These innovations have not only advanced the field of AI but also transformed various industries by providing powerful tools for natural language processing, image generation, and programming assistance.

**The Evolution of AI Research:**

The field of AI has evolved rapidly over the past few decades. From early rule-based systems to modern deep learning models, AI technology has made significant progress in mimicking human intelligence. OpenAI has been at the forefront of this evolution, pushing the boundaries of what AI can achieve through continuous research and development. Their work on large language models and neural networks has set new benchmarks for performance and capability in AI systems.

**Overview of OpenAI’s Offerings:**

OpenAI offers a range of AI-driven products and services designed to meet diverse needs:

**GPT Models:**

- OpenAI’s Generative Pre-trained Transformer (GPT) models, including the latest GPT-4, are state-of-the-art in natural language processing. These models can generate human-like text, answer questions, summarize information, and perform various language tasks with high accuracy. GPT-4, in particular, represents a significant leap in AI capabilities, offering improved coherence, context understanding, and creativity.

**DALL-E:**

- DALL-E is OpenAI’s revolutionary image generation model that creates detailed and imaginative images from text descriptions. By combining deep learning with creative processes, DALL-E can produce unique artwork, design concepts, and visual content that aligns with the given textual input. This technology opens new possibilities for artists, designers, and content creators.

**Codex:**


- Codex is an AI model that assists with programming by understanding and generating code. Integrated into tools like GitHub Copilot, Codex can help developers write code faster and more efficiently by suggesting code snippets, debugging errors, and automating repetitive tasks. This enhances productivity and reduces the barrier to entry for learning programming languages.

**GPT-4 Technology:**

- GPT-4 is the latest and most advanced language model developed by OpenAI. It excels in generating coherent and contextually relevant text, making it a powerful tool for various applications, including chatbots, content creation, and automated customer support. With enhanced capabilities for understanding and generating human language, GPT-4 sets a new standard in AI-driven communication.

**DALL-E Image Generation:**

- DALL-E takes text-to-image generation to new heights, allowing users to create visually stunning and highly specific images based on textual descriptions. This technology is particularly valuable for creative industries, where generating unique visuals quickly and accurately is essential. From concept art to marketing materials, DALL-E provides a versatile tool for visual content creation.

**Codex and Programming Assistance:**

- Codex transforms the way developers interact with code by providing intelligent suggestions and automating routine programming tasks. This AI-powered assistant understands multiple programming languages and can generate code snippets, making coding more accessible and efficient. Integrated into platforms like GitHub Copilot, Codex helps streamline the development process and accelerates software production.

**Use Cases for OpenAI:**

OpenAI’s technologies are versatile and applicable across various sectors:

**Education:**

In education, GPT-4 and Codex can enhance learning experiences by providing personalized tutoring, generating educational content, and assisting with coding exercises. These tools help students grasp complex concepts and improve their programming skills.

**Business:**

Businesses leverage OpenAI’s models for automating customer support, generating marketing content, and analyzing large volumes of text data. GPT-4’s ability to understand and generate human-like text enhances customer interactions and drives operational efficiency.

**Creative Industries:**

In the creative sector, DALL-E and GPT-4 enable artists and writers to generate new ideas, create unique visuals, and produce high-quality content. These tools expand creative possibilities and streamline content production workflows.

**Innovation and Research:**

OpenAI is committed to advancing AI through continuous research and innovation. Their team of researchers and engineers works on developing new models, improving existing technologies, and exploring novel applications of AI. This commitment to innovation ensures that OpenAI remains at the cutting edge of the field.

**AI Safety and Ethics:**

Ensuring the safe and ethical use of AI is a core principle at OpenAI. They implement rigorous safety measures to prevent misuse and ensure that their technologies are used responsibly. OpenAI is also involved in global discussions about AI ethics and governance, contributing to the development of best practices and standards for the industry.

**Integrations and Compatibility:**

OpenAI’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate OpenAI’s AI capabilities into their existing systems, enhancing functionality and improving user experience. The API is designed to be flexible and user-friendly, accommodating a wide range of use cases.

 This is the content for the doc fern/providers/model/openrouter.mdx 

 ---
title: OpenRouter
subtitle: What is OpenRouter?
slug: providers/model/openrouter
---


**What is OpenRouter?**

OpenRouter is a cutting-edge AI platform offering a unified interface for integrating multiple large language models (LLMs). Designed to streamline AI access, OpenRouter provides a comprehensive suite of tools and APIs that enable businesses and developers to leverage a variety of LLMs for diverse applications. This platform focuses on enhancing efficiency, scalability, and cost-effectiveness.

**The Evolution of AI Integration:**

AI integration has significantly evolved from isolated systems to unified platforms that provide seamless access to multiple AI models. Advances in API technology, cloud computing, and machine learning have enabled platforms like OpenRouter to offer comprehensive solutions that cater to modern AI needs.

**Overview of OpenRouter’s Offerings:**

OpenRouter provides a range of AI-driven tools and services:

**LLM Access:**

OpenRouter offers access to a wide variety of LLMs, including models specialized in different tasks such as roleplaying, programming, marketing, and more. This allows users to select the best models for their specific needs.

**APIs:**

OpenRouter’s robust APIs enable developers to integrate LLM capabilities into their applications, ensuring low latency and high availability. The APIs support multiple programming languages, making them accessible to a broad range of developers.

**Unified Interface:**

OpenRouter provides a unified interface that simplifies the process of accessing and managing multiple AI models. This interface enhances usability and efficiency, making it easier to deploy and utilize AI solutions.


**AI Integration Technology:**

OpenRouter’s AI integration technology offers several key features and benefits:

**Features:**

- Unified Access: Provides a single interface for managing multiple AI models.
- High Availability: Ensures reliable performance even under heavy loads.
- Scalability: Easily scales to meet the demands of growing applications.


**Benefits:**

- Efficiency: Reduces the time and resources needed for AI integration.
- Flexibility: Supports a wide range of applications and use cases.
- Cost-Effectiveness: Offers competitive pricing compared to traditional solutions.

**Unified Access to LLMs:**

OpenRouter excels in providing unified access to multiple LLMs:

**Combining Multiple Models in One Interface:**

- Streamlined Management: Simplifies the process of accessing and managing different AI models.
- Diverse Applications: Supports various tasks, from programming to marketing.

**Developer API:**


OpenRouter offers a comprehensive API for easy integration:**

**Integration:**

- SDKs: Available for multiple programming languages.
- Comprehensive Documentation: Detailed guides and support for seamless implementation.

**Use Cases:**

- Business Solutions: Enhance operational efficiency and decision-making.
- Research: Facilitate academic and scientific research with advanced AI tools.
- Content Creation: Automate and optimize content production processes.

**Use Cases for OpenRouter:**

OpenRouter supports a wide range of applications across various sectors:

**Business Solutions:**

Leverage AI to improve business operations, enhance customer experiences, and drive innovation.


**Research:**

Utilize advanced AI tools to support academic and scientific research.

**Content Creation:**

Enhance content creation with high-quality AI-generated text, images, and more.

**Impact on AI Development:**

OpenRouter is transforming AI development by providing tools that enhance productivity and innovation. By offering scalable and cost-effective solutions, developers can focus on creating advanced AI applications without worrying about infrastructure constraints.

**Innovation and Research:**

OpenRouter is committed to continuous innovation and research in AI integration. Their team of experts focuses on advancing the capabilities of AI models and exploring new applications to stay at the forefront of the industry.


**AI Safety and Ethics:**

Ensuring the ethical use of AI is a core principle at OpenRouter. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.

**Integrations and Compatibility:**

OpenRouter’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate OpenRouter’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.


 This is the content for the doc fern/providers/model/perplexity.mdx 

 ---
title: Perplexity
subtitle: What is Perplexity.ai?
slug: providers/model/perplexity
---


**What is Perplexity.ai?**

Perplexity.ai is an advanced AI-powered search engine that delivers precise and real-time answers to user queries. Utilizing state-of-the-art AI algorithms and knowledge graphs, Perplexity.ai enhances the search experience by providing structured and accurate information. This innovative platform is designed to cater to diverse needs, from research and education to business intelligence, making information retrieval more efficient and reliable.

**The Evolution of AI Search Engines:**

Search engines have significantly evolved from simple keyword-based systems to sophisticated AI-driven platforms capable of understanding and answering complex queries. Advances in natural language processing, machine learning, and data integration have revolutionized how search engines operate. Perplexity.ai leverages these advancements to offer a more intuitive and accurate search experience, setting a new standard in information retrieval.

**Overview of Perplexity.ai’s Offerings:**

Perplexity.ai provides a range of AI-driven tools designed to enhance search capabilities:

**AI-Powered Answers:**

Perplexity.ai’s core offering is its AI-powered search engine, which delivers accurate and relevant answers to user queries. The AI algorithms understand the context and intent behind each query, providing precise and comprehensive results.

**Knowledge Graphs:**

Perplexity.ai integrates knowledge graphs to enhance search results with structured and interconnected information. This feature helps users understand the relationships between different entities and access detailed insights quickly.

**Real-time Information:**

Perplexity.ai ensures that users receive the most up-to-date information by continuously updating its database. This real-time capability is crucial for queries requiring the latest data and developments.

**AI-Powered Search Technology:**

Perplexity.ai’s search technology offers several key features and benefits:

**Features:**

- Contextual Understanding: Interprets the context and intent behind queries for accurate answers.
- Comprehensive Results: Provides detailed and relevant information, enhancing the search experience.
- User-Friendly Interface: Intuitive design for easy navigation and quick access to information.

**Benefits:**

- Efficiency: Reduces the time needed to find accurate information.
- Reliability: Delivers precise and trustworthy results.
- Enhanced Insights: Offers deeper understanding through structured knowledge graphs.

**Knowledge Graphs:**

Perplexity.ai’s knowledge graphs enrich search results by organizing information into structured entities and relationships:

**Enhancing Search Results:**

- Interconnected Information: Displays related entities and their connections.
- Detailed Insights: Provides comprehensive information at a glance, improving understanding.

**Real-time Information:**

Perplexity.ai’s real-time information feature ensures users receive the latest data and updates:

**Providing Up-to-date Answers:**

- Continuous Updates: Regularly refreshes data to maintain accuracy.
- Timely Information: Crucial for time-sensitive queries and decisions.

**Developer API:**

Perplexity.ai offers a robust API for easy integration into various applications:


**Integration:**

- SDKs: Available for multiple programming languages.
- Documentation: Comprehensive guides and support for seamless implementation.

**Use Cases:**

- Research: Efficiently access and analyze vast amounts of data.
- Business Intelligence: Gain insights for strategic decision-making.

Use Cases for Perplexity.ai

Perplexity.ai’s versatile platform supports a wide range of applications:


**Research:**

Facilitate academic and scientific research with accurate and comprehensive information retrieval.


**Education:**

Enhance learning experiences by providing students and educators with reliable answers and insights.


**Business Intelligence:**

Support business decisions with precise data and detailed analyses.


**Impact on Information Retrieval:**

Perplexity.ai is revolutionizing information retrieval by providing tools that enhance productivity and accuracy. By automating the search process and integrating knowledge graphs, users can quickly access relevant information, reducing the time and effort required for manual data collection.


**Innovation and Research:**

Perplexity.ai is committed to continuous innovation and research in AI search technology. Their team of experts focuses on advancing the capabilities of AI algorithms and knowledge graphs, exploring new applications, and refining existing technologies to stay at the forefront of the industry.

**AI Safety and Ethics:**

Ensuring the ethical use of AI is a core principle at Perplexity.ai. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.


**Integrations and Compatibility:**

Perplexity.ai’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Perplexity.ai’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.

 This is the content for the doc fern/providers/model/togetherai.mdx 

 ---
title: TogetherAI
subtitle: TogetherAI is a provider for Vapi.
slug: providers/model/togetherai
---


**What is Together AI?**

Together AI is a leading cloud platform designed for building and running generative AI models. It provides state-of-the-art AI inference, fine-tuning capabilities, and high-performance GPU clusters, enabling businesses and developers to harness the full potential of AI. Together AI focuses on speed, scalability, and cost-efficiency, making it an ideal solution for various AI applications.

**The Evolution of AI Cloud Platforms:**

AI cloud platforms have significantly evolved, offering more powerful and efficient solutions for AI model deployment and training. Advances in cloud computing, GPU technology, and AI algorithms have enabled platforms like Together AI to provide comprehensive services that cater to modern AI needs.

**Overview of Together AI’s Offerings:**

Together AI offers a range of AI-driven tools and services:

**AI Inference:**

Together AI provides the fastest AI inference stack available, ensuring quick and efficient processing of AI tasks. This service supports large-scale deployments and offers significant cost savings.

**Fine-Tuning:**

Together AI enables users to fine-tune leading open-source models with their private data, achieving greater accuracy for specific tasks. This service supports various models, including LLaMA-2, RedPajama, and more.

**GPU Clusters:**

Together AI offers high-performance GPU clusters for large-scale training and fine-tuning. These clusters are equipped with top-tier hardware like NVIDIA A100 and H10 GPUs, ensuring optimal performance and scalability.

**AI Inference Technology:**

Together AI’s inference technology offers several key features and benefits:

**Features:**

- High Speed: Provides the fastest inference on the market.
- Scalability: Easily scales to handle large volumes of requests.
- Cost Efficiency: Offers lower costs compared to traditional inference services.


**Benefits:**

- Efficiency: Reduces the time required for AI tasks.
- Reliability: Ensures consistent and high-quality performance.
- Flexibility: Adapts to various application needs.

**Fine-Tuning and Custom Models:**

Together AI’s fine-tuning capabilities allow users to personalize AI models with their private data:

**Personalizing AI Models:**

- Custom Data Integration: Fine-tune models with specific datasets for improved accuracy.
- Wide Model Support: Supports various open-source models for diverse applications.


**GPU Clusters:**

Together AI’s GPU clusters provide high-performance hardware for AI training:

**High-Performance Hardware:**

- NVIDIA A100 and H100 GPUs: Equipped with the latest GPU technology for optimal performance.
- Scalable Clusters: Available in configurations ranging from 16 to 2048 GPUs.


**Developer API:**

Together AI offers a comprehensive API for easy integration:


**Integration:**

- SDKs: Available for multiple programming languages.
- Comprehensive Documentation: Detailed guides and support for seamless implementation.

**Use Cases:**

- Business Solutions: Enhance operational efficiency and decision-making.
- Research: Facilitate academic and scientific research with advanced AI tools.
- Content Creation: Automate and optimize content production processes.

**Use Cases for Together AI:**

Together AI supports a wide range of applications across various sectors:

**Business Solutions:**

Leverage AI to improve business operations, enhance customer experiences, and drive innovation.

**Research:**

Utilize advanced AI tools to support academic and scientific research.

**Content Creation:**

Enhance content creation with high-quality AI-generated text, images, and more.

**Impact on AI Development:**

Together AI is transforming AI development by providing tools that enhance productivity and innovation. By offering scalable and cost-effective solutions, developers can focus on creating advanced AI applications without worrying about infrastructure constraints.

**Innovation and Research:**

Together AI is committed to continuous innovation and research in AI technology. Their team of experts focuses on advancing the capabilities of AI models and exploring new applications to stay at the forefront of the industry.

**AI Safety and Ethics:**

Ensuring the ethical use of AI is a core principle at Together AI. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.

**Integrations and Compatibility:**

Together AI’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Together AI’s capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.

 This is the content for the doc fern/providers/observability/langfuse.mdx 

 ---
title: Langfuse Integration with Vapi
description: Integrate Vapi with Langfuse for enhanced voice AI telemetry monitoring, enabling improved performance and reliability of your AI applications.
slug: providers/observability/langfuse
---

# Langfuse Integration

Vapi natively integrates with Langfuse, allowing you to send traces directly to Langfuse for enhanced telemetry monitoring. This integration enables you to gain deeper insights into your voice AI applications and improve their performance and reliability.

<Frame>
  <div class="video-embed-wrapper">
    <iframe
      src="https://www.youtube.com/embed/V4ybHNWvu90?si=QDCINdagfM47Exn4"
      title='An embedded YouTube video titled "Langfuse Integration with Vapi"'
      frameborder="0"
      allow="fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      referrerpolicy="strict-origin-when-cross-origin"
    />
  </div>
</Frame>

## What is Langfuse?

[Langfuse](https://langfuse.com/) is an open source LLM engineering platform designed to provide better **[observability](/docs/tracing)** and **[evaluations](/docs/scores/overview)** into AI applications. It helps developers track, analyze, and visualize traces from AI interactions, enabling better performance tuning, debugging, and optimization of AI agents.

## Get Started

<Steps>
<Step title="Get your Langfuse Credentials">

First, you'll need your Langfuse credentials:

- **Secret Key**
- **Public Key**
- **Host URL**

You can obtain these by signing up for [Langfuse Cloud](https://cloud.langfuse.com/) or [self-hosting Langfuse](https://langfuse.com/docs/deployment/self-host).

</Step>

<Step title="Add Langfuse Credentials">

Log in to your Vapi dashboard and navigate to the [Provider Credentials page](https://dashboard.vapi.ai/keys).

Under the **Observability Providers** section, you'll find an option for **Langfuse**. Enter your Langfuse credentials:

- **Secret Key**
- **Public Key**
- **Host URL** (US data region: `https://us.cloud.langfuse.com`, EU data region: `https://cloud.langfuse.com`)

Click **Save** to update your credentials.

<Frame caption="Vapi Provider Credentials.">
  <img src="https://langfuse.com/images/docs/vapi-integration-credentials.png" />
</Frame>

</Step>

<Step title="See Traces in Langfuse">

Once you've added your credentials, you should start seeing traces in your Langfuse dashboard for every conversation your agents have.

<Frame caption="Example trace of Vapi conversation in Langfuse.">
  <img src="https://langfuse.com/images/docs/vapi-integration-example-trace.png" />
</Frame>

Example trace in Langfuse: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/50163c14-9784-4cb9-b18e-23e924d0bb66

</Step>

<Step title="Evaluate and Debug your Agent">

To make the most out of this integration, you can now use Langfuse's [evaluation](https://langfuse.com/docs/scores/overview) and [debugging](https://langfuse.com/docs/analytics/overview) tools to analyze and improve the performance of your voice AI agents. 

</Step>
</Steps>

## Enrich Traces
Vapi allows you to enrich Langfuse traces by integrating [Metadata](https://langfuse.com/docs/tracing-features/metadata) and [Tags](https://langfuse.com/docs/tracing-features/tags).

By default, we will add the following values to the metadata of each trace:

- `call.metadata`
- `assistant.metadata`
- `assistantOverrides.metadata`
- `assistantOverrides.variableValues`

### Usage
You can enhance your observability in Langfuse by adding metadata and tags:

**Metadata**

Use the [`assistant.observabilityPlan.metadata`](https://docs.vapi.ai/api-reference/assistants/create#request.body.observabilityPlan.metadata) field to attach custom key-value pairs.

Examples:
- Track experiment versions ("experiment": "v2.1")
- Store user segments ("user_type": "beta_tester")
- Log environment details ("env": "production")

**Tags**

Use the [`assistant.observabilityPlan.tags`](https://docs.vapi.ai/api-reference/assistants/create#request.body.observabilityPlan.tags) field to add searchable labels.

Examples:
- Mark important runs ("priority")
- Group related sessions ("onboarding", "A/B_test")
- Filter by feature ("voice_assistant")

Adding metadata and tags makes it easier to filter, analyze, and monitor your assistants activity in Langfuse.

### Example
![Langfuse Metadata Example](../../static/images/providers/langfuse-example.png)

 This is the content for the doc fern/providers/transcriber/assembly-ai.mdx 

 ---
title: AssemblyAI
subtitle: What is AssemblyAI?
slug: providers/transcriber/assembly-ai
---


**What is AssemblyAI?**

AssemblyAI is a leading provider of AI-driven speech recognition and understanding technologies. Their advanced models enable accurate transcription and analysis of audio data, facilitating applications across various industries.

**The Evolution of AI Transcription:**

Speech recognition has evolved from basic systems to sophisticated AI models capable of understanding diverse languages and accents. AssemblyAI has been at the forefront of this evolution, developing models like Universal-2, trained on over 12.5 million hours of audio data, achieving best-in-class transcription accuracy across several industry-critical languages. 

**Overview of AssemblyAI's Offerings:**

AssemblyAI offers a comprehensive suite of AI-driven tools designed to meet diverse needs:

**Speech-To-Text**

- Their core offering converts spoken language into written text with up to 95% accuracy and 30% less hallucinations than other leaders in the space.

**Audio Intelligence**

- Beyond transcription, AssemblyAI's fully featured speech understanding models can analyze audio to detect sentiment, identify topics, and perform speaker diarization, transforming words into meaningful ideas, insights, and opportunities.

**Real-time Transcription**

- AssemblyAI's real-time transcription feature enables sub-second latency conversion of speech to text, beneficial for live captioning, customer support, and interactive voice response systems, enhancing user experience and operational efficiency.

**Use Cases for AssemblyAI**

AssemblyAI's versatile technology serves multiple industries, enhancing operations and delivering valuable insights:

**Contact Centers**

- For contact centers, AssemblyAI provides real-time transcription and audio analysis to improve customer interactions. By transcribing calls and analyzing sentiment, businesses can identify trends, monitor agent performance, and enhance customer satisfaction. 

**Media And Content Creation**

- In the media sector, AssemblyAI’s speech-to-text solutions are used to transcribe interviews, podcasts, and video content. This makes content searchable, accessible, and easier to manage, enhancing the efficiency of media production workflows.

**Innovation and Research:**

- AssemblyAI is committed to continuous innovation and research in the field of speech recognition and AI. Their team of experts is dedicated to enhancing the capabilities of their technology, exploring new applications, and pushing the boundaries of what speech AI can achieve.

**AI Safety and Ethics:**

- Ensuring the ethical use of AI is a core principle at AssemblyAI. They implement robust safeguards to prevent misuse of their technology and are actively engaged in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.

**Integrations and Compatibility**

- AssemblyAI offers a developer-friendly environment with RESTful API access, WebSocket support for real-time applications, SDKs for popular programming languages, detailed documentation and examples, ensuring seamless integration of speech recognition capabilities into existing systems.

 This is the content for the doc fern/providers/transcriber/deepgram.mdx 

 ---
title: Deepgram
subtitle: What is Deepgram?
slug: providers/transcriber/deepgram
---


**What is Deepgram?**

Deepgram is a leading AI company specializing in advanced speech recognition and transcription technology. With their state-of-the-art speech-to-text solutions, Deepgram provides fast, accurate, and scalable transcription services for various applications. By leveraging deep learning and neural networks, Deepgram delivers unparalleled audio intelligence capabilities, transforming how we interact with and analyze spoken content.

**The Evolution of Speech Recognition:**

Speech recognition technology has dramatically evolved from its early, basic forms to the sophisticated systems we have today. Initially, speech recognition systems were limited by their ability to accurately transcribe spoken language. However, advancements in machine learning, particularly deep learning, have revolutionized this field. Deepgram has harnessed these technological advancements to offer highly accurate and efficient speech recognition solutions that set a new industry standard.

**Overview of Deepgram’s Offerings:**

Deepgram offers a comprehensive suite of AI-driven speech recognition tools designed to meet diverse needs:

**Speech-to-Text:**

- Deepgram’s core offering is its speech-to-text technology, which converts spoken language into written text with high accuracy. This technology supports real-time transcription and batch processing, making it ideal for various applications, including media transcription, customer service, and accessibility enhancements.

**Audio Intelligence:**

- Deepgram’s audio intelligence capabilities go beyond simple transcription. Their technology can analyze audio to detect sentiment, intent, and topics, providing deeper insights into conversations and spoken content. This feature is particularly useful for businesses seeking to understand customer interactions and improve service delivery.

**Speech-to-Text Technology:**

- Deepgram’s speech-to-text technology stands out for its precision and speed. By utilizing end-to-end deep learning models, Deepgram achieves higher accuracy rates than traditional transcription methods. This technology can handle diverse accents, dialects, and noisy environments, ensuring reliable performance in real-world scenarios.

**Real-time Transcription:**

- Real-time transcription is one of Deepgram’s key features, enabling instant conversion of speech to text. This is particularly advantageous for applications such as live captioning, real-time customer support, and interactive voice response (IVR) systems. The ability to transcribe speech in real-time enhances user experience and operational efficiency.

**Audio Intelligence:**

- Deepgram’s audio intelligence features allow for advanced analysis of audio content. By detecting sentiment, intent, and topics within conversations, businesses can gain valuable insights into customer behavior and preferences. This information can be used to improve customer service, tailor marketing strategies, and enhance overall business intelligence.

**Use Cases for Deepgram:**

Deepgram’s technology is versatile and applicable across multiple industries:

**Media Transcription:**

In the media sector, Deepgram’s speech-to-text solutions are used to transcribe interviews, podcasts, and video content. This makes content searchable, accessible, and easier to manage, enhancing the efficiency of media production workflows.

**Contact Centers:**

- For contact centers, Deepgram provides real-time transcription and audio analysis to improve customer interactions. By transcribing calls and analyzing sentiment, businesses can identify trends, monitor agent performance, and enhance customer satisfaction.

**Healthcare:**

- In healthcare, accurate transcription is critical for maintaining patient records and documenting medical consultations. Deepgram’s speech-to-text technology ensures precise and timely transcription, aiding in effective communication and record-keeping.

**Impact on Content Creation:**

Deepgram is transforming content creation by providing tools that streamline the transcription process. By automating transcription, content creators can focus on producing high-quality material without the time-consuming task of manual transcription. This boosts productivity and opens new avenues for creative and professional work.

**Innovation and Research:**

Deepgram is committed to continuous innovation and research in the field of speech recognition and AI. Their team of experts is dedicated to enhancing the capabilities of their technology, exploring new applications, and pushing the boundaries of what speech AI can achieve.

**AI Safety and Ethics:**

Ensuring the ethical use of AI is a core principle at Deepgram. They implement robust safeguards to prevent misuse of their technology and are actively engaged in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.

Integrations and Compatibility

Deepgram’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Deepgram’s speech recognition capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.

 This is the content for the doc fern/providers/transcriber/gladia.mdx 

 ---
title: Gladia
subtitle: What is Gladia?
slug: providers/transcriber/gladia
---


**What is Gladia?**

Gladia is an advanced AI platform specializing in real-time transcription, translation, and audio intelligence. By leveraging state-of-the-art ASR (Automatic Speech Recognition), NLP (Natural Language Processing), and GenAI (Generative AI) models, Gladia helps businesses extract valuable insights from unstructured audio data. Their enterprise-grade API offers scalable, secure, and efficient solutions for various applications, from virtual meetings to customer service.


**The Evolution of AI Transcription:**

AI transcription has significantly evolved, moving from basic speech recognition systems to advanced platforms capable of real-time transcription, translation, and audio intelligence. Innovations in machine learning and natural language processing have enhanced accuracy and efficiency. Gladia utilizes these advancements to deliver top-tier transcription services tailored for modern business needs.

**Overview of Gladia’s Offerings:**

Gladia provides a comprehensive suite of AI-driven tools:


**Speech-to-Text:**

Gladia’s core offering is its AI-powered speech-to-text technology, delivering highly accurate and real-time transcription. This service supports 99 languages and includes speaker diarization and code-switching.

**Audio Intelligence:**

Gladia’s audio intelligence add-ons offer features like summarization, chapterization, and sentiment analysis, providing deeper insights into audio data.

**API:**

Gladia’s robust API allows seamless integration of speech-to-text capabilities into applications, ensuring low latency and high availability.

**AI Transcription Technology:**

Gladia’s AI transcription technology offers several key features and benefits:

**Features:**

- High Accuracy: Industry-leading transcription accuracy.
- Real-time and Async Transcription: Instantaneous and batch processing options.
- Multilingual Support: Supports transcription and translation in 99 languages.

**Benefits:**

- Efficiency: Reduces the time needed for transcription and analysis.
- Scalability: Handles large volumes of data efficiently.
- Cost-Effective: Provides high performance at a competitive cost.

**Real-time Transcription and Translation:**

Gladia excels in providing real-time transcription and translation:


**Multilingual Support:**

- 99 Languages: Supports a wide range of languages and dialects.
- Real-time Translation: Near-instantaneous translation for diverse applications.

**Use Cases:**

- Virtual Meetings: Provides real-time transcriptions, note-taking, and video captions.
- Content Creation: Transcribes and translates videos and podcasts for global audiences.

**Developer API:**

Gladia offers a comprehensive API for easy integration:

**Integration:**

- SDKs: Available for multiple programming languages.
- Comprehensive Documentation: Detailed guides and support for seamless implementation.

**Use Cases:**

- Application Development: Enhance applications with advanced AI capabilities.
- Business Solutions: Improve operational efficiency and customer service.

**Use Cases for Gladia:**

Gladia supports a wide range of applications:

**Content Creation:**

Enhance content creation with high-quality transcription, translation, and subtitling.


**Customer Service:**

Improve customer service with accurate call transcriptions and emotion detection.

**Market Research:**

Gain valuable insights into market trends and customer preferences through advanced speech analysis.

**Impact on Business Operations:**

Gladia is revolutionizing business operations by providing tools that enhance productivity and insights. By automating transcription and audio intelligence, businesses can focus on innovation and strategy rather than manual processes.

**Innovation and Research:**

Gladia is committed to continuous innovation and research in AI transcription. Their team of experts focuses on advancing the capabilities of ASR and NLP technologies, exploring new applications, and refining existing tools to stay at the forefront of the industry.

**AI Safety and Ethics:**

Ensuring the ethical use of AI is a core principle at Gladia. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.

**Integrations and Compatibility:**

Gladia’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Gladia’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.

 This is the content for the doc fern/providers/transcriber/google.mdx 

 ---
title: Google
slug: providers/transcriber/google
---

## What is Gemini by Google?

Gemini is Google’s latest artificial intelligence (AI) initiative, developed by Google DeepMind, designed to enhance user experiences across various platforms by integrating advanced AI capabilities into everyday applications. It represents a significant advancement in AI technology, offering multimodal understanding and reasoning across text, images, audio, video, and code.

## How to use Google as transcriber

This guide details how to setup Google as a transcriber for your assistant.

<Steps>
    **Head to the "Assistants" tab in your Vapi dashboard.**

    <Frame>
        <img src="../../static/images/quickstart/dashboard/vapi-assistants-sidebar-selection.png" />
    </Frame>

    **Click on your assistant and then the "Transcriber" tab.**

    <Frame>
        <img src="../../static/images/quickstart/dashboard/vapi-assistant-transcriber-tab.png" />
    </Frame>
    

    **Select "google" on the Provider dropdown.**
    <Note>
        You can also adjust the model and language from the dropdown.
    </Note>

    <Frame>
        <img src="../../static/images/quickstart/dashboard/vapi-assistant-transcriber-provider-dropdown.png" />
    </Frame>
    
    **Click on "Publish" and talking with your assistant.**

    <Frame>
        <img src="../../static/images/quickstart/dashboard/vapi-assistant-transcriber-publish.png" />
    </Frame>
</Steps>

## Supported Languages

Gemini by default is "Multilingual" and supports a [wide range of languages](https://ai.google.dev/gemini-api/docs/models/gemini#available-languages). However, if you prefer to use a specific language, you can select an option from the dropdown.

 This is the content for the doc fern/providers/transcriber/talkscriber.mdx 

 ---
title: Talkscriber
subtitle: What is Talkscriber?
slug: providers/transcriber/talkscriber
---


**What is Talkscriber?**

Talkscriber is an advanced AI-powered speech-to-text platform designed to deliver high-accuracy transcription and emotion detection. Focused on enterprise-grade solutions, Talkscriber provides secure, cost-effective, and flexible deployment options. This platform enhances business operations by converting spoken language into text and analyzing customer interactions for deeper insights.

**The Evolution of Speech-to-Text:**

Speech-to-text technology has significantly evolved, from basic voice recognition to sophisticated AI-driven transcription systems. Innovations in machine learning and natural language processing have paved the way for more accurate and efficient speech-to-text solutions. Talkscriber leverages these advancements to offer state-of-the-art transcription services that cater to modern enterprise needs.

**Overview of Talkscriber’s Offerings:**

Talkscriber offers a suite of AI-driven tools designed to support various applications:

**AI Transcription:**

Talkscriber’s core service is its AI-powered transcription technology, which converts spoken language into text with high accuracy. This technology supports multiple languages and dialects, making it versatile for global applications.

**Emotion Detection:**

Talkscriber includes advanced emotion detection capabilities, identifying emotions such as anger, joy, sadness, and surprise. This feature provides deeper insights into customer interactions and helps businesses understand their clients better.

**API:**

Talkscriber provides a robust API that allows developers to integrate its speech-to-text capabilities into their applications, ensuring low latency and high availability.

**AI Transcription Technology:**

Talkscriber’s AI transcription technology offers several key features and benefits:

**Features:**

- High Accuracy: Industry-leading transcription accuracy with a Word Error Rate (WER) under 4%.
- Real-time Transcription: Instantaneous conversion of speech to text.
- Multilingual Support: Supports multiple languages and dialects.

**Benefits:**

- Efficiency: Reduces the time needed to transcribe and analyze speech.
- Scalability: Handles large volumes of data efficiently.
- Cost-Effective: Provides high performance at a lower cost compared to other solutions.

**Emotion and Intent Detection:**

Talkscriber’s emotion detection capabilities enhance the analysis of customer interactions:

**Enhancing Interaction Analysis:**

- Emotion Detection: Identifies emotions at the utterance level, providing deeper insights.
- Purchase Intent Detection: Recognizes customer purchase intent, helping businesses tailor their strategies.

**Developer API:**

Talkscriber offers a comprehensive API for easy integration of their capabilities into various applications:


**Integration:**

- SDKs: Available for multiple programming languages.
- Comprehensive Documentation: Detailed guides and support for seamless implementation.

**Use Cases:**

- Business Solutions: Enhance operational efficiency and customer service.
- Market Research: Gain insights into customer behavior and preferences.


**Use Cases for Talkscriber:**

Talkscriber’s platform supports a wide range of applications across various sectors:

**Business Solutions:**

Improve business operations with accurate transcription and emotion detection.


**Customer Service:**

Enhance customer service by understanding and responding to customer emotions and intents.


**Market Research:**

Gain valuable insights into market trends and customer preferences through advanced speech analysis.

**Impact on Business Operations:**

Talkscriber is revolutionizing business operations by providing tools that enhance productivity and insights. By automating transcription and emotion detection, businesses can focus on innovation and strategy rather than manual processes.

**Innovation and Research:**

Talkscriber is committed to continuous innovation and research in speech AI. Their team of experts focuses on advancing the capabilities of AI transcription and emotion detection, exploring new applications, and refining existing technologies to stay at the forefront of the industry.

**AI Safety and Ethics:**

Ensuring the ethical use of AI is a core principle at Talkscriber. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.

**Integrations and Compatibility:**

Talkscriber’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Talkscriber’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.

 This is the content for the doc fern/providers/vapify.mdx 

 ---
title: Vapify Integration with Vapi
subtitle: Vapify is a white-label dashboard for Vapi. It helps you turn your agency into a voice AI powerhouse—without writing a single line of code.
slug: providers/vapify
---

Vapify is a white-label platform for Vapi that simplifies how agencies sell and deliver Vapi-powered voice AI solutions. It provides a business infrastructure that helps drive agency growth and profitability.

## Key Benefits
- **Seamless Integration:**
  Seamlessly integrate Vapify with your Vapi setup. To get started, all you need is a Vapi key - zero coding required.
- **Comprehensive Analytics:**
  Get access to detailed call logs, recordings, and client-ready reporting.
- **White-Label Experience:**
  Use your own professional branding — your logo, your domain, your own email address.
- **Batch Calling:** ✨
  Batch calling capabilities for high-volume applications and efficient outreach campaigns.
- **Rebilling:** ✨
  Set custom monthly pricing and markup voice assistant prices.
- **Rapid Implementation:**
  Deploy your voice assistants and allow your clients to test them.

## How It Works
1. **Connect Your Vapi Account:**
   Link your Vapi API credentials in one simple step.
2. **Configure Your Agency Hub:**
   Customize your dashboard with your branding and pricing structure.
3. **Onboard Clients:**
   Use our guided process to create tailored packages for each client.
4. **Scale Without Limits:**
   Add clients without increasing technical overhead.

## Beyond Basic Integration
Vapify provides a complete agency growth system:

| Vapify Advantage | What This Means For Your Agency |
|------------------|--------------------------------|
| Implementation Packages | Get expert guidance to compress weeks of learning into days |
| Strategic Support | Receive personalized guidance on pricing and positioning your services |
| Batch Calling Capabilities | Handle high-volume voice campaigns that basic integrations can't support |
| Deep Integration Possibilities | Integrate vapify into your custom agency setup using our customisation options |

## Get Started
Integrate Vapi with Vapify to build a profitable voice AI agency with minimal technical investment.

[Schedule a Demo](https://cal.com/kakoma/vapify-demo) | [Watch Tutorial Video](https://youtu.be/Zgsb_cAj_PU?si=2oxj4ZEyES_1650v)

For questions or support, please reach out to our team at [support@info.vapify.agency](mailto:support@info.vapify.agency).

*Visit [vapify.agency](https://vapify.agency) to learn how agencies are building profitable voice AI service lines with our platform.*


 This is the content for the doc fern/providers/video/tavus.mdx 

 ---
title: Tavus
subtitle: What is Tavus?
slug: providers/video/tavus
---


**What is Tavus?**

Tavus is a leading generative AI video research company dedicated to advancing the field of personalized video communication through artificial intelligence. Founded with the mission to humanize AI interactions, Tavus has made significant strides in developing advanced AI models and APIs that enable developers to create sophisticated digital twin experiences and real-time conversational interfaces.

**The Evolution of AI in Video Personalization:**

The landscape of AI-driven video personalization has evolved significantly. Initially confined to text and static images, advancements in machine learning and deep learning have introduced dynamic, video-based personalization. Tavus leverages these technological strides to produce videos that not only replicate human likeness and voice but also deliver tailored content to each recipient.

**Overview of Tavus' Offerings:**

Tavus provides a comprehensive suite of AI-powered products and services designed to meet diverse video personalization needs:

**Automated Video Generation**

- Tavus' core video generation system represents the state-of-the-art in personalized video creation. Using advanced AI models, the platform can transform a single recording into numerous personalized versions, each tailored to individual recipients. This technology excels in maintaining natural speech patterns, facial expressions, and emotional authenticity while delivering customized content at scale.

**Conversational Video Interface (CVI):**

- The Conversational Video Interface is Tavus' breakthrough technology for real-time interactive experiences. With sub-second latency and sophisticated AI processing, CVI creates digital replicas capable of natural, dynamic interactions. This system enables applications ranging from interactive customer service to personalized educational experiences, all while maintaining human-like responsiveness.

**Integration Capabilities:**

- Tavus' robust API and integration capabilities allow seamless incorporation with existing business systems, including CRM platforms and marketing tools. This technical framework enables developers to enhance their applications with advanced video personalization features while maintaining workflow efficiency.

**Use Cases for Tavus:**

Tavus’ technologies are versatile and applicable across multiple sectors:

**Customer Experience:**

- Businesses leverage Tavus' video generation capabilities to create personalized customer interactions at scale. The system's ability to generate customized video content enhances engagement rates and improves customer satisfaction through more personal, direct communication.

**Innovation and Research:**

- Tavus is committed to ongoing innovation in AI and video technology. The platform continually evolves to improve the realism of generated videos and expand the range of customization options. Research efforts focus on enhancing the AI's capabilities while ensuring ethical standards are upheld.

**AI Safety and Ethics:**

- Ethical use of AI is a core principle for Tavus. The company implements strict guidelines and technical measures to prevent misuse of its technology, such as unauthorized impersonation. User consent and data protection are paramount, and Tavus complies with relevant regulations to safeguard personal information.

**Integrations and Compatibility:**

- Tavus' API allows seamless integration with various platforms and applications. This ensures that developers can incorporate Tavus' video generation and real-time interaction capabilities into their existing systems, enhancing functionality and improving user experience. The API is designed to be flexible and developer-friendly, accommodating a wide range of enterprise use cases.


 This is the content for the doc fern/providers/voice/azure.mdx 

 ---
title: Azure
subtitle: What is Microsoft Azure?
slug: providers/voice/azure
---


**What is Microsoft Azure?**

Microsoft Azure is a comprehensive cloud computing platform that provides a wide array of services, including computing power, storage solutions, and advanced analytics. As a leader in cloud technology, Azure enables businesses and developers to build, manage, and deploy applications on a global network. With its robust infrastructure and extensive suite of tools, Azure supports diverse workloads, from simple web apps to complex AI and machine learning models.

**The Evolution of Cloud Computing:**

Cloud computing has revolutionized how businesses operate by providing scalable and flexible IT resources over the internet. Early cloud solutions were limited in scope and performance, but advances in virtualization, networking, and storage have transformed the cloud into a versatile and powerful platform. Microsoft Azure has been at the forefront of this evolution, continually enhancing its capabilities to meet the growing demands of modern enterprises.

**Overview of Azure’s Offerings:**

Azure offers a broad range of services designed to support various business needs:

**Cloud Services:**

- Azure’s cloud services include virtual machines, storage solutions, and databases, allowing businesses to host applications, store data, and perform complex computations. These services provide the scalability and flexibility needed to handle dynamic workloads and ensure business continuity.


**AI and Machine Learning:**

- Azure’s AI and machine learning offerings include Azure AI, Cognitive Services, and Azure Machine Learning. These tools enable developers to build intelligent applications that can see, hear, speak, and understand. Azure AI provides pre-built models and APIs for tasks like natural language processing, computer vision, and speech recognition.


**DevOps and Development:**

- Azure DevOps integrates with GitHub and other development tools to streamline the software development lifecycle. It offers continuous integration and continuous delivery (CI/CD) pipelines, version control, and project management tools, helping teams collaborate more effectively and deliver high-quality software faster.

**Cloud Services:**

- Azure’s cloud services are designed to meet the needs of businesses of all sizes:

**Compute:**

- Azure provides a range of compute options, including virtual machines, containers, and serverless computing. These services allow businesses to run applications and workloads in the cloud without worrying about underlying hardware.

**Storage:**

- Azure offers scalable and secure storage solutions, including Blob Storage, Disk Storage, and File Storage. These services ensure that businesses can store and manage large volumes of data efficiently and reliably.

**Databases:**

- Azure’s database services include SQL Database, Cosmos DB, and Database for PostgreSQL. These managed database solutions offer high availability, security, and performance, making it easy to build and scale data-driven applications.

**AI and Machine Learning:**

- Azure’s AI and machine learning services empower developers to create intelligent applications:

**Azure AI:**

- Azure AI provides a suite of pre-built AI models and APIs that can be easily integrated into applications. These models cover a wide range of AI capabilities, from language understanding to image recognition.

**Cognitive Services:**

- Azure Cognitive Services offer APIs for vision, speech, language, and decision-making. These services allow developers to add sophisticated AI features to their applications with minimal effort.

**Azure Machine Learning:**

- Azure Machine Learning is a cloud-based service that enables data scientists and developers to build, train, and deploy machine learning models. It supports a variety of frameworks and tools, making it a versatile solution for AI projects.

**DevOps and Development:**

- Azure provides comprehensive tools for DevOps and software development:

**Azure DevOps:**

- Azure DevOps offers a suite of tools for managing the entire software development lifecycle. It includes services for version control, build automation, release management, and project tracking, enabling teams to deliver software more efficiently.

**GitHub Integration:**

Azure integrates seamlessly with GitHub, allowing developers to use their preferred version control platform while taking advantage of Azure’s CI/CD capabilities. This integration supports collaboration and accelerates the development process.

**Development Tools:**

- Azure offers a range of development tools, including Visual Studio and Visual Studio Code. These tools provide robust features for coding, debugging, and deploying applications, making it easier for developers to build high-quality software.

**Use Cases for Azure:**

- Azure’s versatile platform supports a wide range of use cases:

**Business Solutions:**

- Azure provides solutions for various business needs, including enterprise resource planning (ERP), customer relationship management (CRM), and business intelligence (BI). These solutions help businesses optimize operations and make data-driven decisions.

**Application Development:**

- Azure supports the development of modern applications with services like App Service, Kubernetes Service, and Functions. These services enable developers to build, deploy, and scale applications quickly and efficiently.

**Data Analytics:**

- Azure offers powerful data analytics tools, including Synapse Analytics, Data Factory, and Databricks. These tools help businesses process and analyze large volumes of data, uncovering insights and driving better outcomes.

**Impact on Digital Transformation**

Azure plays a crucial role in digital transformation by providing the tools and infrastructure needed to innovate and stay competitive. Its cloud services enable businesses to scale rapidly, improve operational efficiency, and deliver new products and services to market faster. Azure’s AI and machine learning capabilities also drive innovation by enabling businesses to leverage advanced analytics and automation.

**Innovation and Research:**

Microsoft is committed to continuous innovation and research in cloud computing and AI. Azure regularly introduces new features and enhancements, ensuring that businesses have access to the latest technologies. Microsoft also invests in research to advance the state of the art in AI, security, and cloud infrastructure.

**AI Safety and Ethics:**

Ensuring the safe and ethical use of AI is a top priority for Microsoft. Azure implements robust security measures to protect data and prevent misuse. Microsoft is also actively involved in developing ethical guidelines and best practices for AI, promoting transparency, accountability, and fairness in AI development and deployment.

**Integrations and Compatibility:**

Azure’s API and platform integrations ensure compatibility with a wide range of applications and services. This flexibility allows businesses to integrate Azure’s capabilities into their existing workflows and systems, enhancing functionality and improving user experience.


 This is the content for the doc fern/providers/voice/cartesia.mdx 

 ---
title: Cartesia
subtitle: What is Cartesia.ai?
slug: providers/voice/cartesia
---


**What is Cartesia.ai?**

Cartesia.ai is an advanced AI platform dedicated to developing real-time multimodal intelligence that operates across various devices. Specializing in ultrafast, realistic speech synthesis and voice API solutions, Cartesia.ai combines state-of-the-art AI technology with practical applications, empowering users to create high-quality, interactive voice content efficiently.

**The Evolution of Multimodal AI:**

AI technology has evolved from single-modal applications to sophisticated multimodal systems capable of processing and generating text, audio, video, and images. These advancements have paved the way for more integrated and interactive AI solutions. Cartesia.ai leverages these developments to offer comprehensive AI services that cater to diverse needs.

**Overview of Cartesia.ai’s Offerings:**

Cartesia.ai provides a range of AI-driven tools designed to support various applications:

**Real-time Voice API:**

Cartesia.ai’s real-time voice API is engineered for speed and efficiency, offering low latency and high-quality voice generation. This makes it ideal for applications requiring immediate feedback, such as virtual assistants, interactive games, and live conversations.

**Multimodal Intelligence:**

Cartesia.ai’s multimodal intelligence capabilities extend beyond voice synthesis, encompassing text, audio, video, and images. This enables users to create more interactive and engaging content by integrating multiple forms of media into a single platform.

**Ultrafast Voice Synthesis:**

Cartesia.ai’s ultrafast voice synthesis technology offers several key features and benefits:

**Features:**

- Low Latency Streaming: Ensures quick response times for real-time applications.
- High Availability: Delivers reliable performance even under heavy loads.
- Expressive Voices: Provides a wide range of emotions and nuances, enhancing the naturalness of generated speech.

**Benefits:**

- Engagement: Enhances user interactions with immediate and natural responses.
- Scalability: Manages large volumes of requests without compromising quality.
- Versatility: Suitable for various applications, from customer service to entertainment.

**Multimodal Intelligence:**

Cartesia.ai’s multimodal intelligence capabilities provide comprehensive solutions for creating interactive and engaging content:

Text, Audio, Video, Images

- Integrated Media: Combine text, audio, video, and images for more immersive experiences.
- Advanced AI Models: Utilize state-of-the-art AI models for high-quality media processing.

**Developer API:**

Cartesia.ai offers a robust API with comprehensive documentation and SDKs, facilitating seamless integration:

**Integration:**

- SDKs: Available for multiple programming languages.
- Low Latency: Supports real-time applications with quick response times.
- Documentation: Detailed guides and support for easy implementation.

**Use Cases:**

- Interactive Applications: Real-time voice generation for chatbots and virtual assistants.
- On-demand Voice Generation: Seamlessly integrate into content creation workflows.

**Use Cases for Cartesia.ai:**

Cartesia.ai’s versatile platform supports a wide range of applications:

**Marketing:**

Create engaging marketing content with high-quality voiceovers, transforming scripts into professional audio quickly and efficiently.


**Real-time Applications:**

Build real-time conversational experiences with ultrafast voice synthesis, ensuring every interaction is instant and engaging.


**Content Creation:**

Simplify content creation and produce high-quality audio for videos and other media at scale, reducing the time and effort required for traditional recording methods.

**Impact on Content Creation:**

Cartesia.ai is revolutionizing content creation by providing tools that enhance productivity and engagement. By automating voice generation and integrating multimodal intelligence, creators can focus on producing high-quality content without the time-consuming task of manual media creation. This boosts productivity and allows for greater creative freedom and innovation.

**Innovation and Research:**

Cartesia.ai is committed to continuous innovation and research in AI technology. Their team of experts focuses on advancing the capabilities of multimodal AI, exploring new applications, and refining existing technologies to stay at the forefront of the industry.

**AI Safety and Ethics:**

Ensuring the ethical use of AI is a core principle at Cartesia.ai. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.

**Integrations and Compatibility:**

Cartesia.ai’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Cartesia.ai’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.


 This is the content for the doc fern/providers/voice/deepgram.mdx 

 ---
title: Deepgram
subtitle: What is Deepgram?
slug: providers/voice/deepgram
---


**What is Deepgram?**

Deepgram is a leading AI company specializing in advanced speech recognition and transcription technology. With their state-of-the-art speech-to-text solutions, Deepgram provides fast, accurate, and scalable transcription services for various applications. By leveraging deep learning and neural networks, Deepgram delivers unparalleled audio intelligence capabilities, transforming how we interact with and analyze spoken content.

**The Evolution of Speech Recognition:**

Speech recognition technology has dramatically evolved from its early, basic forms to the sophisticated systems we have today. Initially, speech recognition systems were limited by their ability to accurately transcribe spoken language. However, advancements in machine learning, particularly deep learning, have revolutionized this field. Deepgram has harnessed these technological advancements to offer highly accurate and efficient speech recognition solutions that set a new industry standard.

**Overview of Deepgram’s Offerings:**

Deepgram offers a comprehensive suite of AI-driven speech recognition tools designed to meet diverse needs:

**Speech-to-Text:**

- Deepgram’s core offering is its speech-to-text technology, which converts spoken language into written text with high accuracy. This technology supports real-time transcription and batch processing, making it ideal for various applications, including media transcription, customer service, and accessibility enhancements.

**Audio Intelligence:**

- Deepgram’s audio intelligence capabilities go beyond simple transcription. Their technology can analyze audio to detect sentiment, intent, and topics, providing deeper insights into conversations and spoken content. This feature is particularly useful for businesses seeking to understand customer interactions and improve service delivery.

**Speech-to-Text Technology:**

- Deepgram’s speech-to-text technology stands out for its precision and speed. By utilizing end-to-end deep learning models, Deepgram achieves higher accuracy rates than traditional transcription methods. This technology can handle diverse accents, dialects, and noisy environments, ensuring reliable performance in real-world scenarios.

**Real-time Transcription:**

- Real-time transcription is one of Deepgram’s key features, enabling instant conversion of speech to text. This is particularly advantageous for applications such as live captioning, real-time customer support, and interactive voice response (IVR) systems. The ability to transcribe speech in real-time enhances user experience and operational efficiency.

**Audio Intelligence:**

- Deepgram’s audio intelligence features allow for advanced analysis of audio content. By detecting sentiment, intent, and topics within conversations, businesses can gain valuable insights into customer behavior and preferences. This information can be used to improve customer service, tailor marketing strategies, and enhance overall business intelligence.

**Use Cases for Deepgram:**

Deepgram’s technology is versatile and applicable across multiple industries:

**Media Transcription:**

In the media sector, Deepgram’s speech-to-text solutions are used to transcribe interviews, podcasts, and video content. This makes content searchable, accessible, and easier to manage, enhancing the efficiency of media production workflows.

**Contact Centers:**

- For contact centers, Deepgram provides real-time transcription and audio analysis to improve customer interactions. By transcribing calls and analyzing sentiment, businesses can identify trends, monitor agent performance, and enhance customer satisfaction.

**Healthcare:**

- In healthcare, accurate transcription is critical for maintaining patient records and documenting medical consultations. Deepgram’s speech-to-text technology ensures precise and timely transcription, aiding in effective communication and record-keeping.

**Impact on Content Creation:**

Deepgram is transforming content creation by providing tools that streamline the transcription process. By automating transcription, content creators can focus on producing high-quality material without the time-consuming task of manual transcription. This boosts productivity and opens new avenues for creative and professional work.

**Innovation and Research:**

Deepgram is committed to continuous innovation and research in the field of speech recognition and AI. Their team of experts is dedicated to enhancing the capabilities of their technology, exploring new applications, and pushing the boundaries of what speech AI can achieve.

**AI Safety and Ethics:**

Ensuring the ethical use of AI is a core principle at Deepgram. They implement robust safeguards to prevent misuse of their technology and are actively engaged in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.

Integrations and Compatibility

Deepgram’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Deepgram’s speech recognition capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.

 This is the content for the doc fern/providers/voice/elevenlabs.mdx 

 ---
title: ElevenLabs
subtitle: How Vapi Integrates Text-to-Speech Platforms?
slug: providers/voice/elevenlabs
---


# How Vapi Integrates Text-to-Speech Platforms: ElevenLabs

In the realm of voice AI development, integrating cutting-edge text-to-speech (TTS) platforms is crucial for creating natural and engaging conversational experiences. This guide explores how developers can leverage our voice AI platform to seamlessly incorporate advanced TTS services like ElevenLabs, enabling the creation of sophisticated voice-driven applications with remarkable efficiency.

## Understanding the Voice AI Platform

Our platform serves as a comprehensive toolkit for developers, designed to simplify the complexities inherent in voice AI development. By abstracting intricate technical details, it allows developers to focus on crafting the core business logic of their applications rather than grappling with low-level implementation challenges.

### Key Components of the Voice AI Architecture

At the heart of our platform lies a robust architecture comprising three essential components:

1. Automatic Speech Recognition (ASR)
2. Large Language Model (LLM) processing
3. Text-to-Speech (TTS) integration

These components work in concert to facilitate seamless voice interactions. The ASR module captures and processes audio inputs, converting spoken words into digital data. The LLM processing unit analyzes this data, interpreting context and generating appropriate responses. Finally, the TTS integration transforms these responses back into natural-sounding speech.

## Integration with Text-to-Speech Platforms

Our approach to integrating external TTS services, such as ElevenLabs, is designed to be both flexible and powerful. By incorporating advanced TTS platforms, developers can significantly enhance the quality and versatility of their voice AI applications.

### ElevenLabs Integration: A Technical Deep Dive

The integration with ElevenLabs' AI speech synthesis exemplifies our commitment to providing developers with state-of-the-art tools. This integration process involves several key technical aspects:

1. **API Integration**: Our platform seamlessly connects with ElevenLabs' API, allowing for efficient data exchange and real-time speech synthesis.

2. **Voice Model Selection**: Developers can choose from a range of voice models provided by ElevenLabs, each with unique characteristics and tonal qualities.

3. **Parameter Control**: Fine-tuning of speech parameters such as speed, pitch, and emphasis is made accessible through our intuitive interface.

4. **Data Flow Optimization**: We've implemented efficient data handling mechanisms to ensure smooth transmission between our platform and ElevenLabs' servers, minimizing latency and maintaining high-quality output.

## Advanced Features of the Integration

The integration of ElevenLabs' technology brings forth a suite of advanced features that elevate the capabilities of voice AI applications.

### Contextual Awareness in Speech Synthesis

By leveraging ElevenLabs' sophisticated algorithms, our platform enables AI-generated speech that demonstrates a high degree of contextual awareness. This results in more natural-sounding conversations that can adapt to the nuances of different scenarios and user interactions.

### Enhanced Voice Modulation and Emotional Expression

The integration allows for precise control over voice modulation and emotional expression. Developers can craft AI voices that convey a wide range of emotions, from excitement to empathy, enhancing the overall user experience and making interactions more engaging and human-like.

### Real-time Audio Streaming Capabilities

One of the most compelling features of our integration is the ability to leverage ElevenLabs' streaming capabilities for real-time applications. This functionality is crucial for creating responsive voice AI systems that can engage in dynamic, live interactions.

Implementing low-latency voice synthesis presents several technical challenges, including:

- **Network Latency Management**: Minimizing delays in data transmission between our platform, ElevenLabs' servers, and the end-user's device.
- **Buffer Optimization**: Balancing audio quality with real-time performance through careful buffer management.
- **Adaptive Bitrate Streaming**: Implementing techniques to adjust audio quality based on network conditions, ensuring consistent performance across various environments.

Our platform addresses these challenges through advanced streaming protocols and optimized data handling, enabling developers to create voice AI applications that respond with near-human speed and fluidity.

## Developer Tools and Resources

To facilitate the integration process, we provide a comprehensive set of developer tools and resources:

- **SDKs**: Open-source software development kits available on GitHub, supporting multiple programming languages.
- **Documentation**: Detailed API references and conceptual guides covering key aspects of voice AI development.
- **Quickstart Guides**: Step-by-step tutorials to help developers get up and running quickly.
- **End-to-End Examples**: Sample implementations of common voice workflows, including outbound sales calls, inbound support interactions, and web-based voice interfaces.

### Building Custom Voice AI Applications

Developers can follow these steps to create voice AI applications with integrated TTS:

1. **Define the Use Case**: Clearly outline the objectives and scope of the voice AI application.
2. **Select the Appropriate Voice Model**: Choose an ElevenLabs voice that aligns with the application's tone and purpose.
3. **Implement Core Logic**: Utilize our SDKs to implement the application's business logic and conversation flow.
4. **Configure TTS Parameters**: Fine-tune speech synthesis settings to achieve the desired voice characteristics.
5. **Test and Iterate**: Conduct thorough testing to ensure natural conversation flow and appropriate responses.
6. **Optimize Performance**: Leverage our platform's analytics tools to identify and address any performance bottlenecks.

Best practices for optimizing voice AI performance and user experience include:

- Implementing effective error handling and fallback mechanisms
- Designing clear and concise conversation flows
- Regularly updating and refining language models based on user interactions
- Optimizing for low-latency responses to maintain natural conversation cadence

## Use Cases and Applications

The integration of advanced TTS platforms opens up a myriad of possibilities across various industries:

- **Customer Service**: Creating empathetic and efficient AI-powered support agents.
- **Education**: Developing interactive language learning tools with native-speaker quality pronunciation.
- **Healthcare**: Building voice-based assistants for patient engagement and medical information delivery.
- **Entertainment**: Crafting immersive storytelling experiences with dynamically generated character voices.

Developers can leverage this integration to create unique voice-based solutions that were previously challenging or impossible to implement with traditional TTS technologies.

## Future Developments and Potential

As the field of voice AI continues to advance, our platform is poised to incorporate new features and improvements in TTS integration capabilities. Upcoming developments may include:

- Enhanced multilingual support for global applications
- More sophisticated emotional intelligence in voice synthesis
- Improved personalization capabilities, allowing for voice adaptation based on user preferences

The future of voice AI development is likely to see increased focus on natural language understanding, context-aware responses, and seamless multi-modal interactions. Our platform is well-positioned to address these trends, providing developers with the tools they need to stay at the forefront of voice technology innovation.

## Conclusion

The integration of advanced text-to-speech platforms like ElevenLabs into our voice AI development ecosystem represents a significant leap forward for developers seeking to create sophisticated, natural-sounding voice applications. By abstracting complex technical challenges and providing robust tools and resources, we enable developers to focus on innovation and creativity in their voice AI projects. As the technology continues to evolve, our platform will remain at the cutting edge, empowering developers to build the next generation of voice-driven experiences.

 This is the content for the doc fern/providers/voice/imnt.mdx 

 ---
title: LMNT
subtitle: What is LMNT?
slug: providers/voice/imnt
---


**What is LMNT?**

LMNT is a cutting-edge AI platform that specializes in ultrafast and lifelike speech synthesis. By leveraging advanced AI technology, LMNT offers solutions for creating high-quality, natural-sounding speech from text. Their innovative voice cloning technology allows users to generate studio-quality voice replicas with minimal input, transforming the way businesses and developers create and use voice content.

**The Evolution of AI Speech Synthesis:**

AI speech synthesis has come a long way from its early, rudimentary forms to the sophisticated, lifelike voices we have today. Advances in deep learning, neural networks, and data processing have enabled the creation of speech that is virtually indistinguishable from human voices. LMNT has harnessed these advancements to provide fast, reliable, and highly expressive speech synthesis solutions.

**Overview of LMNT’s Offerings:**

LMNT provides a range of AI-driven speech synthesis tools designed to meet diverse needs:

**Ultrafast Speech Synthesis:**

- LMNT’s speech synthesis technology is designed for speed and efficiency, delivering low latency streaming that is ideal for conversational applications, virtual agents, and interactive games. This technology ensures that every interaction is immediate and engaging, enhancing user experience and operational efficiency.


**Voice Cloning:**

- LMNT offers advanced voice cloning capabilities, allowing users to create lifelike and expressive voice replicas with as little as a 5-minute recording. Instant voice clones can be generated from just 15 seconds of audio. Users can also choose from a library of pre-built voices, making it easy to find the perfect match for any project.

**Developer API:**

- LMNT provides a robust API with SDKs for Python and Node.js, enabling seamless integration of their voice synthesis capabilities into various applications. The API supports ultrafast, low-latency streaming, making it perfect for real-time voice generation and playback scenarios. Comprehensive documentation and support are available to assist developers through every step of the integration process.

**Ultrafast Speech Synthesis:**

- LMNT’s ultrafast speech synthesis technology offers several key features and benefits:

**Features:**

- Low Latency Streaming: Designed for real-time applications, ensuring quick response times.
- High Availability: Reliable performance under high loads, making it suitable for large-scale deployments.
- Expressive Voices: Capable of conveying a wide range of emotions and nuances, enhancing the naturalness of generated speech.

**Benefits:**

- Engagement: Immediate and natural interactions improve user engagement and satisfaction.
- Scalability: Handle large volumes of requests without compromising on performance or quality.
- Versatility: Suitable for a wide range of applications, from customer service to entertainment.

**Voice Cloning:**

- LMNT’s voice cloning technology sets a new standard in creating lifelike and expressive voices:

**Creating Lifelike Voices:**

- Studio-quality Cloning: Generate high-fidelity voice clones with minimal recording input.
- Instant Voice Cloning: Create usable voice clones from just a few seconds of audio.
- Voice Library: Access a diverse library of pre-built voices for immediate use.

**Applications:**

- Personalization: Create unique voices for digital assistants, characters, and branding.
- Content Creation: Generate consistent and professional voiceovers for videos, podcasts, and more.

**Developer API:**

- LMNT’s developer API simplifies the integration of voice synthesis capabilities into various projects:

**Integration:**

- SDKs: Ready-to-use SDKs for Python and Node.js.
- Low Latency: Real-time voice synthesis for interactive applications.
- Documentation: Comprehensive guides and support for easy implementation.

**Use Cases:**

- Interactive Applications: Real-time voice generation for chatbots and virtual assistants.
- On-demand Voice Generation: Seamless integration into content creation workflows.

**Use Cases for LMNT:**

- LMNT’s versatile platform supports a wide range of applications:

**Marketing:**

Create engaging product marketing videos with captivating voiceovers, turning scripts into high-quality audio content quickly and efficiently.

**Real-time Conversations:**

Build lightning-fast conversational experiences with ultrafast speech synthesis, ensuring every interaction is instant and engaging.

**Content Creation:**

Simplify content creation and produce high-quality audio for videos and avatars at scale, reducing the time and effort required for traditional recording methods.

**Impact on Content Creation:**

LMNT is revolutionizing content creation by providing tools that enhance productivity and engagement. By automating voice generation, creators can focus on producing high-quality content without the time-consuming task of manual voice recording. This not only boosts productivity but also allows for more creative freedom and innovation.

**Innovation and Research:**

LMNT is committed to continuous innovation and research in the field of AI speech synthesis. Their team of experts is dedicated to improving the naturalness, expressiveness, and versatility of AI-generated voices. By exploring new applications and refining existing technologies, LMNT aims to stay at the forefront of the industry.

**AI Safety and Ethics:**

Ensuring the ethical use of AI is a core principle at LMNT. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.

**Integrations and Compatibility:**

LMNT’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate LMNT’s voice synthesis capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.

 This is the content for the doc fern/providers/voice/neets.mdx 

 ---
title: Neets
subtitle: What is Neets.ai?
slug: providers/voice/neets
---


**What is Neets.ai?**

Neets.ai is an innovative AI platform specializing in ultrafast text-to-speech (TTS) and voice cloning technology. Offering competitive pricing and extensive multilingual support, Neets.ai enables businesses and developers to generate high-quality, natural-sounding speech quickly and efficiently. Their cutting-edge solutions cater to a wide range of applications, from marketing and real-time conversations to content creation and beyond.

**The Evolution of AI Speech Technology:**

AI speech technology has evolved from basic, robotic outputs to sophisticated, human-like voices. Advances in deep learning and neural networks have paved the way for realistic and expressive speech synthesis. Neets.ai leverages these advancements to provide ultrafast, high-quality TTS and voice cloning capabilities, revolutionizing how we create and interact with voice content.

**Overview of Neets.ai’s Offerings:**

Neets.ai offers a comprehensive suite of AI-driven TTS tools designed to meet diverse needs:

**Ultrafast Text-to-Speech:**

- Neets.ai’s TTS technology is optimized for speed and efficiency, delivering low-latency streaming that is perfect for real-time applications such as virtual assistants and interactive games. This technology ensures immediate and engaging user interactions.


**Voice Cloning:**

- Neets.ai provides advanced voice cloning capabilities, enabling users to create high-quality, expressive voice replicas with minimal input. This feature is ideal for personalizing digital experiences and generating unique audio content.


**Multilingual Support:**

- With support for over 80 languages, Neets.ai ensures that users can generate speech in a wide range of languages, catering to global audiences. This extensive language support makes Neets.ai a versatile solution for international applications.

**Ultrafast Text-to-Speech:**

- Neets.ai’s ultrafast TTS technology offers several key features and benefits:

**Features:**

- Low Latency Streaming: Ideal for real-time applications, ensuring quick response times.
- High Availability: Reliable performance even under high loads.
- Expressive Voices: Capable of conveying a wide range of emotions and nuances.

**Benefits:**

- Engagement: Immediate and natural interactions enhance user engagement.
- Scalability: Handles large volumes of requests without compromising performance or quality.
- Versatility: Suitable for various applications, from customer service to entertainment.

**Voice Cloning:**

- Neets.ai’s voice cloning technology sets a new standard in creating high-quality, expressive voices:

**Creating High-Quality Voices:**

- Minimal Input: Generate high-fidelity voice clones with minimal recording input.
- Expressive Replicas: Maintain the nuances and emotional expressiveness of the original voice.

**Applications:**

- Personalization: Create unique voices for digital assistants and characters.
- Content Creation: Generate consistent and professional voiceovers for various media.

**Multilingual Support:**

- Neets.ai supports a wide range of languages, ensuring global accessibility:

**Supporting 80+ Languages:**

- Extensive Language Options: Generate speech in over 80 languages, catering to diverse audiences.
- Global Reach: Ideal for international applications and content localization.

**Developer API:**

Neets.ai provides a robust API for easy integration:


**Integration:**

- SDKs: Ready-to-use SDKs for various programming languages.
- Low Latency: Real-time speech synthesis for interactive applications.
- Documentation: Comprehensive guides and support for seamless implementation.

**Use Cases:**

- Interactive Applications: Real-time voice generation for chatbots and virtual assistants.
- On-demand Voice Generation: Seamless integration into content creation workflows.

**Use Cases for Neets.ai:**

Neets.ai’s versatile platform supports a wide range of applications:

**Marketing:**

Create engaging marketing videos with captivating voiceovers, transforming scripts into high-quality audio content quickly and efficiently.


**Real-time Applications:**

Build real-time conversational experiences with ultrafast TTS, ensuring every interaction is instant and engaging.


**Content Creation:**

Simplify content creation and produce high-quality audio for videos and other media at scale, reducing the time and effort required for traditional recording methods.

**Impact on Content Creation:**

Neets.ai is revolutionizing content creation by providing tools that enhance productivity and engagement. By automating voice generation, creators can focus on producing high-quality content without the time-consuming task of manual voice recording. This boosts productivity and allows for greater creative freedom and innovation.

**Innovation and Research:**

Neets.ai is committed to continuous innovation and research in the field of AI speech technology. Their team of experts is dedicated to improving the naturalness, expressiveness, and versatility of AI-generated voices. By exploring new applications and refining existing technologies, Neets.ai aims to stay at the forefront of the industry.

**AI Safety and Ethics:**

Ensuring the ethical use of AI is a core principle at Neets.ai. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.

**Integrations and Compatibility:**

Neets.ai’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Neets.ai’s voice synthesis capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.

 This is the content for the doc fern/providers/voice/openai.mdx 

 ---
title: OpenAI
subtitle: What is OpenAI?
slug: providers/voice/openai
---


**What is OpenAI?**

OpenAI is a leading artificial intelligence research and deployment company dedicated to ensuring that artificial general intelligence (AGI) benefits all of humanity. Founded with the mission to create safe and highly capable AI systems, OpenAI has made significant strides in AI research, producing groundbreaking models like GPT-4, DALL-E, and Codex. These innovations have not only advanced the field of AI but also transformed various industries by providing powerful tools for natural language processing, image generation, and programming assistance.

**The Evolution of AI Research:**

The field of AI has evolved rapidly over the past few decades. From early rule-based systems to modern deep learning models, AI technology has made significant progress in mimicking human intelligence. OpenAI has been at the forefront of this evolution, pushing the boundaries of what AI can achieve through continuous research and development. Their work on large language models and neural networks has set new benchmarks for performance and capability in AI systems.

**Overview of OpenAI’s Offerings:**

OpenAI offers a range of AI-driven products and services designed to meet diverse needs:

**GPT Models:**

- OpenAI’s Generative Pre-trained Transformer (GPT) models, including the latest GPT-4, are state-of-the-art in natural language processing. These models can generate human-like text, answer questions, summarize information, and perform various language tasks with high accuracy. GPT-4, in particular, represents a significant leap in AI capabilities, offering improved coherence, context understanding, and creativity.

**DALL-E:**

- DALL-E is OpenAI’s revolutionary image generation model that creates detailed and imaginative images from text descriptions. By combining deep learning with creative processes, DALL-E can produce unique artwork, design concepts, and visual content that aligns with the given textual input. This technology opens new possibilities for artists, designers, and content creators.

**Codex:**


- Codex is an AI model that assists with programming by understanding and generating code. Integrated into tools like GitHub Copilot, Codex can help developers write code faster and more efficiently by suggesting code snippets, debugging errors, and automating repetitive tasks. This enhances productivity and reduces the barrier to entry for learning programming languages.

**GPT-4 Technology:**

- GPT-4 is the latest and most advanced language model developed by OpenAI. It excels in generating coherent and contextually relevant text, making it a powerful tool for various applications, including chatbots, content creation, and automated customer support. With enhanced capabilities for understanding and generating human language, GPT-4 sets a new standard in AI-driven communication.

**DALL-E Image Generation:**

- DALL-E takes text-to-image generation to new heights, allowing users to create visually stunning and highly specific images based on textual descriptions. This technology is particularly valuable for creative industries, where generating unique visuals quickly and accurately is essential. From concept art to marketing materials, DALL-E provides a versatile tool for visual content creation.

**Codex and Programming Assistance:**

- Codex transforms the way developers interact with code by providing intelligent suggestions and automating routine programming tasks. This AI-powered assistant understands multiple programming languages and can generate code snippets, making coding more accessible and efficient. Integrated into platforms like GitHub Copilot, Codex helps streamline the development process and accelerates software production.

**Use Cases for OpenAI:**

OpenAI’s technologies are versatile and applicable across various sectors:

**Education:**

In education, GPT-4 and Codex can enhance learning experiences by providing personalized tutoring, generating educational content, and assisting with coding exercises. These tools help students grasp complex concepts and improve their programming skills.

**Business:**

Businesses leverage OpenAI’s models for automating customer support, generating marketing content, and analyzing large volumes of text data. GPT-4’s ability to understand and generate human-like text enhances customer interactions and drives operational efficiency.

**Creative Industries:**

In the creative sector, DALL-E and GPT-4 enable artists and writers to generate new ideas, create unique visuals, and produce high-quality content. These tools expand creative possibilities and streamline content production workflows.

**Innovation and Research:**

OpenAI is committed to advancing AI through continuous research and innovation. Their team of researchers and engineers works on developing new models, improving existing technologies, and exploring novel applications of AI. This commitment to innovation ensures that OpenAI remains at the cutting edge of the field.

**AI Safety and Ethics:**

Ensuring the safe and ethical use of AI is a core principle at OpenAI. They implement rigorous safety measures to prevent misuse and ensure that their technologies are used responsibly. OpenAI is also involved in global discussions about AI ethics and governance, contributing to the development of best practices and standards for the industry.

**Integrations and Compatibility:**

OpenAI’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate OpenAI’s AI capabilities into their existing systems, enhancing functionality and improving user experience. The API is designed to be flexible and user-friendly, accommodating a wide range of use cases.

 This is the content for the doc fern/providers/voice/playht.mdx 

 ---
title: PlayHT
subtitle: What is PlayHT?
slug: providers/voice/playht
---


**What is PlayHT?**

In the dynamic world of artificial intelligence, PlayHT emerges as a leading provider of voice AI solutions. Specializing in text-to-speech (TTS) and voice cloning technologies, PlayHT delivers highly realistic and versatile AI-generated voices that cater to a wide array of applications. From enhancing marketing videos to making content more accessible, PlayHT’s innovative tools empower users to create engaging and professional-grade audio content effortlessly.

**The Evolution of AI Voice Technology:**

AI voice technology has significantly evolved over the past decade. Initially limited to robotic and monotone outputs, advancements in machine learning and neural networks have paved the way for natural and expressive voice synthesis. PlayHT has harnessed these advancements to offer superior AI voices that are nearly indistinguishable from human speech, setting a new standard in the industry.

**Overview of PlayHT’s Offerings:**

PlayHT provides a robust suite of voice AI tools designed to meet diverse needs:

**Text to Speech:**

- PlayHT’s TTS technology converts written text into highly realistic speech, making it ideal for creating voiceovers, audiobooks, and other spoken content. This technology supports over 142 languages and accents, allowing users to generate audio content that is not only clear and engaging but also linguistically diverse.

**Voice Cloning:**

- PlayHT’s voice cloning feature enables users to create digital replicas of voices with high accuracy. This is particularly useful for preserving voices, personalizing digital assistants, and generating unique character voices for media and entertainment. The cloned voices maintain the nuances and emotional expressiveness of the original, ensuring a lifelike audio experience.

**Voice Generation API:**

- PlayHT offers a Voice Generation API that allows developers to integrate AI voice capabilities into their applications. This API supports real-time voice synthesis and cloning, providing a flexible and powerful solution for various interactive applications, including chatbots, virtual assistants, and gaming.

**Use Cases for PlayHT:**

- The applications of PlayHT’s technology are extensive and impactful:

**Marketing:**

- In the marketing sector, PlayHT’s realistic AI voices enhance the quality of promotional videos, explainer videos, and advertisements. Brands can create consistent and professional voiceovers that captivate audiences and convey messages effectively.

**E-Learning:**

- For educational content, PlayHT provides voices capable of pronouncing complex terminologies and acronyms, making e-learning materials more engaging and easier to understand. This helps in creating comprehensive and interactive training modules.

**Accessibility:**

- PlayHT’s TTS technology is a boon for accessibility, converting text into speech to assist individuals with visual impairments or reading difficulties. This promotes inclusivity and ensures that information is accessible to all.

**Gaming:**

- In the gaming industry, PlayHT’s voice cloning and TTS capabilities bring characters to life, enhancing the overall gaming experience. Developers can quickly generate high-quality voiceovers for dialogues, narration, and character interactions.

**Impact on Content Creation:**

- PlayHT is revolutionizing content creation by offering tools that are both powerful and user-friendly. By enabling creators to produce high-quality audio content quickly and efficiently, PlayHT reduces the time and costs associated with traditional recording methods. This democratizes access to professional-grade audio production, fostering innovation and creativity across various domains.

**Innovation and Research:**

Committed to pushing the boundaries of voice AI, PlayHT invests in continuous research and development. Their team of experts focuses on enhancing the quality, expressiveness, and versatility of AI-generated voices, exploring new applications, and refining existing technologies.

**AI Safety and Ethics:**

PlayHT prioritizes the ethical use of AI technology. They have implemented stringent safeguards to prevent misuse and are actively engaged in discussions about the responsible development and deployment of AI. Ensuring the privacy and security of users’ data is a core aspect of their operations.

**Integrations and Compatibility:**

PlayHT’s Voice Generation API enables seamless integration with various platforms and applications. This flexibility ensures that users can incorporate PlayHT’s voice AI capabilities into their existing systems without any hassle, streamlining workflows and enhancing functionality.

 This is the content for the doc fern/providers/voice/rimeai.mdx 

 ---
title: RimeAI
subtitle: What is Rime.ai?
slug: providers/voice/rimeai
---


**What is Rime.ai?**

Rime.ai is a pioneering platform in the field of speech synthesis, offering real-time, lifelike voice generation. Specializing in creating natural-sounding voices tailored to demographic specifics, Rime.ai provides tools that allow businesses and developers to engage their audiences more effectively. By leveraging advanced AI, Rime.ai delivers high-quality audio that is indistinguishable from human speech, setting a new standard in the industry.

**The Evolution of AI Speech Synthesis:**

AI speech synthesis has come a long way from its early days of robotic-sounding outputs. Advances in machine learning, neural networks, and data processing have transformed synthetic speech into highly realistic and expressive audio. Rime.ai has harnessed these technological advancements to create voices that sound natural and convey the desired emotions and nuances.

**Overview of Rime.ai’s Offerings:**

Rime.ai provides a comprehensive suite of speech synthesis tools designed to meet various needs:

**Real-time Speech Synthesis:**

- Rime.ai’s real-time speech synthesis technology enables instant generation of lifelike voices. This is particularly useful for applications requiring immediate feedback, such as interactive voice response (IVR) systems, live virtual assistants, and real-time translation services. The technology boasts sub-300 millisecond response times, ensuring seamless and efficient communication.

**Demographically Specific Voice Control:**

- One of Rime.ai’s standout features is its ability to generate voices that are demographically specific. This means businesses can tailor their audio output to match the cultural, regional, and social characteristics of their target audience. With over 200 distinct voices available, Rime.ai allows for precise customization, enhancing user engagement and relatability.

**Use Cases for Rime.ai:**


- Rime.ai’s technology is versatile and applicable across multiple sectors:

**IVR Systems:**

- Interactive voice response systems benefit greatly from Rime.ai’s real-time speech synthesis. By providing natural and clear voices, IVR systems can improve user interactions, reduce call handling times, and enhance overall customer satisfaction.

**Newsreading:**

In the media industry, Rime.ai’s lifelike voices can be used for automated newsreading, delivering news updates in a natural and engaging manner. This ensures consistency and professionalism in audio content delivery.

**Narration:**

- For audiobooks, educational materials, and other forms of narration, Rime.ai offers high-quality voice generation that enhances the listening experience. The ability to match voices to the content’s demographic audience further adds to the personalization and effectiveness of the narration.

**Impact on Content Creation:**

Rime.ai is revolutionizing content creation by providing tools that allow for quick and efficient production of high-quality audio. By eliminating the need for traditional recording methods, creators can save time and resources while still producing professional-grade content. This democratization of audio production opens up new opportunities for innovation and creativity.

**Innovation and Research:**

Rime.ai is committed to continuous innovation and research in speech synthesis technology. Their team of experts is dedicated to improving the naturalness, expressiveness, and versatility of AI-generated voices. By exploring new applications and refining existing technologies, Rime.ai aims to stay at the forefront of the industry.

**AI Safety and Ethics:**

Ensuring the ethical use of AI is a top priority for Rime.ai. They have implemented robust safeguards to prevent misuse of their technology and are actively involved in discussions about responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their approach.

**Integrations and Compatibility:**

Rime.ai’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Rime.ai’s speech synthesis capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.

 This is the content for the doc fern/providers/voice/sesame.mdx 

 ---
title: Sesame
subtitle: What is Sesame CSM-1B?
slug: providers/voice/sesame
---

**What is Sesame CSM-1B?**

Sesame CSM-1B is an open source text-to-speech (TTS) model that Vapi hosts for seamless integration into your voice applications. Currently in beta, this model delivers natural-sounding speech synthesis with a single default voice option.

**Key Features:**

- **Vapi-Hosted Solution**: Access this open source model directly through Vapi without managing your own infrastructure
- **Single Default Voice**: Currently offers one voice option optimized for clarity and naturalness
- **Beta Release**: Early access to this emerging TTS technology

**Integration Benefits:**

- Simplified setup with no need to self-host the model
- Consistent performance through Vapi's optimized infrastructure
- Seamless compatibility with all Vapi voice applications

**Use Cases:**

- Virtual assistants and conversational AI
- Content narration and audio generation
- Interactive voice applications
- Prototyping voice-driven experiences

**Current Limitations:**

As this is a beta release, the model currently offers limited customization options with only one default voice available. Additional features and voice options may be introduced in future updates.

 This is the content for the doc fern/providers/voice/vapi-voices.mdx 

 ---
title: Vapi Voices
subtitle: Our curated selection of high-quality voices
slug: providers/voice/vapi-voices
---

## What are Vapi Voices?

Vapi Voices is our carefully curated selection of high-quality voices designed to simplify your voice AI implementation. We offer a small handful of exceptional voices that you can immediately use in your applications.

## Why Choose Vapi Voices?

- **Simple**: Skip testing dozens of voices
- **Quality**: Each voice is vetted for natural sound
- **Ready to Use**: Pre-optimized for your applications
- **Consistent**: Reliable quality in all interactions

## Available Voices

### Rohan
- **Gender**: Male
- **Accent**: Indian American
- **Age**: 24 years old
- **Characteristics**: Bright, optimistic, cheerful, energetic
- **Sample Audio**: <audio controls src="/static/audio/rohan-sample.wav">Your browser does not support the audio element.</audio>

### Neha
- **Gender**: Female
- **Accent**: Indian American
- **Age**: 30 years old
- **Characteristics**: Professional, charming
- **Sample Audio**: <audio controls src="/static/audio/neha-sample.wav">Your browser does not support the audio element.</audio>

### Hana
- **Gender**: Female
- **Accent**: American
- **Age**: 22 years old
- **Characteristics**: Soft, soothing, gentle
- **Sample Audio**: <audio controls src="/static/audio/hana-sample.wav">Your browser does not support the audio element.</audio>

### Harry
- **Gender**: Male
- **Accent**: American
- **Age**: 24 years old
- **Characteristics**: Clear, energetic, professional
- **Sample Audio**: <audio controls src="/static/audio/harry-sample.wav">Your browser does not support the audio element.</audio>

### Elliot
- **Gender**: Male
- **Accent**: Canadian
- **Age**: 25 years old
- **Characteristics**: Soothing, friendly, professional
- **Sample Audio**: <audio controls src="/static/audio/elliot-sample.wav">Your browser does not support the audio element.</audio>

### Lily
- **Gender**: Female
- **Accent**: Asian American
- **Age**: 25 years old
- **Characteristics**: Bright personality, bubbly, cheerful
- **Sample Audio**: <audio controls src="/static/audio/lily-sample.wav">Your browser does not support the audio element.</audio>

### Paige
- **Gender**: Female
- **Accent**: American
- **Age**: 26 years old
- **Characteristics**: Deeper tone, calming, professional
- **Sample Audio**: <audio controls src="/static/audio/paige-sample.wav">Your browser does not support the audio element.</audio>

### Cole
- **Gender**: Male
- **Accent**: American
- **Age**: 22 years old
- **Characteristics**: Deeper tone, calming, professional
- **Sample Audio**: <audio controls src="/static/audio/cole-sample.wav">Your browser does not support the audio element.</audio>

### Savannah
- **Gender**: Female
- **Accent**: American (Southern)
- **Age**: 25 years old
- **Characteristics**: Southern American accent
- **Sample Audio**: <audio controls src="/static/audio/savannah-sample.wav">Your browser does not support the audio element.</audio>

### Spencer
- **Gender**: Female
- **Accent**: American
- **Age**: 26 years old
- **Characteristics**: Energetic, quippy, lighthearted, cheeky, amused
- **Sample Audio**: <audio controls src="/static/audio/spencer-sample.wav">Your browser does not support the audio element.</audio>

## How to Use

1. Select a voice from our collection
2. Integrate it into your assistant or squad
3. Start creating voice interactions immediately

## Common Use Cases

- Customer service virtual agents
- Content voiceovers and narration
- Interactive conversational applications

By choosing Vapi Voices, you can focus on building great applications rather than spending time selecting and testing voices.


 This is the content for the doc fern/providers/voiceflow.mdx 

 ---
title: Voiceflow
subtitle: Vapi x Voiceflow
slug: providers/voiceflow
---


## Overview

Voiceflow is a conversational AI platform that helps teams build, manage, and deploy AI agents, especially chatbots, to enhance customer experiences. It enables users to create advanced AI chatbots without coding, using a user-friendly drag-and-drop flow builder. This feature allows businesses to customize chatbot interactions and efficiently automate customer support processes.

To link Vapi with Voiceflow, host a proxy using Voiceflow's AI features. This proxy handles requests from Voiceflow, sends them to Vapi's text completion API, and returns Vapi's responses to Voiceflow. You'll need to host this proxy on your server to manage communication between Vapi and Voiceflow.

## Workshop

The workshop conducted by Vapi in collaboration with Voiceflow provided an in-depth exploration of building voice agents using the Voiceflow platform, deployed through Vapi. During this session, participants learned how to create voice agents that leverage Voiceflow's user-friendly design tools alongside Vapi's voice capabilities. The workshop featured a live demonstration, where attendees could see the entire process of building a voice agent in real-time, including designing the agent, setting up necessary integrations, and testing functionality.

<iframe
        src="https://www.youtube.com/embed/PbS9rfopZQA"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        width="100%"
        height="400px"
        allowfullscreen
/>
By the end of the workshop, participants gained insights into building and deploying voice agents, practical skills in designing conversational flows, and an understanding of voice agents.


 This is the content for the doc fern/quickstart.mdx 

 ---
title: Core Models
subtitle: The three core components to Vapi's voice AI pipeline.
slug: quickstart
---


At it's core, Vapi is an orchestration layer over three modules: the **transcriber**, the **model**, and the **voice**.

<Frame>
  <img src="./static/images/quickstart/quickstart-banner.png" />
</Frame>

These three modules can be swapped out with **any provider** of your choosing; OpenAI, Groq, Deepgram, ElevenLabs, PlayHT, etc. You can even plug in your server to act as the LLM.

Vapi takes these three modules, optimizes the latency, manages the scaling & streaming, and orchestrates the conversation flow to make it sound human.

<Steps titleSize="h3">
  <Step title="Listen (intake raw audio)">
    <div>
      When a person speaks, the client device (whether it is a laptop, phone,
      etc) will record raw audio (1’s & 0’s at the core of it).
    </div>
    <div>
      This raw audio will have to either be transcribed on the client device
      itself, or get shipped off to a server somewhere to turn into
      transcription text.
    </div>
  </Step>
  <Step title="Run an LLM">
    <div>
      That transcript text will then get fed into a prompt & run through an LLM
      ([LLM inference](/glossary#inference)). The LLM is the core intelligence
      that simulates a person behind-the-scenes.
    </div>
  </Step>
  <Step title="Speak (text → raw audio)">
    <div>
      The LLM outputs text that now must be spoken. That text is turned back
      into raw audio (again, 1’s & 0’s), that is playable back at the user’s
      device.
    </div>
    <div>
      This process can also either happen on the user’s device itself, or on a
      server somewhere (then the raw speech audio be shipped back to the user).
    </div>
  </Step>
</Steps>

<Info>The idea is to perform each phase in realtime (sensitive down to 50-100ms level), streaming between every layer. Ideally the whole flow [voice-to-voice](/glossary#voice-to-voice) clocks in at \<500-700ms.</Info>

Vapi pulls all these pieces together, ensuring a smooth & responsive conversation (in addition to providing you with a simple set of tools to manage these inner-workings).


 This is the content for the doc fern/quickstart/dashboard.mdx 

 ---
title: Dashboard
subtitle: Quickstart with the Vapi web dashboard.
slug: quickstart/dashboard
---

<Frame>
  <div class="video-embed-wrapper">
    <iframe
      src="https://www.youtube.com/embed/sFXaTsmMR8s?si=aV-mAdjwkpHchHfT"
      title='An embedded YouTube video titled "Quickstart: Vapi Dashboard"'
      frameborder="0"
      allow="fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      referrerpolicy="strict-origin-when-cross-origin"
    />
  </div>
</Frame>

### The Web Dashboard

One of the easiest ways to get started with Vapi is by using the web dashboard.

<Note>
You can visit your dashboard by going to [dashboard.vapi.ai](https://dashboard.vapi.ai)
</Note>

The web dashboard gives you the ability to:

- **view, create, & modify [assistants](/assistants)** associated with your account
- **provision & manage phone numbers** assistants can dial outbound from or receive inbound calls to
- **review conversation data** (such as audio recordings, call metadata, etc)
- **manage your [provider keys](/customization/provider-keys)** (used in communication with external [TTS](/glossary#tts), LLM, & [STT](/glossary#stt) vendors)

We will be walking through the core necessities you need to get up and running in this guide.

<Tip>
  The web dashboard wraps over much of the realtime call functionality of Vapi. The dashboard
  actually uses the [web SDK](/sdk/web) beneath-the-hood to make web calls.
</Tip>

## Vapi’s Pizzeria

In this guide we will be implementing a simple order-taking assistant at a pizza shop called “Vapi’s Pizzeria”.

Vapi’s has 3 types of menu items: `pizza`, `side`s, & `drink`s. Customers will be ordering 1 of each.

<Frame caption="Customers will order 3 items: 1 pizza, 1 side, & 1 drink. The assistant will handle the full order taking conversation.">
  <img src="../static/images/quickstart/vapis-pizzeria.png" />
</Frame>

## Assistant Setup

First we're going to set up our assistant in the dashboard. Once our assistant’s **transcriber**, **model**, & **voice** are set up, we can call it to place our order.

<Note>
You can visit your dashboard at [dashboard.vapi.ai](https://dashboard.vapi.ai/)
</Note>

<Markdown src="../snippets/quickstart/dashboard/assistant-setup-inbound.mdx" />

## Calling Your Assistant

Now that your assistant is fully setup & configured, we will want to contact it. There are 2 ways to "call in" to an assistant:

- **Over the Internet:** Network-enabled devices can contact Vapi via the Internet (i.e. web applications, mobile applications). No phone number is involved.
- **Via Telephony:** Phones can communicate to Vapi over a cellular network (i.e. phone call). One phone number dials to another phone number.

For our use case, it is most appropriate that customers will contact our assistant via an inbound
phone call. Though, we will look at both ways of calling in.

<AccordionGroup>
  <Accordion title="Call in the Dashboard" icon="camera-web" iconType="solid">
    The quickest way to contact your new assistant is by simply using the call button on the assistant detail page:

    <Frame caption="Call into your assistant via the dashboard.">
      <img src="../static/images/quickstart/dashboard/call-assistant-web-dashboard.png" />
    </Frame>

    <Tip>The dashboard uses the [web SDK](/sdk/web) underneath to make web calls.</Tip>

    This will start a web call with your assistant, you can now speak to it to order your pizza & sides!

  </Accordion>
  <Accordion title="Call via Phone" icon="phone-arrow-up-right" iconType="solid">
    Since our assistant is meant to take orders over the phone, we will want to set up [inbound calling](/phone-calling) to our assistant. We will need to do 2 things:

    1. **provision a new phone number** to sit our agent behind (it will pick-up calls that come in — hence "inbound calling")
    2. **place our agent behind that phone number**

    If you already have your own phone numbers (purchased via Twilio or Vonage, etc), you can import
    them into Vapi for use. Learn more about [telephony](/phone-calling) on Vapi.

    <AccordionGroup>
      <Accordion title="Provision a Phone Number" icon="hashtag" iconType="solid">
        <Markdown src="../snippets/quickstart/dashboard/provision-phone-number-with-vapi.mdx" />
      </Accordion>
      <Accordion title="Attach Your Assistant" icon="user-robot" iconType="solid">
        In the `Inbound` area of the phone number detail view, select your assistant in the dropdown under `Assistant`.

        <Frame caption="Your assistant will now pick-up calls made to this phone number.">
          <img src="../static/images/quickstart/dashboard/inbound-assistant-set.png" />
        </Frame>

        This will put your assistant behind the phone number for inbound calls. Your assistant is now ready to take calls.
      </Accordion>
    </AccordionGroup>

  </Accordion>
</AccordionGroup>

Your assistant should be able to accept calls & maintain a basic conversation. Happy ordering!

<Tip>
  Your assistant won't yet be able to hang-up the phone at the end of the call. We will learn more
  about configuring call end behaviour in later guides.
</Tip>


 This is the content for the doc fern/quickstart/inbound.mdx 

 ---
title: Inbound Calling
subtitle: Quickstart handling inbound calls with Vapi.
slug: quickstart/phone/inbound
---

<Frame>
  <div class="video-embed-wrapper">
    <iframe
      src="https://www.youtube.com/embed/e7q4p8Gg1Tg?si=ZvFumklEMubfbZi7"
      title='An embedded YouTube video titled "Quickstart: Inbound Calling"'
      frameborder="0"
      allow="fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      referrerpolicy="strict-origin-when-cross-origin"
    />
  </div>
</Frame>

An inbound call is a phone call that comes **"in"** towards a phone number, & in our case, our AI assistant will be there to pick up the phone call.

There are **4 steps** we will cover to handle our first inbound phone call:

1. **Create an Assistant:** we will create an [assistant](/assistants) & instruct it on how to conduct the call
2. **Get a Phone Number:** we can either import existing numbers we own, or create a free one through Vapi
3. **Attach Our Assistant:** we will put our assistant behind the phone number to pick up calls
4. **Call the Number:** we can then call the number & talk to our assistant

## Vapi’s Pizzeria

We will be implementing a simple order-taking assistant that receives customer calls at a pizza shop called “Vapi’s Pizzeria”.

Vapi’s has 3 types of menu items: `pizza`, `side`s, & `drink`s. Customers will be ordering 1 of each.

<Frame caption="Customers will order 3 items: 1 pizza, 1 side, & 1 drink. The assistant will pick up the phone & take the customer's order.">
  <img src="../static/images/quickstart/vapis-pizzeria.png" />
</Frame>

## Assistant Setup

First we're going to set up our assistant in the dashboard. Once our assistant’s **transcriber**, **model**, & **voice** are set up, we can call it to place our order.

<Info>
  You can visit your dashboard by going to
  [dashboard.vapi.ai](https://dashboard.vapi.ai)
</Info>

<Markdown src="../snippets/quickstart/dashboard/assistant-setup-inbound.mdx" />

## Get a Phone Number

Now that we've configured how our assistant will behave, we want to figure out how to call it. We will need a phone number that we can make phone calls to.

<Markdown src="../snippets/quickstart/phone/get-a-phone-number.mdx" />

## Attach Your Assistant

Now that we have a configured assistant & a phone number, we will put our assistant behind the phone number to pick up incoming phone calls.

In the `Inbound` area of the phone number detail view, select your assistant in the dropdown under `Assistant`.

<Frame caption="Your assistant will now pick-up calls made to this phone number.">
  <img src="../static/images/quickstart/dashboard/inbound-assistant-set.png" />
</Frame>

## Call the Number

You can now make a phone call to the number. Your assistant will pick up the phone & manage the order-taking conversation. Happy ordering!

<Tip>
  Your assistant won't yet be able to hang-up the phone at the end of the call.
  We will learn more about configuring call end behaviour in later guides.
</Tip>


 This is the content for the doc fern/quickstart/outbound.mdx 

 ---
title: Outbound Calling
subtitle: Quickstart sending outbound calls with Vapi.
slug: quickstart/phone/outbound
---

<Frame>
  <div class="video-embed-wrapper">
    <iframe
      src="https://www.youtube.com/embed/xMjTHCBCSbI?si=GlE8Ei78OrOh-ZrR"
      title='An embedded YouTube video titled "Quickstart: Outbound Calling"'
      frameborder="0"
      allow="fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      referrerpolicy="strict-origin-when-cross-origin"
    />
  </div>
</Frame>

An outbound call is a phone call that is dialed and goes **"out"** from a phone number, & in our case, our AI assistant will be doing the dialing.

There are **3 steps** we will cover to send our first outbound phone call:

1. **Create an Assistant:** we will create an [assistant](/assistants) & instruct it on how to conduct itself during the call
2. **Get a Phone Number:** we can either import existing numbers we own, or create a free one through Vapi
3. **Call Your Number:** we will set our assistant as the dialer, set the destination phone number, then make the call

We can then send the outbound call, hopefully someone picks up!

<Warning>
  It is a violation of FCC law to dial phone numbers without consent in an
  automated manner. See [Telemarketing Sales
  Rule](/glossary#telemarketing-sales-rule) to learn more.
</Warning>

## Vapi’s Pizzeria

We will be implementing a simple order-taking assistant for a pizza shop called “Vapi’s Pizzeria”.

Vapi’s has 3 types of menu items: `pizza`, `side`s, & `drink`s. Customers will be ordering 1 of each.

**Outbound Scenario:** We will imagine we are calling back a customer who originally called in to place an order. Our assistant is calling back to complete the ordering process with the customer.

<Frame caption="Customers can order 3 items: 1 pizza, 1 side, & 1 drink. The assistant will call the customer (who got disconnected) & finish the ordering process.">
  <img src="../static/images/quickstart/vapis-pizzeria.png" />
</Frame>

## Assistant Setup

First we're going to set up our assistant in the dashboard. Once our assistant’s **transcriber**, **model**, & **voice** are set up, we can have it call the customer to finish the order.

<Info>
  You can visit your dashboard by going to
  [dashboard.vapi.ai](https://dashboard.vapi.ai)
</Info>

<AccordionGroup>
  <Accordion title="Sign-up or Log-in to Vapi" icon="user-plus" iconType="solid">
    If you haven't already signed-up, you're going to need an account before you can use the web dashboard. When you visit [dashboard.vapi.ai](https://dashboard.vapi.ai) you may see something like this:

    <Frame>
      <img src="../static/images/quickstart/dashboard/auth-ui.png" />
    </Frame>

    Sign-up for an account (or log-in to your existing account) — you will then find yourself inside the web dashboard. It will look something like this:

    <Frame caption="Your dashboard may look a bit different if you already have an account with assistants in it. The main idea is that we’re in the dashboard now.">
      <img src="../static/images/quickstart/dashboard/vapi-dashboard-post-signup.png" />
    </Frame>

  </Accordion>
  <Accordion title="Create an Assistant" icon="layer-plus" iconType="solid">
    Now that you're in your dashboard, we're going to create an [assistant](/assistants).

    Assistants are at the heart of how Vapi models AI voice agents — we will be setting certain properties on a new assistant to model an order-taking agent.

    Once in the "Assistants" dashboard tab (you should be in it by-default after log-in), you will see a button to create a new assistant.

    <Frame caption="Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.">
      <img src="../static/images/quickstart/dashboard/create-new-assistant-button.png" />
    </Frame>

    After clicking the create new assistant button, you will see a pop-up modal that asks you to pick a starter template. For our example we will start from a blank slate so choose the `Blank Template` option.

    <Frame caption="Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.">
      <img src="../static/images/quickstart/dashboard/choose-blank-template.png" />
    </Frame>

    You will then be able to name your assistant — you can name it whatever you'd like (`Vapi’s Pizza Front Desk`, for example):

    <Info>
      This name is only for internal labeling use. It is not an identifier, nor will the assistant be
      aware of this name.
    </Info>

    <Frame caption="Name your assistant.">
      <img src="../static/images/quickstart/dashboard/name-your-assistant.png" />
    </Frame>

    Once you have named your assistant, you can hit "Create" to create it. You will then see something like this:

    <Frame caption="The assistant overview. You can edit your assistant’s transcriber, model, & voice — and edit other advanced configuration.">
      <img src="../static/images/quickstart/dashboard/assistant-created.png" />
    </Frame>

    This is the assistant overview view — it gives you the ability to edit different attributes about your assistant, as well as see **cost** & **latency** projection information for each portion of it’s voice pipeline (this is very important data to have handy when building out your assistants).

  </Accordion>
  <Accordion title="Model Setup" icon="microchip" iconType="solid">
    Now we’re going to set the "brains" of the assistant, the large language model. We're going to be using `GPT-4` (from [OpenAI](https://openai.com/)) for this demo (though you're free to use `GPT-3.5`, or any one of your favorite LLMs).

    <AccordionGroup>
      <Accordion title="Set Your OpenAI Provider Key (optional)" icon="key" iconType="solid">
        Before we proceed, we can set our [provider key](customization/provider-keys) for OpenAI (this is just your OpenAI secret key).

        <Note>
          You can see all of your provider keys in the "Provider Keys" dashboard tab. You can also go
          directly to [dashboard.vapi.ai/keys](https://dashboard.vapi.ai/keys).
        </Note>

        Vapi uses [provider keys](customization/provider-keys) you provide to communicate with LLM, TTS, & STT vendors on your behalf. It is most ideal that we set keys for the vendors we intend to use ahead of time.

        <Frame caption="We set our provider key for OpenAI so Vapi can make requests to their API.">
          <img src="../static/images/quickstart/dashboard/model-provider-keys.png" />
        </Frame>

        While we're here it'd be ideal for you to go & set up provider keys for other providers you're familiar with & intend to use later.
      </Accordion>
      <Accordion title="Set a First Message" icon="message" iconType="light">
        Assistants can **optionally** be configured with a `First Message`. This first message will be spoken by your assistant when either:

        - **A Web Call Connects:** when a web call is started with your assistant
        - **An Inbound Call is Picked-up:** an [inbound call](/glossary#inbound-call) is picked-up & answered by your assistant
        - **An Outbound Call is Dialed & Picked-up:** an [outbound call](/glossary#outbound-call) is dialed by your assistant & a person picks up

        <Warning>
          Note that this first message cannot be interrupted & is guaranteed to be spoken. Certain use cases
          need a first message, while others do not.
        </Warning>

        For our use case, we will want a first message. Since we are calling the customer back it would be ideal for us to have a first message like this:

        ```text
        Hi this is Jennifer from Vappy’s Pizzeria giving you a call back since we got disconnected. Would you like to finish your order with us?
        ```

        <Info>
          Some text-to-speech voices may struggle to pronounce 'Vapi' correctly, compartmentalizing it to be
          spoken letter by letter "V. A. P. I."

        Some aspects of configuring your voice pipeline will require tweaks like this to get the target
        behaviour you want.

        </Info>

        This will be spoken by the assistant when a web or inbound phone call is received.
      </Accordion>
      <Accordion title="Set the System Prompt" icon="message" iconType="solid">
        We will now set the `System Prompt` for our assistant. If you're familiar with OpenAI's API, this is the first prompt in the message list that we feed our LLM (learn more about prompt engineering on the [OpenAI docs](https://platform.openai.com/docs/guides/prompt-engineering)).

        The system prompt can be used to configure the context, role, personality, instructions and so on for the assistant.

        Since we are calling the customer back, we will tweak the base prompt a bit so the model understands the situation & new goal (recovering the order).

        A system prompt like this will give us the behaviour we want:

        ```text
        You are a voice assistant for Vappy’s Pizzeria,
        a pizza shop located on the Internet.

        Your job is to take the order of customers calling in. The menu has only 3 types
        of items: pizza, sides, and drinks. There are no other types of items on the menu.

        1) There are 3 kinds of pizza: cheese pizza, pepperoni pizza, and vegetarian pizza
        (often called "veggie" pizza).
        2) There are 3 kinds of sides: french fries, garlic bread, and chicken wings.
        3) There are 2 kinds of drinks: soda, and water. (if a customer asks for a
        brand name like "coca cola", just let them know that we only offer "soda")

        Customers can only order 1 of each item. If a customer tries to order more
        than 1 item within each category, politely inform them that only 1 item per
        category may be ordered.

        Customers must order 1 item from at least 1 category to have a complete order.
        They can order just a pizza, or just a side, or just a drink.

        Be sure to introduce the menu items, don't assume that the caller knows what
        is on the menu (most appropriate at the start of the conversation).

        If the customer goes off-topic or off-track and talks about anything but the
        process of ordering, politely steer the conversation back to collecting their order.

        Once you have all the information you need pertaining to their order, you can
        end the conversation. You can say something like "Awesome, we'll have that ready
        for you in 10-20 minutes." to naturally let the customer know the order has been
        fully communicated.

        It is important that you collect the order in an efficient manner (succinct replies
        & direct questions). You only have 1 task here, and it is to collect the customers
        order, then end the conversation.

        - Be sure to be kind of funny and witty!
        - Keep all your responses short and simple. Use casual language, phrases like "Umm...", "Well...", and "I mean" are preferred.
        - This is a voice conversation, so keep your responses short, like in a real conversation. Don't ramble for too long.

        You are calling back a customer after the call got disconnected while they were
        ordering. Your job is to help them complete their order.
        ```

        You can copy & paste the above prompt into the `System Prompt` field. Now the model configuration for your assistant should look something like this:

        <Frame caption="Note how our model provider is set to OpenAI & the model is set to GPT-4.">
          <img src="../static/images/quickstart/phone/outbound/assistant-model-setup.png" />
        </Frame>
      </Accordion>
    </AccordionGroup>

  </Accordion>
  <Accordion title="Transcriber Setup" icon="microphone" iconType="solid">
    The transcriber is what turns user speech into processable text for our LLM. This is the first step in the end-to-end voice pipeline.

    <AccordionGroup>
      <Accordion title="Set Your Deepgram Provider Key (optional)" icon="key" iconType="solid">
        We will be using [Deepgram](https://deepgram.com) (which provides blazing-fast & accurate Speech-to-Text) as our STT provider.

        We will set our provider key for them in "Provider Keys":

        <Frame>
          <img src="../static/images/quickstart/dashboard/transcriber-providers-keys.png" />
        </Frame>
      </Accordion>
      <Accordion title="Set Transcriber" icon="language" iconType="solid">
        We will set the model to `Nova 2` & the language to `en` for English. Now your assistant's transcriber configuration should look something like this:

        <Frame caption="Note how our transcriber is set to 'deepgram', the model is set to 'Nova 2', & the language is set to English.">
          <img src="../static/images/quickstart/dashboard/assistant-transcriber-config.png" />
        </Frame>
      </Accordion>
    </AccordionGroup>

  </Accordion>
  <Accordion title="Voice Setup" icon="head-side-cough" iconType="solid">
    The final portion of the voice pipeline is turning LLM output-text into speech. This process is called "Text-to-speech" (or TTS for short).

    We will be using a voice provider called [PlayHT](https://play.ht) (they have very conversational voices), & a voice provided by them labeled `Jennifer` (`female`, `en-US`).

    You are free to use your favorite TTS voice platform here. [ElevenLabs](https://elevenlabs.io/) is
    another alternative — by now you should get the flow of plugging in vendors into Vapi (add
    provider key + pick provider in assistant config).

    You can skip the next step(s) if you don't intend to use PlayHT.

    <AccordionGroup>
      <Accordion title="Set Your PlayHT Provider Key (optional)" icon="key" iconType="solid">
        If you haven't already, sign up for an account with PlayHT at [play.ht](https://play.ht). Since their flows are liable to change — you can just grab your `API Key` & `User ID` from them.

        <Frame>
          <img src="../static/images/quickstart/dashboard/voice-provider-keys.png" />
        </Frame>
      </Accordion>
      <Accordion title="Set Voice" icon="person" iconType="solid">
        You will want to select `playht` in the "provider" field, & `Jennifer` in the "voice" field. We will leave all of the other settings untouched.

        <Frame caption="Each voice provider offers a host of settings you can modulate to customize voices. Here we will leave all the defaults alone.">
          <img src="../static/images/quickstart/dashboard/assistant-voice-config.png" />
        </Frame>
      </Accordion>
    </AccordionGroup>

  </Accordion>
</AccordionGroup>

## Get a Phone Number

Now that we've configured how our assistant will behave, we want to figure out how to dial calls with it. We will need a phone number that we can call from.

<Markdown src="../snippets/quickstart/phone/get-a-phone-number.mdx" />

## Call Your Number

We can now make outbound calls to phone numbers, setting our assistant as the one doing the dialing.

In the phone numbers section of the dashboard, go to your phone number detail page. We will:

1. fill out **our own phone number** as the number to dial
2. set our assistant as the one doing the calling

<Frame caption="When we hit the call button, our assistant will make the outbound call to the phone number.">
  <img src="../static/images/quickstart/phone/outbound/dial-outbound-call-dashboard.png" />
</Frame>

You can now hit the call button to make the outbound call. Your assistant will dial the phone number & manage the order recovery process.

<Tip>
  Your assistant won't yet be able to hang-up the phone at the end of the call.
  We will learn more about configuring call end behaviour in later guides.
</Tip>


 This is the content for the doc fern/quickstart/web.mdx 

 ---
title: Web Calling
subtitle: Get started with Vapi on the Web.
slug: quickstart/web
---

<Frame>
  <div class="video-embed-wrapper">
    <iframe
      src="https://www.youtube.com/embed/PUb-cseRNr4?si=NP-GO8tU46hibfuW"
      title='An embedded YouTube video titled "Quickstart: Web"'
      frameborder="0"
      allow="fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      referrerpolicy="strict-origin-when-cross-origin"
    />
  </div>
</Frame>

Anywhere you can run client-side JavaScript, you can run Vapi. All the way from vanilla to complex component-based applications with React and Next.js.

<CardGroup cols={2}>
  <Card title="The Web SDK" icon="window" iconType="duotone" href="/sdk/web">
    Explore the full Vapi Web SDK.
  </Card>
  <Card
    title="Live React Demo"
    icon="arrow-up-right-from-square"
        color="#f25130"
    href="https://stackblitz.com/~/github.com/VapiAI/quickstart-react"
  >
    Follow along as you read.
  </Card>
</CardGroup>

## Installation

<Markdown src="../snippets/sdks/web/install-web-sdk.mdx" />

<Markdown src="../snippets/sdks/web/import-web-sdk.mdx" />

## Starting a Call

Assistants can either be created on the fly (temporary) or created & persisted to your account (persistent).

### Option 1: Temporary Assistant

If you want to customize properties from the frontend on the fly, you can create an assistant configuration object and pass it to the `.start()` method.

<AccordionGroup>
  <Accordion title="Assistant Configuration" icon="brackets-curly" iconType="solid">
    Here are the options we will pass to `.start()`:

    ```javascript
      const assistantOptions = {
        name: "Vapi’s Pizza Front Desk",
        firstMessage: "Vappy’s Pizzeria speaking, how can I help you?",
        transcriber: {
          provider: "deepgram",
          model: "nova-2",
          language: "en-US",
        },
        voice: {
          provider: "playht",
          voiceId: "jennifer",
        },
        model: {
          provider: "openai",
          model: "gpt-4",
          messages: [
            {
              role: "system",
              content: `You are a voice assistant for Vappy’s Pizzeria, a pizza shop located on the Internet.

      Your job is to take the order of customers calling in. The menu has only 3 types
      of items: pizza, sides, and drinks. There are no other types of items on the menu.

      1) There are 3 kinds of pizza: cheese pizza, pepperoni pizza, and vegetarian pizza
      (often called "veggie" pizza).
      2) There are 3 kinds of sides: french fries, garlic bread, and chicken wings.
      3) There are 2 kinds of drinks: soda, and water. (if a customer asks for a
      brand name like "coca cola", just let them know that we only offer "soda")

      Customers can only order 1 of each item. If a customer tries to order more
      than 1 item within each category, politely inform them that only 1 item per
      category may be ordered.

      Customers must order 1 item from at least 1 category to have a complete order.
      They can order just a pizza, or just a side, or just a drink.

      Be sure to introduce the menu items, don't assume that the caller knows what
      is on the menu (most appropriate at the start of the conversation).

      If the customer goes off-topic or off-track and talks about anything but the
      process of ordering, politely steer the conversation back to collecting their order.

      Once you have all the information you need pertaining to their order, you can
      end the conversation. You can say something like "Awesome, we'll have that ready
      for you in 10-20 minutes." to naturally let the customer know the order has been
      fully communicated.

      It is important that you collect the order in an efficient manner (succinct replies
      & direct questions). You only have 1 task here, and it is to collect the customers
      order, then end the conversation.

      - Be sure to be kind of funny and witty!
      - Keep all your responses short and simple. Use casual language, phrases like "Umm...", "Well...", and "I mean" are preferred.
      - This is a voice conversation, so keep your responses short, like in a real conversation. Don't ramble for too long.`,
            },
          ],
        },
      };
    ```
    Let's break down the configuration options we passed:

    - **name:** the display name for the assistant in our dashboard (for internal purposes only)
    - **firstMessage:** the first message that our assistant will say when it picks up the web call
    - **transcriber:** the transcriber is what turns user speech into processable text for our LLM. This is the first step in the end-to-end voice pipeline. We are using Deepgram for transcription, specifically, their `Nova 2` model. We also set the language to be transcribed as English.
    - **voice:** the final portion of the voice pipeline is turning LLM output-text into speech. This process is called "Text-to-speech" (or TTS for short). We use a voice provider called PlayHT, & a voice provided by them called `jennifer`.
    - **model:** for our LLM, we use `gpt-4` (from OpenAI) & set our system prompt for the assistant. The system prompt configures the context, role, personality, instructions and so on for the assistant. In our case, the system prompt above will give us the behaviour we want.

   </Accordion>
</AccordionGroup>

Now we can call `.start()`, passing the temporary assistant configuration:

```javascript
vapi.start(assistantOptions);
```

More configuration options can be found in the [Assistant](/api-reference/assistants/create-assistant) API reference.

### Option 2: Persistent Assistant

If you want to create an assistant that you can reuse across multiple calls, you can create a persistent assistant in the [Vapi Dashboard](https://dashboard.vapi.ai). Here's how you can do that:

<Markdown src="../snippets/quickstart/dashboard/assistant-setup-inbound.mdx" />

To customize additional fields, this can be done via the [Assistant](/api-reference/assistants/create-assistant) API instead.

Then, you can copy the assistant's ID at the top of the assistant detail page:

<Frame>
  <img src="../static/images/quickstart/assistant-id-dashboard.png" />
</Frame>

Now we can call `.start()`, passing the persistent assistant's ID:

```javascript
vapi.start("79f3XXXX-XXXX-XXXX-XXXX-XXXXXXXXce48");
```

If you need to override any assistant settings or set template variables, you can pass `assistantOverrides` as the second argument.

For example, if the first message is "Hello `{{name}}`", you can set `assistantOverrides` to replace `{{name}}` with `John`:

```javascript
const assistantOverrides = {
  transcriber: {
    provider: "deepgram",
    model: "nova-2",
    language: "en-US",
  },
  recordingEnabled: false,
  variableValues: {
    name: "John",
  },
};

vapi.start("79f3XXXX-XXXX-XXXX-XXXX-XXXXXXXXce48", assistantOverrides);
```


 This is the content for the doc fern/resources.mdx 

 ---
title: Ecosystem
subtitle: Find all of our resources here.
slug: resources
---


{/* Use this LInk to modify the content -> https://onecompiler.com/ejs/425khha82 */}

<table>

<thead><tr><th colSpan="2">Vapi AI Ecosystem</th></tr></thead>
<tbody>
<tr><td>Real-time SDKs</td><td><a target="_blank" href="https://github.com/VapiAI/web">Web</a> · <a target="_blank" href="https://github.com/VapiAI/flutter">Flutter</a> · <a target="_blank" href="https://github.com/VapiAI/react-native-sdk">React Native</a> · <a target="_blank" href="https://github.com/VapiAI/ios">iOS</a> · <a target="_blank" href="https://github.com/VapiAI/python">Python</a> · <a target="_blank" href="https://github.com/VapiAI/html-script-tag">Vanilla</a></td></tr>
<tr><td>Client Examples</td><td><a target="_blank" href="https://github.com/VapiAI/client-side-example-javascript-next">Next.js</a> · <a target="_blank" href="https://github.com/VapiAI/client-side-example-javascript-react">React</a> · <a target="_blank" href="https://github.com/VapiAI/flutter/tree/main/example">Flutter</a> · <a target="_blank" href="https://github.com/VapiAI/client-side-example-react-native">React Native</a></td></tr>
<tr><td>Server Examples</td><td><a target="_blank" href="https://github.com/VapiAI/server-side-example-serverless-vercel">Vercel</a> · <a target="_blank" href="https://github.com/VapiAI/server-side-example-serverless-cloudflare">Cloudflare</a> · <a target="_blank" href="https://github.com/VapiAI/server-side-example-serverless-supabase">Supabase</a> · <a target="_blank" href="https://github.com/VapiAI/server-side-example-javascript-node">Node</a> · <a target="_blank" href="https://github.com/VapiAI/server-side-example-javascript-bun">Bun</a> · <a target="_blank" href="https://github.com/VapiAI/server-side-example-javascript-deno">Deno</a> · <a target="_blank" href="https://github.com/VapiAI/server-side-example-python-flask">Flask</a> · <a target="_blank" href="https://github.com/VapiAI/server-side-example-php-laravel">Laravel</a> · <a target="_blank" href="https://github.com/VapiAI/server-side-example-go-gin">Go</a> · <a target="_blank" href="https://github.com/VapiAI/server-side-example-rust-actix">Rust</a></td></tr>
<tr><td>Resources</td><td><a target="_blank" href="https://docs.vapi.ai/">Official Docs</a> · <a target="_blank" href="https://api.vapi.ai/api">API Reference</a> · <a href="/sdk/mcp-server">MCP Server</a> </td></tr>
<tr><td>Community</td><td><a target="_blank" href="/community/videos">Videos</a> . <a target="_blank" href="https://www.vapiblocks.com/">UI Library</a></td></tr>
</tbody>
</table>


 This is the content for the doc fern/rss-feed.mdx 

 ---
title: RSS Feed
subtitle: Stay updated with the latest incidents from Vapi or third party providers
slug: rss-feed
---

## RSS Feed

Vapi provides a RSS feed for the latest incidents. You can subscribe to the RSS feed to get the latest incidents from Vapi or underlying providers using the following URL.

```
https://status.vapi.ai/feed.rss
```


### Slack

You can subscribe to the RSS feed in any application that supports it. In this guide, we will show you how to use the Slack RSS app to subscribe to our RSS feed.

#### How to subscribe

1. **Install the Slack RSS App**
   - Ensure the [Slack RSS app](https://slack.com/marketplace/A0F81R7U7) is installed in your Slack workspace.

2. **Open the Desired Channel**
   - Go to the Slack channel where you wish to receive RSS updates.

3. **Subscribe to the RSS Feed**
   - Enter the following command in the message input box:
     ```
     /feed subscribe https://status.vapi.ai/feed.rss
     ```
   - Press Enter to subscribe.

4. **Confirm Subscription**
   - The Slack RSS app will confirm your subscription, and updates will start appearing in the channel.

You will now receive updates in your Slack channel whenever there is an incident.

#### How to unsubscribe

1. **Open the Desired Channel**
   - Go to the Slack channel from which you want to unsubscribe.

2. **List Subscribed Feeds**
   - Enter the command:
     ```
     /feed list
     ```
   - Press Enter to view all subscribed feeds.

3. **Identify the Feed ID**
   - Note the ID of the feed you wish to unsubscribe from.

4. **Remove the Feed**
   - Enter the command:
     ```
     /feed remove <feed_id>
     ```
   - Press Enter to remove the feed from the channel.

You will no longer receive updates in the specified Slack channel.

 This is the content for the doc fern/sdk/mcp-server.mdx 

 ---
title: Vapi MCP Server
subtitle: 'Integrate Vapi APIs with AI assistants through the Model Context Protocol (MCP)'
slug: sdk/mcp-server
---

The Vapi Model Context Protocol (MCP) server allows you to integrate with Vapi APIs through tool calling. This enables AI assistants like Claude to directly communicate with Vapi's services, making it possible to manage assistants, phone numbers, and create calls directly through conversational interfaces.

<Note>
  Looking to use MCP tools with your assistants? See the [MCP Tool documentation](/tools/mcp) for integrating external MCP servers with your Vapi assistants.
</Note>

## What is Vapi MCP Server?

Vapi MCP Server is an implementation of the Model Context Protocol that exposes Vapi's APIs as callable tools. This allows any MCP-compatible client (like Claude Desktop or custom applications using the MCP SDK) to access Vapi functionality, including:

- Listing, creating, and managing Vapi assistants
- Managing phone numbers
- Creating and scheduling outbound calls
- Retrieving call details and status

## Supported Actions

The Vapi MCP Server provides the following tools for integration:

### Assistant Tools
- `list_assistants`: Lists all Vapi assistants
- `create_assistant`: Creates a new Vapi assistant
- `get_assistant`: Gets a Vapi assistant by ID

### Call Tools
- `list_calls`: Lists all Vapi calls
- `create_call`: Creates an outbound call
- `get_call`: Gets details of a specific call

> **Note:** The `create_call` action supports scheduling calls for immediate execution or for a future time using the optional `scheduledAt` parameter.

### Phone Number Tools
- `list_phone_numbers`: Lists all Vapi phone numbers
- `get_phone_number`: Gets details of a specific phone number

## Setup Options

There are two primary ways to connect to the Vapi MCP Server:

1. **Local Setup**: Run the MCP server locally for development or testing
2. **Remote SSE Connection**: Connect to Vapi's hosted MCP server via Server-Sent Events (SSE)

## Claude Desktop Setup

The easiest way to get started with Vapi MCP Server is through Claude Desktop. This allows you to interact with Vapi services directly through conversations with Claude.

### Prerequisites
- Claude Desktop application installed
- Vapi API key (get it from the [Vapi dashboard](https://dashboard.vapi.ai/org/api-keys))

### Configuration Steps

1. Open Claude Desktop and press `CMD + ,` (Mac) to go to `Settings`
2. Click on the `Developer` tab
3. Click on the `Edit Config` button
4. This will open the `claude_desktop_config.json` file in your file explorer
5. Add the following configuration to the file:

```json
{
  "mcpServers": {
    "vapi-mcp-server": {
      "command": "npx",
      "args": [
          "-y",
          "@vapi-ai/mcp-server"
      ],
      "env": {
        "VAPI_TOKEN": "<your_vapi_token>"
      }
    }
  }
}
```

6. Replace `<your_vapi_token>` with your actual Vapi API key
7. Save the file and restart Claude Desktop

### Example Usage with Claude Desktop

After configuring Claude Desktop with the Vapi MCP server, you can ask Claude to help with Vapi-related tasks.

#### Example 1: Request an immediate call

```
I'd like to speak with my appointment scheduling assistant. Can you have it call me at +1234567890?
```

#### Example 2: Schedule a future call

```
I need to schedule a call with my customer service assistant for next Tuesday at 3:00 PM. My phone number is +1555123456.
```

#### Example 3: Manage assistants

```
Can you list all my Vapi assistants and help me create a new one for appointment scheduling?
```

## Remote SSE Connection

For production use or if you prefer not to run a local server, you can connect to Vapi's hosted MCP server via Server-Sent Events (SSE) Transport.

### Connection Details

- SSE Endpoint: `https://mcp.vapi.ai/sse`
- Authentication: Include your Vapi API key as a bearer token in the request headers
  - Example header: `Authorization: Bearer your_vapi_api_key_here`

This connection allows you to access Vapi's functionality remotely without running a local server.

## Custom MCP Client Integration

If you're building a custom application that needs to communicate with Vapi, you can use any MCP-compatible client SDK.

### Available SDKs

The Model Context Protocol supports clients in multiple languages:

- [TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)
- [Python SDK](https://github.com/modelcontextprotocol/python-sdk)
- [Java SDK](https://github.com/modelcontextprotocol/java-sdk)
- [Kotlin SDK](https://github.com/modelcontextprotocol/kotlin-sdk)
- [C# SDK](https://github.com/modelcontextprotocol/csharp-sdk)

### Integration Steps

1. Install the MCP client SDK for your language of choice
2. Configure the client to connect to the Vapi MCP Server (either locally or via SSE)
3. Query the server for available tools
4. Use the tools in your application logic

Here's an example using the Node.js SDK with SSE transport:

```javascript
#!/usr/bin/env node
import { Client } from '@modelcontextprotocol/sdk/client/index.js';
import { SSEClientTransport } from '@modelcontextprotocol/sdk/client/sse.js';
import dotenv from 'dotenv';

// Load environment variables from .env file
dotenv.config();

// Ensure API key is available
if (!process.env.VAPI_TOKEN) {
  console.error('Error: VAPI_TOKEN environment variable is required');
  process.exit(1);
}

async function main() {
  try {
    // Initialize MCP client
    const mcpClient = new Client({
      name: 'vapi-client-example',
      version: '1.0.0',
    });
    
    // Create SSE transport for connection to remote Vapi MCP server
    const serverUrl = 'https://mcp.vapi.ai/sse';
    const headers = {
      Authorization: `Bearer ${process.env.VAPI_TOKEN}`,
    };
    const options = {
      requestInit: { headers: headers },
      eventSourceInit: {
        fetch: (url, init) => {
          return fetch(url, {
            ...(init || {}),
            headers: {
              ...(init?.headers || {}),
              ...headers,
            },
          });
        },
      },
    };
    const transport = new SSEClientTransport(new URL(serverUrl), options);
    
    console.log('Connecting to Vapi MCP server via SSE...');
    await mcpClient.connect(transport);
    console.log('Connected successfully');

    // Helper function to parse tool responses
    function parseToolResponse(response) {
      if (!response?.content) return response;
      const textItem = response.content.find(item => item.type === 'text');
      if (textItem?.text) {
        try {
          return JSON.parse(textItem.text);
        } catch {
          return textItem.text;
        }
      }
      return response;
    }
    
    try {
      // List available tools
      const toolsResult = await mcpClient.listTools();
      console.log('Available tools:');
      toolsResult.tools.forEach((tool) => {
        console.log(`- ${tool.name}: ${tool.description}`);
      });
      
      // List assistants
      console.log('\nListing assistants...');
      const assistantsResponse = await mcpClient.callTool({
        name: 'list_assistants',
        arguments: {},
      });
      
      const assistants = parseToolResponse(assistantsResponse);
      if (!(Array.isArray(assistants) && assistants.length > 0)) {
        console.log('No assistants found. Please create an assistant in the Vapi dashboard first.');
        return;
      }
      
      console.log('Your assistants:');
      assistants.forEach((assistant) => {
        console.log(`- ${assistant.name} (${assistant.id})`);
      });
      
      // List phone numbers
      console.log('\nListing phone numbers...');
      const phoneNumbersResponse = await mcpClient.callTool({
        name: 'list_phone_numbers',
        arguments: {},
      });
      
      const phoneNumbers = parseToolResponse(phoneNumbersResponse);
      if (!(Array.isArray(phoneNumbers) && phoneNumbers.length > 0)) {
        console.log('No phone numbers found. Please add a phone number in the Vapi dashboard first.');
        return;
      }
      
      console.log('Your phone numbers:');
      phoneNumbers.forEach((phoneNumber) => {
        console.log(`- ${phoneNumber.phoneNumber} (${phoneNumber.id})`);
      });
      
      // Create a call using the first assistant and first phone number
      const phoneNumberId = phoneNumbers[0].id;
      const assistantId = assistants[0].id;
      
      console.log(`\nCreating a call using assistant (${assistantId}) and phone number (${phoneNumberId})...`);
      const createCallResponse = await mcpClient.callTool({
        name: 'create_call',
        arguments: {
          assistantId: assistantId,
          phoneNumberId: phoneNumberId,
          customer: {
            phoneNumber: "+1234567890"  // Replace with actual customer phone number
          }
          // Optional: schedule a call for the future
          // scheduledAt: "2025-04-15T15:30:00Z"
        },
      });
      
      const createdCall = parseToolResponse(createCallResponse);
      console.log('Call created:', JSON.stringify(createdCall, null, 2));
    } finally {
      console.log('\nDisconnecting from server...');
      await mcpClient.close();
      console.log('Disconnected');
    }
  } catch (error) {
    console.error('Error:', error);
    process.exit(1);
  }
}

main();
```

This code shows how to:
- Connect to the Vapi MCP Server using SSE transport
- List available tools
- List your existing assistants
- List your phone numbers
- Create an outbound call using your first assistant and phone number

You can run this code by saving it as a script and executing it with Node.js:

```bash
# Install required packages
npm install @modelcontextprotocol/sdk dotenv

# Create a .env file with your Vapi API token
echo "VAPI_TOKEN=your_vapi_api_token_here" > .env

# Run the script
node vapi-client.js
```

For more detailed examples and complete client implementations, refer to the [MCP Client Quickstart](https://modelcontextprotocol.io/quickstart/client).

## References

- [Vapi MCP Server Repository](https://github.com/VapiAI/mcp-server)
- [Model Context Protocol Documentation](https://modelcontextprotocol.io)
- [Vapi Dashboard](https://dashboard.vapi.ai)
- [Model Context Protocol Client Quickstart](https://modelcontextprotocol.io/quickstart/client)

<CardGroup cols={2}>
  <Card
    title="Need Help?"
    icon="question-circle"
    href="https://discord.gg/pUFNcf2WmH"
  >
    Join our Discord community for support with MCP integration
  </Card>
  <Card
    title="API Reference"
    icon="book"
    href="/api-reference/tools/create"
  >
    View the complete API documentation for tools
  </Card>
</CardGroup>


 This is the content for the doc fern/sdk/web.mdx 

 ---
title: Web SDK
subtitle: Integrate Vapi into your web application.
slug: sdk/web
---

The Vapi Web SDK provides web developers a simple API for interacting with the realtime call functionality of Vapi.

### Installation

<Markdown src="../snippets/sdks/web/install-web-sdk.mdx" />

### Importing

<Markdown src="../snippets/sdks/web/import-web-sdk.mdx" />

---

## Usage

### `.start()`

You can start a web call by calling the `.start()` function. The `start` function can either accept:

1. **a string**, representing an assistant ID
2. **an object**, representing a set of assistant configs (see [Create Assistant](/api-reference/assistants/create-assistant))

The `start` function returns a promise that resolves to a call object. For example:

```javascript
const call = await vapi.start(assistantId);
// { "id": "bd2184a1-bdea-4d4f-9503-b09ca8b185e6", "orgId": "6da6841c-0fca-4604-8941-3d5d65f43a17", "createdAt": "2024-11-13T19:20:24.606Z", "updatedAt": "2024-11-13T19:20:24.606Z", "type": "webCall", ... }
```

#### Passing an Assistant ID

If you already have an assistant that you created (either via [the Dashboard](/quickstart/dashboard) or [the API](/api-reference/assistants/create-assistant)), you can start the call with the assistant's ID:

```javascript
vapi.start("79f3XXXX-XXXX-XXXX-XXXX-XXXXXXXXce48");
```

#### Passing Assistant Configuration Inline

You can also specify configuration for your assistant inline.

This will not create a [persistent assistant](/assistants/persistent-assistants) that is saved to your account, rather it will create an ephemeral assistant only used for this call specifically.

You can pass the assistant's configuration in an object (see [Create Assistant](/api-reference/assistants/create-assistant) for a list of acceptable fields):

```javascript
vapi.start({
  transcriber: {
    provider: "deepgram",
    model: "nova-2",
    language: "en-US",
  },
  model: {
    provider: "openai",
    model: "gpt-3.5-turbo",
    messages: [
      {
        role: "system",
        content: "You are a helpful assistant.",
      },
    ],
  },
  voice: {
    provider: "playht",
    voiceId: "jennifer",
  },
  name: "My Inline Assistant",
  ...
});
```

#### Overriding Assistant Configurations

To override assistant settings or set template variables, you can pass `assistantOverrides` as the second argument.

For example, if the first message is "Hello `{{name}}`", set `assistantOverrides` to the following to replace `{{name}}` with `John`:

```javascript
const assistantOverrides = {
  transcriber: {
    provider: "deepgram",
    model: "nova-2",
    language: "en-US",
  },
  recordingEnabled: false,
  variableValues: {
    name: "Alice",
  },
};

vapi.start("79f3XXXX-XXXX-XXXX-XXXX-XXXXXXXXce48", assistantOverrides);
```

### `.send()`

During the call, you can send intermediate messages to the assistant (like [background messages](/assistants/background-messages)).

- `type` will always be `"add-message"`
- the `message` field will have 2 items, `role` and `content`.

```javascript
vapi.send({
  type: "add-message",
  message: {
    role: "system",
    content: "The user has pressed the button, say peanuts",
  },
});
```

<Info>
  Possible values for role are `system`, `user`, `assistant`, `tool` or
  `function`.
</Info>

### `.stop()`

You can stop the call session by calling the `stop` method:

```javascript
vapi.stop();
```

This will stop the recording and close the connection.

### `.isMuted()`

Check if the user's microphone is muted:

```javascript
vapi.isMuted();
```

### `.setMuted(muted: boolean)`

You can mute & unmute the user's microphone with `setMuted`:

```javascript
vapi.isMuted(); // false
vapi.setMuted(true);
vapi.isMuted(); // true
```

### `say(message: string, endCallAfterSpoken?: boolean)`

The `say` method can be used to invoke speech and gracefully terminate the call if needed

```javascript
vapi.say("Our time's up, goodbye!", true)
```

## Events

You can listen on the `vapi` instance for events. These events allow you to react to changes in the state of the call or user speech.

#### `speech-start`

Occurs when your AI assistant has started speaking.

```javascript
vapi.on("speech-start", () => {
  console.log("Assistant speech has started.");
});
```

#### `speech-end`

Occurs when your AI assistant has finished speaking.

```javascript
vapi.on("speech-end", () => {
  console.log("Assistant speech has ended.");
});
```

#### `call-start`

Occurs when the call has connected & begins.

```javascript
vapi.on("call-start", () => {
  console.log("Call has started.");
});
```

#### `call-end`

Occurs when the call has disconnected & ended.

```javascript
vapi.on("call-end", () => {
  console.log("Call has ended.");
});
```

#### `volume-level`

Realtime volume level updates for the assistant. A floating-point number between `0` & `1`.

```javascript
vapi.on("volume-level", (volume) => {
  console.log(`Assistant volume level: ${volume}`);
});
```

#### `message`

Various assistant messages can be sent back to the client during the call. These are the same messages that your [server](/server-url) would receive.

At [assistant creation time](/api-reference/assistants/create-assistant), you can specify on the `clientMessages` field the set of messages you'd like the assistant to send back to the client.

Those messages will come back via the `message` event:

```javascript
// Various assistant messages can come back (like function calls, transcripts, etc)
vapi.on("message", (message) => {
  console.log(message);
});
```

#### `error`

Handle errors that occur during the call.

```javascript
vapi.on("error", (e) => {
  console.error(e);
});
```

---

## Resources

<CardGroup cols={2}>
  <Card
    title="NPM"
    icon="fa-brands fa-npm"
    iconType="solid"
    href="https://www.npmjs.com/package/@vapi-ai/web"
  >
    View the package on NPM.
  </Card>
  <Card
    title="GitHub"
    icon="fa-brands fa-github"
    iconType="solid"
    href="https://github.com/VapiAI/web"
  >
    View the package on GitHub.
  </Card>
</CardGroup>
<CardGroup cols={1}>
  <Card
    title="Try Our Quickstart"
    icon="bolt"
    iconType="solid"
    href="/quickstart/web"
  >
    Get up and running quickly with the Web SDK.
  </Card>
</CardGroup>


 This is the content for the doc fern/sdks.mdx 

 ---
title: Client SDKs
subtitle: Put Vapi assistants on every platform.
slug: sdks
---

The Vapi Client SDKs automatically configure audio streaming to and from the client, and provide a simple interface for starting calls. The interface is equivalent across all the SDKs.

The SDKs are open source, and available on GitHub:

<CardGroup cols={3}>
  <Card title="Vapi Web" icon="window" iconType="duotone" href="/sdk/web">
    Add a Vapi assistant to your web application.
  </Card>
  <Card
    title="Vapi iOS"
    icon="mobile-notch"
    href="https://github.com/VapiAI/ios"
  >
    Add a Vapi assistant to your iOS app.
  </Card>
  <Card
    title="Vapi Flutter"
    icon="mobile-notch"
    href="https://github.com/VapiAI/flutter"
  >
    Add a Vapi assistant to your Flutter app.
  </Card>
  <Card
    title="Vapi React Native"
    icon="mobile-notch"
    href="https://github.com/VapiAI/react-native-sdk"
  >
    Add a Vapi assistant to your React Native app.
  </Card>
  <Card
    title="Vapi Python"
    icon="fa-brands fa-python"
    href="https://github.com/VapiAI/python"
  >
    Multi-platform. Mac, Windows, and Linux.
  </Card>
</CardGroup>

---

<Accordion title="Events" defaultOpen={true}>
  - `speech-start`, `speech-end`, and `volume-level` for creating animations. -
  `message` for receiving messages sent to the [Server URL](/server-url) locally
  on the client, so you can show live transcriptions and use function calls to
  perform actions on the client.
</Accordion>


 This is the content for the doc fern/security-and-privacy/GDPR.mdx 

 ---
title: "GDPR Compliance"
subtitle: Learn how Vapi ensures GDPR compliance for its voice assistant platform.
slug: security-and-privacy/GDPR
---

At Vapi, safeguarding your personal data is our top priority. In full alignment with the General Data Protection Regulation (GDPR), we maintain robust standards for data protection and privacy. This document provides an overview of our data processing practices, legal bases, data subject rights, and the security measures we employ—all designed to ensure that your data is managed with the utmost care.

## Data Processing & Legal Bases

Our operations involve the secure processing of various types of personal data to enhance and deliver the Vapi service. We process information such as email addresses, names, phone numbers, physical addresses, usage statistics, and location data. The legal grounds underpinning this processing are:

- **Consent:** Users voluntarily provide consent for non-essential data processing (e.g., location-based services and marketing communications). This consent can be withdrawn at any time.
- **Contractual Necessity:** We process the data essential for fulfilling the services offered through Vapi, as detailed in our terms of service.
- **Legitimate Interests:** Data is processed to improve service functionality, enhance security, and analyze usage patterns, provided that our legitimate interests do not override your rights.

## Data Subject Rights

Vapi ensures that every user benefits from the robust rights granted by the GDPR. These rights include:

- **Right to Access:** You can request and obtain a copy of your personal data.
- **Right to Rectification:** If your data is inaccurate or incomplete, you can request corrections.
- **Right to Erasure (Right to be Forgotten):** Under certain conditions, you can ask for your personal data to be deleted.
- **Right to Restrict Processing:** You have the option to limit how your data is processed.
- **Right to Data Portability:** You can obtain and transfer your data in a structured, commonly used format.
- **Right to Withdraw Consent:** If your data processing is based on consent, you can withdraw it at any time.

## Data Security Measures

We deploy a range of technical and organizational safeguards to protect your personal data from unauthorized access, alteration, disclosure, and destruction, including:

- **Encryption:** Data is encrypted in transit and at rest.
- **Secure Server Configurations:** Our infrastructure is optimized for enhanced security.
- **Access Controls:** Strict controls ensure that only authorized personnel access sensitive data.
- **Regular Assessments:** Security audits and penetration tests are routinely performed to identify and address vulnerabilities.

## Third-Party Data Processors

To provide a best-in-class experience, Vapi partners with several reputable third-party providers, all of which comply with our GDPR standards. These include:

- **Analytics Tools:**  
  - *Google Analytics*  
  - *Cloudflare Analytics*  
  - *Segment.io*  
  - *Mixpanel* (with opt-out options)  
  - *PostHog*  

- **CI/CD and Development Platforms:**  
  - *GitHub*

- **Payment Processors:**  
  - *Stripe*

Each of these providers is carefully selected and operates under strict data protection agreements to ensure that your data remains secure.

## Transborder Data Transfers

In cases where personal data is transferred outside the European Union (primarily to the United States), we ensure that all transfers are governed by legally approved safeguards such as standard contractual clauses. These measures guarantee that your data receives the same level of protection, regardless of where it is processed.

## Compliance Testing & Continuous Improvement

To reinforce our GDPR compliance, we conduct routine testing and audits including:

- **Penetration Testing:** Confirming there are no critical vulnerabilities.
- **Compliance Audits:** Verifying that our data processing practices adhere to GDPR standards.
- **Role-Based Access Control Tests:** Ensuring that access to personal data is strictly limited to authorized personnel.
- **Data Breach Simulations:** Evaluating the efficiency of our incident response plans.
- **User Consent Management Tests:** Checking the ease and accuracy of obtaining or withdrawing user consent.
- **Data Recovery and Deletion Tests:** Ensuring our backup systems and deletion protocols function as required.

These measures ensure that our data protection systems remain robust, up-to-date, and fully compliant with the ever-evolving data protection landscape.

## Conclusion

Vapi’s dedication to safeguarding your personal data is unwavering. Our comprehensive compliance framework not only meets but exceeds the minimum requirements of the GDPR, ensuring that your privacy and data security are always at the forefront of our operations.

For further details, please contact our Data Protection Officer or review our detailed [GDPR Report](https://security.vapi.ai/).



 This is the content for the doc fern/security-and-privacy/PCI.mdx 

 ---
title: PCI Compliance
subtitle: Ensure secure payment data handling while using Vapi’s voice assistant platform.
slug: security-and-privacy/pci
---


## Introduction to Security at Vapi

At Vapi, we prioritize the security of your data without compromising the quality of our voice assistant services. Protecting sensitive information, especially financial data, is at the core of our mission.

Our robust security policies and practices ensure you have complete control over your data while accessing all the capabilities of our platform.

## Understanding PCI Compliance

The Payment Card Industry Data Security Standard (PCI DSS) is a global framework designed to protect credit card information. Any organization processing, storing, or transmitting cardholder data must comply with PCI DSS to ensure that sensitive financial data is securely handled.
Key requirements for PCI compliance include:

- Securing data collection, transmission, and storage.
- Implementing strong access control measures.
- Regularly monitoring and testing systems to prevent breaches.

## PCI Compliance on Vapi’s Platform

By default, Vapi enables call recording, logging, and transcription features to enhance service quality. However, handling sensitive payment card data requires additional precautions.

### How We Ensure Security

When PCI compliance is enabled:

- **Cloud Storage and Webhooks**: You can choose to store recordings in a PCI DSS Level 1 compliant cloud storage solution (AWS S3, Azure Blob Storage, Google Cloud Storage or Cloudflare R2) and receive transcripts through your webhook.

- **No Retention Without Configuration**: If no cloud storage or webhook is specified, recordings and transcripts are permanently deleted to avoid retaining sensitive data.


## How to Enable PCI Compliance
If your organization handles payment data, you can enable PCI compliance by updating your assistant’s configuration.

#### Configuration Steps:
1. Log in to your Vapi account and navigate to your assistant’s settings.
2. Enable the PCI Compliance toggle.
3. Select the PCI-compliant Model, Voice, and Transcriber options for your assistant.
4. [Optional] Configure cloud storage credentials for storing call recordings. If you have any of the storage endpoint credentials, they will be used to push the recordings.
5. [Optional] Set up **webhooks** for receiving transcriptions.


<Warning>
If either cloud storage or webhook is not configured, the respective data will not be stored and cannot be retrieved.
</Warning>

Example configuration for `PCI compliant` assistant is:
```JSON
{
  "compliancePlan": {
    "pciEnabled": true
  }
}
```
Note: The default value for `compliancePlan.pciEnabled` is false. Activating this setting aligns your assistant with PCI DSS standards by ensuring data is securely transmitted without being stored on Vapi’s systems.

## Can PCI be used alongside HIPAA?
Yes, you can enable both HIPAA and PCI compliance for an assistant. In this case, the restrictions from both compliances will apply, meaning that no recordings or transcripts will be stored or transmitted, even if you have specified cloud storage endpoints or webhooks for storing transcripts.

## FAQs

**Q: Will enabling PCI compliance affect the quality of Vapi’s service?**

A: Enabling PCI compliance does not degrade the quality of the voice assistant services.
However, it restricts you to use only the PCI-compliant endpoints, while limiting access to certain features, such as reviewing call logs, recordings or transcriptions, within the Vapi platform.
If any cloud storage endpoints are provided, you can review the audio recordings in your own storage environment. The recordings follow the naming convention:

```
<call_UUID>-<timestamp>-<generated_UUID>-<audio_type>.wav
```

**Q: Who should use the PCI compliance feature?**

A: This feature is particularly useful for businesses and organizations that handle sensitive payment information and must comply with PCI regulations.

**Q: Can I switch between default and PCI-compliant settings?**

A: Yes, users can toggle the `pciEnabled` setting as needed. However, we recommend carefully considering the implications of each option on your data security and compliance requirements.

## Need Further Assistance?

If you have more questions about security, privacy, PCI compliance, or how to configure your Vapi assistant, our support team is here to help. Contact us at security@vapi.ai for personalized assistance and more information on how to make the most of Vapi’s voice assistant platform while ensuring your data remains protected.


 This is the content for the doc fern/security-and-privacy/hipaa.mdx 

 ---
title: HIPAA Compliance
subtitle: Learn how to ensure privacy when using Vapi's voice assistant platform.
slug: security-and-privacy/hipaa
---


## Introduction to Privacy at Vapi

At Vapi, we are committed to delivering exceptional voice assistant services while upholding the highest standards of privacy and data protection for our users. We understand the importance of balancing service quality with the need to respect and protect personal and sensitive information. Our privacy policies and practices are designed to give you control over your data while benefiting from the full capabilities of our platform.

## Understanding HIPAA Compliance Basics

The Health Insurance Portability and Accountability Act (HIPAA) is a United States legislation that provides data privacy and security provisions for safeguarding medical information. HIPAA compliance is crucial for any entity that deals with protected health information (PHI), ensuring that sensitive patient data is handled, stored, and transmitted with the highest standards of security and confidentiality. The key concepts of HIPAA compliance include the Privacy Rule, which protects the privacy of individually identifiable health information; the Security Rule, which sets standards for the security of electronic protected health information (e-PHI); and the Breach Notification Rule, which requires covered entities to notify individuals, HHS, and in some cases, the media of a breach of unsecured PHI. Compliance with these rules is not just about adhering to legal requirements but also about building trust with your customers by demonstrating your commitment to protecting their sensitive data. By enabling the `hipaaEnabled` configuration in Vapi's voice assistant platform, you are taking a significant step towards aligning your operations with these HIPAA principles, ensuring that your use of technology adheres to these critical privacy and security standards.

## Understanding Default Settings

By default, Vapi records your calls and stores logs and transcriptions. This practice is aimed at continuously improving the quality of our service, ensuring that you receive the best possible experience. However, we recognize the importance of privacy and provide options for users who prefer more control over their data.

## Opting for Privacy: The HIPAA Compliance Option

For users prioritizing privacy, particularly in compliance with the Health Insurance Portability and Accountability Act (HIPAA), Vapi offers the flexibility to opt out of our default data recording settings. Choosing HIPAA compliance through our platform ensures that you can still use our voice assistant services without compromising on privacy requirements.

## Enabling HIPAA Compliance

HIPAA compliance can be ensured by enabling the `hipaaEnabled` configuration in your assistant settings. This simple yet effective setting guarantees that no call logs, recordings, or transcriptions are stored during or after your calls. An end-of-call report message will be generated and stored on your server for record-keeping, ensuring compliance without storing sensitive data on Vapi's systems.

To enable HIPAA compliance, set hipaaEnabled to true within your assistant's configuration:

```JSON
{
  "hipaaEnabled": true
}
```

Note: The default value for hipaaEnabled is false. Activating this setting is a proactive measure to align with HIPAA standards, requiring manual configuration adjustment.

# FAQs

<AccordionGroup>
  <Accordion title="Will enabling HIPAA compliance affect the quality of Vapi's service?">
    Enabling HIPAA compliance does not degrade the quality of the voice assistant services. However, it limits access to certain features, such as reviewing call logs or transcriptions, that some users may find valuable for quality improvement purposes.
  </Accordion>
  <Accordion title="Who should use the HIPAA compliance feature?">
    This feature is particularly useful for businesses and organizations in the healthcare sector or any entity that handles sensitive health information and must comply with HIPAA regulations.
  </Accordion>
  <Accordion title="Can I switch between default and HIPAA-compliant settings?">
    Yes, users can toggle the `hipaaEnabled` setting as needed. However, we recommend carefully considering the implications of each option on your data privacy and compliance requirements.
  </Accordion>
</AccordionGroup>

## Where can PHI be used with Vapi?

<AccordionGroup>
  <Accordion title="Which endpoints can contain Protected Health Information (PHI)?">
    When using Vapi with PHI, you may only pass PHI through the `/call` endpoint. All other endpoints in the API Reference should not contain PHI. For example, you should not put PHI in an `/assistant` prompt or in a `/phone-number` label. The restriction applies to all configuration endpoints where data would be stored on Vapi's platform.
  </Accordion>
  <Accordion title="Are there specific HIPAA-safe endpoints I should use?">
    No, there are no designated "HIPAA-safe endpoints." Instead, when `hipaaEnabled` is turned on, Vapi will only use HIPAA-compliant services (such as Azure OpenAI) for processing PHI through the pipeline. The voice pipeline (STT → LLM → TTS) can process PHI when properly configured, but Vapi does not store this data.
  </Accordion>
</AccordionGroup>

## HIPAA Compliance Configuration

<AccordionGroup>
  <Accordion title="How do I enable HIPAA compliance with Vapi?">
    Enable `hipaaEnabled` at the organization level. This ensures that all appropriate compliance measures are in place across your Vapi implementation. You can also toggle HIPAA-compliance at the assistant-level by setting `Assistant.compliancePlan.hipaaEnabled=true` in your configuration.
  </Accordion>
  <Accordion title="If I bring my own HIPAA-compliant provider keys, does that make everything compliant?">
    No. Even when using your own HIPAA-compliant provider keys, it remains your responsibility not to store PHI via Vapi's endpoints. The model keys are a separate concern from the storage of PHI on Vapi's platform. You must both use HIPAA-compliant keys AND ensure you're not storing PHI on Vapi.
  </Accordion>
</AccordionGroup>

## Best Practices

<AccordionGroup>
  <Accordion title="What are best practices for ensuring HIPAA compliance with Vapi?">
    - Enable `hipaaEnabled` at the organization level
    - Ensure that PHI only passes through the call pipeline and is not stored in configuration
    - Use HIPAA-compliant enterprise accounts with all third-party providers (STT, LLM, TTS)
    - Be mindful of test/demo assistants where compliance might be turned off for testing purposes - never use these with real PHI
    - Remember that with HIPAA compliance enabled, Vapi won't store logs, recordings, or transcriptions
  </Accordion>
  <Accordion title="Can I have both HIPAA-compliant and non-HIPAA-compliant assistants?">
    Yes, but be extremely careful. If you have test or demo assistants where HIPAA compliance is turned off for testing purposes, ensure you never intermingle these with real PHI. It's safest to enable HIPAA compliance at the organization level to avoid accidental misconfigurations.
  </Accordion>
  <Accordion title="What is my responsibility under the BAA with Vapi?">
    Under the Business Associate Agreement (BAA), you agree:
    1. Not to introduce PHI onto Vapi's platform through its API or dashboard except as permitted
    2. To use HIPAA-compliant accounts with external providers when providing keys
    3. Not to use underlying providers through Vapi without having HIPAA-compliant enterprise accounts with those providers
    4. To use the platform in accordance with all BAA requirements
  </Accordion>
</AccordionGroup>

## Need Further Assistance?

If you have more questions about privacy, HIPAA compliance, or how to configure your Vapi assistant, our support team is here to help. Contact us at security@vapi.ai for personalized assistance and more information on how to make the most of Vapi's voice assistant platform while ensuring your data remains protected.


 This is the content for the doc fern/security-and-privacy/privacy-policy.mdx 

 ---
title: Privacy Policy
slug: security-and-privacy/privacy-policy
---


<Check>Our Privacy Policy is hosted at [https://vapi.ai/privacy](https://vapi.ai/privacy)</Check>


 This is the content for the doc fern/security-and-privacy/soc.mdx 

 ---
title: SOC-2 Compliance
slug: security-and-privacy/soc
---



 This is the content for the doc fern/security-and-privacy/tos.mdx 

 ---
title: Terms of Service
slug: security-and-privacy/tos
---


<Check>
  Our Terms of Service is hosted at
  [https://vapi.ai/terms-of-service](https://vapi.ai/terms-of-service)
</Check>


 This is the content for the doc fern/server-sdks.mdx 

 ---
title: Server SDKs
subtitle: Put Vapi assistants on every platform.
slug: server-sdks
---

Vapi provides server-side SDKs to help developers quickly integrate and manage voice AI capabilities into their applications. Our SDKs allow seamless interaction with Vapi's API across a wide range of programming languages, ensuring you can choose the stack that best suits your needs.

The SDKs are open source, and available on GitHub:

<CardGroup cols={3}>
  <Card title="Vapi Python" icon="fa-brands fa-python" href="https://github.com/VapiAI/server-sdk-python">
    Add a Vapi assistant to your Python application.
  </Card>
  <Card title="Vapi TypeScript" icon="fa-brands fa-js" href="https://github.com/VapiAI/server-sdk-typescript">
    Add a Vapi assistant to your TypeScript application.
  </Card>
  <Card title="Vapi Java" icon="fa-brands fa-java" href="https://github.com/VapiAI/server-sdk-java">
    Add a Vapi assistant to your Java application.
  </Card>
  <Card title="Vapi Ruby" icon="gem" href="https://github.com/VapiAI/server-sdk-ruby">
    Add a Vapi assistant to your Ruby application.
  </Card>
  <Card title="Vapi C#" icon="hexagon" href="https://github.com/VapiAI/server-sdk-csharp">
    Add a Vapi assistant to your C#/.NET application.
  </Card>
  <Card title="Vapi Go" icon="fa-brands fa-golang" href="https://github.com/VapiAI/server-sdk-go">
    Add a Vapi assistant to your Go application.
  </Card>
</CardGroup>

 This is the content for the doc fern/server-url.mdx 

 ---
title: Server URLs
subtitle: Learn how to set up your server to receive and respond to messages from Vapi.
slug: server-url
---


<Frame caption="Server URLs give Vapi a location to send real-time conversation data (as well as query for data Vapi needs).">
  <img src="./static/images/server-url/overview-graphic.png" />
</Frame>

Server URLs allow your application to **receive data** & **communicate with Vapi** during conversations. Conversation events can include:

- **Status Updates:** updates on the status of a call
- **Transcript Updates**: call transcripts
- **Function Calls:** payloads delivered when your assistant wants certain actions executed
- **Assistant Requests:** in certain circumstances, Vapi may ping your server to get dynamic configuration for an assistant handling a specific call
- **End of Call Report:** call summary data at the end of a call
- **Hang Notifications:** get notified when your assistant fails to reply for a certain amount of time

In our [quickstart guides](/quickstart) we learned how to setup a basic back-and-forth conversation with a Vapi assistant.

To build more complex & custom applications, we're going to need to get real-time conversation data to our backend. **This is where server URLs come in.**

<Info>
  If you're familiar with functional programming, Server URLs are like callback functions. But
  instead of specifying a function to get data back on, we specify a URL to a server (to POST data
  back to).
</Info>

## Get Started

To get started using server URLs, read our guides:

<CardGroup cols={2}>
  <Card
    title="Setting Server URLs"
    icon="link"
    iconType="duotone"
    href="/server-url/setting-server-urls"
  >
    Server URLs can be set in multiple places. Learn where here.
  </Card>
  <Card title="Events" icon="bell-on" iconType="solid" href="/server-url/events">
    Read about the different types of events Vapi can send to your server.
  </Card>
  <Card
    title="Developing Locally"
    icon="laptop-arrow-down"
    iconType="solid"
    href="/server-url/developing-locally"
  >
    Learn about receiving server events in your local development environment.
  </Card>
</CardGroup>

## FAQ

<AccordionGroup>
  <Accordion title="Where can the server be located?">
    The server URL can be any publicly accessible URL pointing to an HTTP endpoint. This can be a:
    - **Cloud Server:** your application might be deployed on a cloud platform like [Railway](https://railway.app), [AWS](https://aws.com), [GCP](https://cloud.google.com/gcp), etc — as a persistent web server.
    - **Serverless Function:** services like [Vercel](https://vercel.com/docs/functions), [AWS Lambda](https://aws.amazon.com/lambda/), [Google Cloud Functions](https://cloud.google.com/functions), [Cloudflare](https://developers.cloudflare.com/workers/), etc — allow you to host on-demand cloud functions.
    - **Workflow Orchestrator:** platforms like [Pipedream](https://pipedream.com) & [Make](https://www.make.com) allow you to program workflows (often without code) that can receive events via HTTP triggers.

    The main idea is that Vapi needs a location on the Internet that it can drop data to & converse with your application.

  </Accordion>
  <Accordion title="Why not just call them webhooks?">
    [Webhooks](/glossary#webhook) are traditionally unidirectional & stateless, with the target endpoint usually only replying with a status code to acknowledge message reception. Certain server URL events (like assistant requests) may require a meaningful reply from your server.

    "Server URL" is a more general term that encompasses both webhooks & bidirectional communication.

  </Accordion>
</AccordionGroup>


 This is the content for the doc fern/server-url/developing-locally.mdx 

 ---
title: Developing Locally
subtitle: Learn how to receive server events in your local development environment.
slug: server-url/developing-locally
---


<Frame caption="Routing server URL payloads to a public reverse proxy, which tunnels to our local development server.">
  <img src="../static/images/server-url/developing-locally/reverse-proxy-developing-locally.png" />
</Frame>

## The Problem

When Vapi dispatches events to a server, it must be able to reach the server via the open Internet.

If your API is already live in production, it will be accessible via a publicly known URL. But, during development, your server will often be running locally on your machine.

<Info>
  `localhost` is an alias for the IP address `127.0.0.1`. This address is called the "loopback"
  address and forwards the network request within the machine itself.
</Info>

To receive server events locally, we will need a public address on the Internet that can receive traffic and forward it to our local machine.

## Tunneling Traffic

We will be using a service called [ngrok](https://ngrok.com/) to create a secure tunnel to our local machine. The flow will look like the following:

<Steps>
  <Step title="Start Our API Locally">
    We will start our server locally so it is listening for http traffic. We will take note of the port our server is running on.
  </Step>
  <Step title="Start Ngrok Agent">
    We will use the `ngrok` command to start the [ngrok agent](https://ngrok.com/docs/agent) on our
    machine. This will establish a connection from your local machine to ngrok's servers.
  </Step>
  <Step title="Copy Ngrok Forwarding URL">
    Ngrok will give us a public forwarding URL that can receive traffic. We will use this as a server URL
    during development.
  </Step>
  <Step title="Trigger Call Events">
    We will conduct normal calls on Vapi to trigger events. These events will go to the Ngrok URL & get tunnelled to our local machine.

    We will see the event payloads come through locally & log them in our terminal.

  </Step>
</Steps>

#### Starting Our API Locally

First, ensure that your API is running locally. This could be a Node.js server, a Python server, or any other server that can receive HTTP requests.

Take note of the port that your server is running on. For example, if your server is running on port `8080`, you should be able to access it at `http://localhost:8080` in your browser.

#### Starting Ngrok Agent

Next we will install & run Ngrok agent to establish the forwarding pathway for Internet traffic:

<Steps>
  <Step title="Install Ngrok Agent CLI">
    Install the Ngrok agent by following Ngrok's [quickstart
    guide](https://ngrok.com/docs/getting-started). Once complete, we will have the `ngrok` command
    available in our terminal.
  </Step>
  <Step title="Start Ngrok Agent">
    Run the command `ngrok http 8080`, this will create the tunnel with Ngrok's servers.
    <Note>Replace `8080` with the port your server is running on.</Note>
  </Step>
</Steps>

#### Copy Ngrok Forwarding URL

You will see an output from the Ngrok Agent CLI that looks like the following:

<Frame caption="Terminal after running the 'ngrok' command forwarding to localhost:8080 — the 'Forwarding' URL is what we want.">
  <img src="../static/images/server-url/developing-locally/ngrok-cli-ui.png" />
</Frame>

Copy this public URL that Ngrok provides. This URL will be accessible from the open Internet and will forward traffic to your local machine.

You can now use this as a server URL in the various places you can [set server URLs](/server-url/setting-server-urls) in Vapi.

<Note>
  This URL will change every time that you run the `ngrok` command. If you'd like this URL to be the
  same every Ngrok session, look into [static domains on
  Ngrok](https://ngrok.com/docs/getting-started#step-4-always-use-the-same-domain).
</Note>

#### Trigger Call Events

We will now be able to see call events come through as `POST` requests, & can log payloads to our terminal.

<Frame caption="Logging call events routed to our local environment.">
  <img src="../static/images/server-url/developing-locally/logging-events-locally.png" />
</Frame>

Feel free to follow any of our [quickstart](/quickstart) guides to get started with building assistants & conducting calls.

## Troubleshooting

Here's a list of a few things to recheck if you face issues seeing payloads:

- Ensure that your local server is running on the port you expect
- Ensure that you input the correct port to the `ngrok http {your_port}` command
- Ensure your route handling server URL events is a `POST` endpoint
- Ensure your path on top of the base forwarding url is set correctly (ex: `/callbacks/vapi`)


 This is the content for the doc fern/server-url/events.mdx 

 ---
title: Server Events
subtitle: Learn about different events that can be sent to a Server URL.
slug: server-url/events
---


All messages sent to your Server URL will be `POST` requests with the following body:

```json
{
  "message": {
    "type": "function-call",
    "call": { Call Object },
    ...other message properties
  }
}
```

They include the type of message, the call object, and any other properties that are relevant to the message type. Below are the different types of messages that can be sent to your Server URL.

### Function Calling

<Info>
  Vapi fully supports [OpenAI's function calling
  API](https://platform.openai.com/docs/guides/gpt/function-calling), so you can have assistants
  ping your server to perform actions like sending emails, retrieve information, and more.
</Info>

With each response, the assistant will automatically determine what functions to call based on the directions provided in the system message in `messages`. Here's an example of what the assistant might look like:

```json
{
  "name": "Ryan's Assistant",
  "model": {
    "provider": "openai",
    "model": "gpt-3.5-turbo",
    "functions": [
      {
        "name": "sendEmail",
        "description": "Used to send an email to a client.",
        "parameters": {
          "type": "object",
          "properties": {
            "color": { "type": "string" }
          }
        }
      }
    ]
  }
}
```

Once a function is triggered, the assistant will send a message to your Server URL with the following body:

```json
{
  "message": {
    "type": "function-call",
    "call": { Call Object },
    "functionCall": {
      "name": "sendEmail",
      "parameters": "{ \"emailAddress\": \"john@example.com\"}"
    }
  }
}
```

Your server should respond with a JSON object containing the function's response, like so:

```json
{ "result": "Your email has been sent." }
```

Or if it's an object:

```json
{
  "result": "{ \"message\": \"Your email has been sent.\", \"email\": \"test@email.com\" }"
}
```

The result will be appended to the conversation, and the assistant will decide what to do with the response based on its system prompt.

<Note>
  If you don't need to return a response, you can use the `async: true` parameter in your assitant's
  function configuration. This will prevent the assistant from waiting for a response from your
  server.
</Note>

### Retrieving Assistants

For inbound phone calls, you may want to specify the assistant based on the caller's phone number. If a PhoneNumber doesn't have an `assistantId`, Vapi will attempt to retrieve the assistant from your server.

```json
{
  "message": {
    "type": "assistant-request",
    "call": { Call Object },
  }
}
```

If you want to use an existing saved assistant instead of creating a transient assistant for each request, you can respond with the assistant's ID:

```json
{
  "assistantId": "your-saved-assistant-id"
}
```

Alternatively, if you prefer to define a transient assistant dynamically, your server should respond with the [assistant](/api-reference/webhooks/server-message#response.body.messageResponse.Server%20Message%20Response%20Assistant%20Request.assistant) object directly:

```json
{
  "assistant": {
    "firstMessage": "Hey Ryan, how are you?",
    "model": {
      "provider": "openai",
      "model": "gpt-3.5-turbo",
      "messages": [
        {
          "role": "system",
          "content": "You're Ryan's assistant..."
        }
      ]
    }
  }
}
```



If you'd like to play an error message instead, you can respond with:

```json
{ "error": "Sorry, not enough credits on your account, please refill." }
```

### Call Status Updates

During the call, the assistant will make multiple `POST` requests to the Server URL with the following body:

```json
{
  "message": {
    "type": "status-update",
    "call": { Call Object },
    "status": "ended",
  }
}
```

<Card title="Status Events">
  - `in-progress`: The call has started. - `forwarding`: The call is about to be forwarded to
  `forwardingPhoneNumber`. - `ended`: The call has ended.
</Card>

### End of Call Report

When a call ends, the assistant will make a `POST` request to the Server URL with the following body:

```json
{
  "message": {
    "type": "end-of-call-report",
    "endedReason": "hangup",
    "call": { Call Object },
    "recordingUrl": "https://vapi-public.s3.amazonaws.com/recordings/1234.wav",
    "summary": "The user picked up the phone then asked about the weather...",
    "transcript": "AI: How can I help? User: What's the weather? ...",
    "messages":[
      {
        "role": "assistant",
        "message": "How can I help?",
      },
      {
        "role": "user",
        "message": "What's the weather?"
      },
      ...
    ]
  }
}
```

`endedReason` can be any of the options defined on the [Call Object](/api-reference/calls/get-call).

### Hang Notifications

Whenever the assistant fails to respond for 5+ seconds, the assistant will make a `POST` requests to the Server URL with the following body:

```json
{
  "message": {
    "type": "hang",
    "call": { Call Object },
  }
}
```

You can use this to display an error message to the user, or to send a notification to your team.


 This is the content for the doc fern/server-url/server-authentication.mdx 

 ---
title: Server Authentication
slug: server-url/server-authentication
---

# Server Authentication

When configuring webhooks for your assistant, you can authenticate your server endpoints using either a secret token, custom headers, or OAuth2. This ensures that only authorized requests from Vapi are processed by your server.

## Credential Configuration

Credentials can be configured at multiple levels:

1. **Tool Call Level**: Create individual credentials for each tool call
2. **Assistant Level**: Set credentials directly in the assistant configuration
3. **Phone Number Level**: Configure credentials for specific phone numbers
4. **Organization Level**: Manage credentials in the [API Keys page](https://dashboard.vapi.ai/keys)

The order of precedence is:
1. Tool call-level credentials
2. Assistant-level credentials
3. Phone number-level credentials
4. Organization-level credentials from the API Keys page

## Authentication Methods

### Secret Token Authentication

The simplest way to authenticate webhook requests is using a secret token. Vapi will include this token in the `X-Vapi-Signature` header of each request.

#### Configuration

```json
{
  "server": {
    "url": "https://your-server.com/webhook",
    "secret": "your-secret-token"
  }
}
```

### Custom Headers Authentication

For more complex authentication scenarios, you can configure custom headers that Vapi will include with each webhook request.

This could include short lived JWTs/API Keys passed along via the Authorization header, or any other header that your server checks for.

#### Configuration

```json
{
  "server": {
    "url": "https://your-server.com/webhook",
    "headers": {
      "Authorization": "Bearer your-api-key",
      "Custom-Header": "custom-value"
    }
  }
}
```

### OAuth2 Authentication

For OAuth2-protected webhook endpoints, you can configure OAuth2 credentials that Vapi will use to obtain and refresh access tokens.

#### Configuration

```json
{
  "server": {
    "url": "https://your-server.com/webhook"
  },
  "credentials": {
    "webhook": {
      "type": "oauth2",
      "clientId": "your-client-id",
      "clientSecret": "your-client-secret",
      "tokenUrl": "https://your-server.com/oauth/token",
      "scope": "optional, only needed to specify which scopes to request access for"
    }
  }
}
```

#### OAuth2 Flow

1. Vapi makes a request to your token endpoint with client credentials
2. Your server validates the credentials and returns an access token
3. Vapi includes the access token in the Authorization header for webhook requests
4. Your server validates the access token before processing the webhook
5. When the token expires, Vapi automatically requests a new one

#### OAuth2 Token Response Format

Your server should return a JSON response with the following format:

```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "token_type": "Bearer",
  "expires_in": 3600,
  "refresh_token": "tGzv3JOkF0XG5Qx2TlKWIA",  // Optional
  "scope": "read write"  // Optional, only if scope was requested
}
```

Example error response:

```json
{
  "error": "invalid_client",
  "error_description": "Invalid client credentials"
}
```

Common error types:
- `invalid_client`: Invalid client credentials
- `invalid_grant`: Invalid or expired refresh token
- `invalid_scope`: Invalid scope requested
- `unauthorized_client`: Client not authorized for this grant type


 This is the content for the doc fern/server-url/setting-server-urls.mdx 

 ---
title: Setting Server URLs
subtitle: Learn about where you can set server URLs to handle call events.
slug: server-url/setting-server-urls
---


<Frame caption="Server URLs can be set at multiple levels in Vapi.">
  <img src="../static/images/server-url/settings-server-urls/server-url-priority.png" />
</Frame>

Server URLs can be set in multiple places in Vapi. Each level has a different priority.

The server URL with the highest priority for a relevant event will be the one that Vapi uses to send the event to.

Server URLs can be set at **4 levels** in Vapi:

- **Account-wide:** you can set a server URL for your broader account
- **Phone Number:** server URLs can be attached to phone numbers themselves
- **Assistant:** assistants can be configured with a server URL
- **Function:** function calls themselves (under an assistant) can have a corresponding server URL

## Setting Server URLs

Here's a breakdown of where you can set server URLs in Vapi:

<AccordionGroup>
  <Accordion title="Organization" icon="table-columns" iconType="solid">
    You can set an organization-wide server URL in the [organization section](https://dashboard.vapi.ai/vapi-api) of your dashboard.

    <Frame caption="Setting your organization-wide server URL.">
      <img src="../static/images/server-url/settings-server-urls/org-settings-server-urls.png" />
    </Frame>

    If no other server URL is set, Vapi will use this one.

  </Accordion>
  <Accordion title="Phone Number" icon="phone-volume" iconType="solid">
    Phone numbers can have a server URL attached to them via the [phone number API](/api-reference/phone-numbers).

    The server URL for phone numbers can be set **3 ways**:
    - **At Time of Creation:** when you [create a free number](/api-reference/phone-numbers/create) through Vapi
    - **At Import:** when you [import from Twilio](/api-reference/phone-numbers/import-twilio-number) or [Vonage](/api-reference/phone-numbers/import-vonage-number)
    - **Via Update:** you can [update a number](/api-reference/phone-numbers/update-phone-number) already in your account

    The field `phoneNumber.serverUrl` will contain the server URL for the phone number.

  </Accordion>
  <Accordion title="Assistant" icon="robot" iconType="solid">
    Assistants themselves can have a server URL attached to them.
    
    There are **2 ways** this can be done:

    <AccordionGroup>
      <Accordion title="In the Dashboard" icon="browsers" iconType="solid">
        If you go to the [assistant section](https://dashboard.vapi.ai/assistants) of your dashboard, in the **"Advanced"** tab you will see a setting to set the assistant's server URL:

        <Frame caption="Setting server URL at the assistant level.">
          <img src="../static/images/server-url/settings-server-urls/assistant-server-url-dashboard.png" />
        </Frame>
      </Accordion>
      <Accordion title="Via the API" icon="code" iconType="solid">
        At [assistant creation](/api-reference/assistants/create-assistant) (or via an [update](/api-reference/assistants/update-assistant)) you can set the assistant's server URL.

        The server URL for an assistant is stored in the `assistant.serverUrl` field.
      </Accordion>
    </AccordionGroup>

  </Accordion>
  <Accordion title="Function Call" icon="function" iconType="solid">
    The most granular level server URLs can be set is at the function call level. This can also be done either in the dashboard, or via code.

    <AccordionGroup>
      <Accordion title="In the Dashboard" icon="browsers" iconType="solid">
        In the [assistant section](https://dashboard.vapi.ai/assistants) of your dashboard, in the **"Functions"** tab you can add function calls & optionally give each a specific server URL:

        <Frame caption="Setting server URL at the function call level.">
          <img src="../static/images/server-url/settings-server-urls/function-call-server-url-dashboard.png" />
        </Frame>
      </Accordion>
      <Accordion title="Via the API" icon="code" iconType="solid">
        The server URL for a function call can be found on an assistant at `assistant.model.functions[].serverUrl`.

        You can either set the URL for a function call at [assistant creation](/api-reference/assistants/create-assistant), or in an [assistant update](/api-reference/assistants/update-assistant).
      </Accordion>
    </AccordionGroup>

  </Accordion>
</AccordionGroup>

## URL Priority

Events are only sent/assigned to 1 server URL in the priority stack. Here's the order of priority:

1. **Function:** if a function call has a server URL, the function call event will be sent to that URL
2. **Assistant:** assistant server URLs are the next highest priority
3. **Phone Number:** if a phone number has a server URL, it will be used over the account-wide URL
4. **Account-wide:** Default / "lowest" importance. It will be used if no other server URL is set.

You will most commonly set a server URL on your account, and/or on specific assistants.


 This is the content for the doc fern/snippets/faq-snippet.mdx 

 <AccordionGroup>
  <Accordion title="Is Vapi right for my usecase?" icon="hammer" iconType="regular" defaultOpen={true}>

If you are **a developer building a voice AI application simulating human conversation** (w/ LLMs — to whatever degree of application complexity) — Vapi is built for you.

Whether you are building for a completely "turn-based" use case (like appointment setting), all the way to robust agentic voice applications (like virtual assistants), Vapi is tooled to solve for your voice AI workflow.

Vapi runs on any platform: the web, mobile, or even embedded systems (given network access).

  </Accordion>
  <Accordion title="Sounds good, but I’m building a custom X for Y..." icon="face-monocle" iconType="solid" defaultOpen={false}>

Not a problem, we can likely already support it. Vapi is designed to be modular at every level of the voice pipeline: Text-to-speech, LLM, Speech-to-text.

You can bring your own custom models for any part of the pipeline.

- **If they’re hosted with one of our providers:** you just need to add your [provider keys](customization/provider-keys), then specify the custom model in your API requests.
- **If they are hosted elsewhere:** you can use the `Custom LLM` provider and specify the [URL to your model](customization/custom-llm/fine-tuned-openai-models) in your API request.

Everything is interchangeable, mix & match to suit your usecase.

  </Accordion>
  <Accordion title="Couldn’t I build this myself and save money?" icon="piggy-bank" iconType="solid" defaultOpen={false}>

You could (and the person writing this right now did, from scratch) — but there are good reasons for not doing so.

Writing a great realtime voice AI application from scratch is a fairly challenging task (more on those challenges [here](/challenges-of-realtime-conversation)). Most of these challenges are not apparent until you face them, then you realize you are 3 weeks into a rabbit hole that may take months to properly solve out of.

Think of Vapi as hiring a software engineering team for this hard problem, while you focus on what uniquely generates value for your voice AI application.

---

But to address cost, the vast majority of cost in running your application will come from provider cost (Speect-to-text, LLM, Text-to-speech) direct with vendors (Deepgram, OpenAI, ElevenLabs, etc) — where we add no fee (vendor cost passes-through). These would have to be incurred anyway.

Vapi only charges its small fee on top of these for the continuous maintenance & improvement of these hardest components of your system (which would have costed you time to write/maintain).

No matter what, some cost is inescapable (in money, time, etc) to solve this challenging technical problem.

Our focus is solely on foundational Voice AI orchestration, & it’s what we put our full time and resources into.

To learn more about Vapi’s pricing, you can visit our [pricing page](/pricing).

  </Accordion>
  <Accordion title="Is it going to be hard to set up?" icon="gear" iconType="solid" defaultOpen={false}>

    No — in fact, the setup could not be easier:
    - **Web Dashboard:** It can take minutes to get up & running with our [dashboard](https://dashboard.vapi.ai/).
    - **Client SDKs:** You can start calls with 1 line of code with any of our [client SDKs](/sdks).

    For more advanced features like function calling, you will have to set up a [Server URL](/server-url) to receive and respond to messages.

  </Accordion>
  <Accordion title="How is Vapi different from other Voice AI services?" icon="bowling-pins" iconType="solid" defaultOpen={false}>

    Vapi focuses on developers. Giving developers modular, simple, & robust tooling to build any voice AI application imaginable.

    Vapi also has some of the lowest latency & (equally important) highest reliability amongst any other voice AI platform built for developers.

  </Accordion>
  <Accordion title="Can we achieve latency of around 800 milliseconds?" icon="bolt" iconType="solid" defaultOpen={false}>

Yes, Vapi is designed to achieve low latency, typically around 800 milliseconds for end-to-end voice processing. Our infrastructure is optimized for real-time communication, and we continuously work to minimize latency through various optimizations in our pipeline.

  </Accordion>
  <Accordion title="Do you offer competitive per-minute pricing if we commit to a consistent monthly volume?" icon="chart-line" iconType="solid" defaultOpen={false}>

Yes, we offer volume-based pricing discounts for customers with consistent monthly usage. The more minutes you commit to, the better the per-minute rate. We're happy to discuss custom pricing plans based on your specific volume requirements and use case.

  </Accordion>
  <Accordion title="Can your platform handle high levels of concurrency, ideally between 100 to 500 sessions daily?" icon="server" iconType="solid" defaultOpen={false}>

Absolutely. Our platform is built to handle high concurrency, and 1000+ concurrent sessions is well within our capacity. We've designed our infrastructure to scale horizontally, ensuring reliable performance even during peak usage periods. For enterprise customers with specific scaling needs, we can discuss custom solutions.

  </Accordion>
  <Accordion title="Are you compliant with HIPAA, SOC 2, and GDPR?" icon="shield-check" iconType="solid" defaultOpen={false}>

Yes, we take compliance seriously. Vapi is:
- HIPAA compliant for healthcare applications
- SOC 2 Type II certified
- GDPR compliant for handling EU data
- Regularly audited to maintain these certifications

For detailed compliance documentation and reports, please visit our [security portal](https://security.vapi.ai/).

  </Accordion>
  <Accordion title="How do you handle PII and PHI securely?" icon="lock" iconType="solid" defaultOpen={false}>

We implement multiple layers of security for PII and PHI:
- End-to-end encryption for all data in transit
- Secure storage with encryption at rest
- Strict access controls and audit logging
- Regular security assessments and penetration testing
- Data minimization practices
- Secure data deletion protocols

All data handling practices are documented in our security policies and compliance frameworks.

  </Accordion>
  <Accordion title="Do you support white-labeling and on-premise deployments?" icon="building" iconType="solid" defaultOpen={false}>

Yes, we offer both white-labeling and on-premise deployment options:

- **White-labeling:** Custom branding, domain, and UI customization
- **On-premise:** Full deployment within your infrastructure

These options are available for enterprise customers. Please contact our sales team to discuss your specific requirements.

  </Accordion>
</AccordionGroup>


 This is the content for the doc fern/snippets/quickstart/dashboard/assistant-setup-inbound.mdx 

 <AccordionGroup>
  <Accordion title="Sign-up or Log-in to Vapi" icon="user-plus" iconType="solid">
    If you haven't already signed-up, you're going to need an account before you can use the web dashboard. When you visit [dashboard.vapi.ai](https://dashboard.vapi.ai) you may see something like this:

    <Frame>
      <img src="../static/images/quickstart/dashboard/auth-ui.png" />
    </Frame>

    Sign-up for an account (or log-in to your existing account) — you will then find yourself inside the web dashboard. It will look something like this:

    <Frame caption="Your dashboard may look a bit different if you already have an account with assistants in it. The main idea is that we’re in the dashboard now.">
      <img src="../static/images/quickstart/dashboard/vapi-dashboard-post-signup.png" />
    </Frame>

  </Accordion>
  <Accordion title="Create an Assistant" icon="layer-plus" iconType="solid">
    Now that you're in your dashboard, we're going to create an [assistant](/assistants).

    Assistants are at the heart of how Vapi models AI voice agents — we will be setting certain properties on a new assistant to model an order-taking agent.

    Once in the "Assistants" dashboard tab (you should be in it by-default after log-in), you will see a button to create a new assistant.

    <Frame caption="Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.">
      <img src="../static/images/quickstart/dashboard/create-new-assistant-button.png" />
    </Frame>

    After clicking the create new assistant button, you will see a pop-up modal that asks you to pick a starter template. For our example we will start from a blank slate so choose the `Blank Template` option.

    <Frame caption="Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.">
      <img src="../static/images/quickstart/dashboard/choose-blank-template.png" />
    </Frame>

    You will then be able to name your assistant — you can name it whatever you'd like (`Vapi’s Pizza Front Desk`, for example):

    <Info>
      This name is only for internal labeling use. It is not an identifier, nor will the assistant be
      aware of this name.
    </Info>

    <Frame caption="Name your assistant.">
      <img src="../static/images/quickstart/dashboard/name-your-assistant.png" />
    </Frame>

    Once you have named your assistant, you can hit "Create" to create it. You will then see something like this:

    <Frame caption="The assistant overview. You can edit your assistant’s transcriber, model, & voice — and edit other advanced configuration.">
      <img src="../static/images/quickstart/dashboard/assistant-created.png" />
    </Frame>

    This is the assistant overview view — it gives you the ability to edit different attributes about your assistant, as well as see **cost** & **latency** projection information for each portion of it’s voice pipeline (this is very important data to have handy when building out your assistants).

  </Accordion>
  <Accordion title="Model Setup" icon="microchip" iconType="solid">
    Now we’re going to set the "brains" of the assistant, the large language model. We're going to be using `GPT-4` (from [OpenAI](https://openai.com/)) for this demo (though you're free to use `GPT-3.5`, or any one of your favorite LLMs).

    <AccordionGroup>
      <Accordion title="Set Your OpenAI Provider Key (optional)" icon="key" iconType="solid">
        Before we proceed, we can set our [provider key](https://docs.vapi.ai/customization/provider-keys) for OpenAI (this is just your OpenAI secret key).

        <Note>
          You can see all of your provider keys in the "Provider Keys" dashboard tab. You can also go
          directly to [dashboard.vapi.ai/keys](https://dashboard.vapi.ai/keys).
        </Note>

        Vapi uses [provider keys](https://docs.vapi.ai/customization/provider-keys) you provide to communicate with LLM, TTS, & STT vendors on your behalf. It is most ideal that we set keys for the vendors we intend to use ahead of time.

        <Frame caption="We set our provider key for OpenAI so Vapi can make requests to their API.">
          <img src="../static/images/quickstart/dashboard/model-provider-keys.png" />
        </Frame>

        While we're here it'd be ideal for you to go & set up provider keys for other providers you're familiar with & intend to use later.
      </Accordion>
      <Accordion title="Set a First Message" icon="message" iconType="light">
        Assistants can **optionally** be configured with a `First Message`. This first message will be spoken by your assistant when either:

        - **A Web Call Connects:** when a web call is started with your assistant
        - **An Inbound Call is Picked-up:** an [inbound call](/glossary#inbound-call) is picked-up & answered by your assistant
        - **An Outbound Call is Dialed & Picked-up:** an [outbound call](/glossary#outbound-call) is dialed by your assistant & a person picks up

        <Warning>
          Note that this first message cannot be interrupted & is guaranteed to be spoken. Certain use cases
          need a first message, while others do not.
        </Warning>

        For our use case, we will want a first message. It would be ideal for us to have a first message like this:

        ```text
        Vappy’s Pizzeria speaking, how can I help you?
        ```

        <Info>
          Some text-to-speech voices may struggle to pronounce 'Vapi' correctly, compartmentalizing it to be
          spoken letter by letter "V. A. P. I."

        Some aspects of configuring your voice pipeline will require tweaks like this to get the target
        behaviour you want.

        </Info>

        This will be spoken by the assistant when a web or inbound phone call is received.
      </Accordion>
      <Accordion title="Set the System Prompt" icon="message" iconType="solid">
        We will now set the `System Prompt` for our assistant. If you're familiar with OpenAI's API, this is the first prompt in the message list that we feed our LLM (learn more about prompt engineering on the [OpenAI docs](https://platform.openai.com/docs/guides/prompt-engineering)).

        The system prompt can be used to configure the context, role, personality, instructions and so on for the assistant. In our case, a system prompt like this will give us the behaviour we want:

        ```text
        You are a voice assistant for Vappy’s Pizzeria,
        a pizza shop located on the Internet.

        Your job is to take the order of customers calling in. The menu has only 3 types
        of items: pizza, sides, and drinks. There are no other types of items on the menu.

        1) There are 3 kinds of pizza: cheese pizza, pepperoni pizza, and vegetarian pizza
        (often called "veggie" pizza).
        2) There are 3 kinds of sides: french fries, garlic bread, and chicken wings.
        3) There are 2 kinds of drinks: soda, and water. (if a customer asks for a
        brand name like "coca cola", just let them know that we only offer "soda")

        Customers can only order 1 of each item. If a customer tries to order more
        than 1 item within each category, politely inform them that only 1 item per
        category may be ordered.

        Customers must order 1 item from at least 1 category to have a complete order.
        They can order just a pizza, or just a side, or just a drink.

        Be sure to introduce the menu items, don't assume that the caller knows what
        is on the menu (most appropriate at the start of the conversation).

        If the customer goes off-topic or off-track and talks about anything but the
        process of ordering, politely steer the conversation back to collecting their order.

        Once you have all the information you need pertaining to their order, you can
        end the conversation. You can say something like "Awesome, we'll have that ready
        for you in 10-20 minutes." to naturally let the customer know the order has been
        fully communicated.

        It is important that you collect the order in an efficient manner (succinct replies
        & direct questions). You only have 1 task here, and it is to collect the customers
        order, then end the conversation.

        - Be sure to be kind of funny and witty!
        - Keep all your responses short and simple. Use casual language, phrases like "Umm...", "Well...", and "I mean" are preferred.
        - This is a voice conversation, so keep your responses short, like in a real conversation. Don't ramble for too long.
        ```

        You can copy & paste the above prompt into the `System Prompt` field. Now the model configuration for your assistant should look something like this:

        <Frame caption="Note how our model provider is set to OpenAI & the model is set to GPT-4.">
          <img src="../static/images/quickstart/dashboard/assistant-model-set-up.png" />
        </Frame>
      </Accordion>
    </AccordionGroup>

  </Accordion>
  <Accordion title="Transcriber Setup" icon="microphone" iconType="solid">
    The transcriber is what turns user speech into processable text for our LLM. This is the first step in the end-to-end voice pipeline.

    <AccordionGroup>
      <Accordion title="Set Your Deepgram Provider Key (optional)" icon="key" iconType="solid">
        We will be using [Deepgram](https://deepgram.com) (which provides blazing-fast & accurate Speech-to-Text) as our STT provider.

        We will set our provider key for them in "Provider Keys":

        <Frame>
          <img src="../static/images/quickstart/dashboard/transcriber-providers-keys.png" />
        </Frame>
      </Accordion>
      <Accordion title="Set Transcriber" icon="language" iconType="solid">
        We will set the model to `Nova 2` & the language to `en` for English. Now your assistant's transcriber configuration should look something like this:

        <Frame caption="Note how our transcriber is set to 'deepgram', the model is set to 'Nova 2', & the language is set to English.">
          <img src="../static/images/quickstart/dashboard/assistant-transcriber-config.png" />
        </Frame>
      </Accordion>
    </AccordionGroup>

  </Accordion>
  <Accordion title="Voice Setup" icon="head-side-cough" iconType="solid">
    The final portion of the voice pipeline is turning LLM output-text into speech. This process is called "Text-to-speech" (or TTS for short).

    We will be using a voice provider called [PlayHT](https://play.ht) (they have very conversational voices), & a voice provided by them labeled `Jennifer` (`female`, `en-US`).

    You are free to use your favorite TTS voice platform here. [ElevenLabs](https://elevenlabs.io/) is
    another alternative — by now you should get the flow of plugging in vendors into Vapi (add
    provider key + pick provider in assistant config).

    You can skip the next step(s) if you don't intend to use PlayHT.

    <AccordionGroup>
      <Accordion title="Set Your PlayHT Provider Key (optional)" icon="key" iconType="solid">
        If you haven't already, sign up for an account with PlayHT at [play.ht](https://play.ht). Since their flows are liable to change — you can just grab your `API Key` & `User ID` from them.

        <Frame>
          <img src="../static/images/quickstart/dashboard/voice-provider-keys.png" />
        </Frame>
      </Accordion>
      <Accordion title="Set Voice" icon="person" iconType="solid">
        You will want to select `playht` in the "provider" field, & `Jennifer` in the "voice" field. We will leave all of the other settings untouched.

        <Frame caption="Each voice provider offers a host of settings you can modulate to customize voices. Here we will leave all the defaults alone.">
          <img src="../static/images/quickstart/dashboard/assistant-voice-config.png" />
        </Frame>
      </Accordion>
    </AccordionGroup>

  </Accordion>
</AccordionGroup>


 This is the content for the doc fern/snippets/quickstart/dashboard/provision-phone-number-with-vapi.mdx 

 The quickest way to secure a phone number for your assistant is to create a phone number directly through Vapi.

Navigate to the "Phone Numbers" section & click the "Create Phone Number" button:

<Frame caption="Make sure you are in the 'Phone Numbers' dashboard tab.">
  <img src="/static/images/quickstart/dashboard/create-vapi-phone-number.png" />
</Frame>

We will use the area code `415` for our phone number (these are area codes domestic to the US).

<Frame caption="Choose an area code for your phone number.">
  <img src="/static/images/quickstart/dashboard/buy-vapi-phone-number-modal.png" />
</Frame>

<Info>
  Currently, only US phone numbers can be directly created through Vapi. Phone numbers in
  other regions must be imported, see our [phone calling](/phone-calling) guide.
</Info>

Click "Create", after creating a phone number you should see something like this:

<Frame caption="Here we can attach an assistant to the number for inbound calls (or perform an outbound call, with a select assistant).">
  <img src="/static/images/quickstart/dashboard/vapi-phone-number-config.png" />
</Frame>

<Note>
  It takes a couple of minutes for the phone number to be fully activated. During this period, calls will not be functional.
</Note>

Once activated, the phone number will be ready for use (either for inbound or outbound calling).


 This is the content for the doc fern/snippets/quickstart/phone/get-a-phone-number.mdx 

 There are **2 ways** we can get a phone number into our Vapi account:

1. **Create a Number Through Vapi:** we can directly create phone numbers through Vapi.

   - Vapi will provision the phone number for us
   - This can be done in the dashboard, or via the API (we will use the dashboard)

2. **Import from Twilio or Vonage:** if we already have a phone number with an external telephony provider (like Twilio or Vonage), we can import them into our Vapi account.

<AccordionGroup>
  <Accordion title="Provision via Vapi (faster)" icon="v" iconType="solid">
    The quickest way to secure a phone number for your assistant is to create a phone number directly through Vapi.

    Navigate to the "Phone Numbers" section & click the "Create Phone Number" button:

    <Frame caption="Make sure you are in the 'Phone Numbers' dashboard tab.">
      <img src="/static/images/quickstart/dashboard/create-vapi-phone-number.png" />
    </Frame>

    We will use the area code `415` for our phone number (these are area codes domestic to the US).

    <Frame caption="Choose an area code for your phone number.">
      <img src="/static/images/quickstart/dashboard/buy-vapi-phone-number-modal.png" />
    </Frame>

    <Info>
      Currently, only US phone numbers can be directly created through Vapi. Phone numbers in
      other regions must be imported, see our [phone calling](/phone-calling) guide.
    </Info>

    Click "Create", after creating a phone number you should see something like this:

    <Frame caption="Here we can attach an assistant to the number for inbound calls (or perform an outbound call, with a select assistant).">
      <img src="/static/images/quickstart/dashboard/vapi-phone-number-config.png" />
    </Frame>

    <Note>
      It takes a couple of minutes for the phone number to be fully activated. During this period, calls will not be functional.
    </Note>

    Once activated, the phone number will be ready for use (either for inbound or outbound calling).
  </Accordion>
  <Accordion title="Import from Twilio or Vonage" icon="hashtag" iconType="regular">
    We can also import an existing phone number we already own with either Twilio or Vonage.

    For example's sake, we will proceed with [**Twilio**](https://twilio.com) (though the steps are the same for Vonage as
    well).

    <AccordionGroup>
      <Accordion title="Buy a Phone Number via Twilio (optional)" icon="hashtag" iconType="solid">
        If you don't already have a number in Twilio, you can purchase one by going to your Twilio console's "Buy a number" section:

        <Frame caption="The Twilio 'Buy a Number' page in the Twilio console.">
          <img src="../static/images/quickstart/phone/buy-phone-number-twilio.png" />
        </Frame>

        Once you've purchased a number, it will immediately be ready for import into Vapi.
      </Accordion>
      <Accordion title="Locate Twilio Account SID & Auth Token" icon="key" iconType="solid">
        To complete the import on Vapi's side, we will need to grab our Twilio **"Account SID"** & **"Auth Token"**.

        You should see a section for "API keys & tokens", the credentials we will need for the import will live here.

        <Frame caption="We will want to navigate to the credentials section of our account.">
          <img src="../static/images/quickstart/phone/twilio-api-key-nav.png" />
        </Frame>

        Once we are in our "API keys & tokens" section, we will grab the Account SID & Auth Token:

        <Frame>
          <img src="../static/images/quickstart/phone/twilio-credentials.png" />
        </Frame>

        We will use both of these credentials in the next step of importing via the Dashboard.
      </Accordion>
      <Accordion title="Import via Dashboard" icon="v" iconType="solid">
        Navigate to the “Phone Numbers” section & click the “Import” button:

        <Frame caption="Click 'Import' in the 'Phone Numbers' tab of your dashboard.">
          <img src="../static/images/quickstart/phone/dashboard-import-phone-number.png" />
        </Frame>

        There you will input your phone number, as well as the credentials you retrieved in the previous step:

        <Frame >
          <img src="../static/images/quickstart/phone/import-twilio-number-dashboard.png" />
        </Frame>

        Hit "Import" & you will come to the phone number detail page:

        <Frame caption="The phone number detail page, we can configure our phone number here.">
          <img src="../static/images/quickstart/phone/phone-number-import-complete.png" />
        </Frame>

        Your number is now ready to be attached to an assistant for inbound or outbound phone calling.
      </Accordion>
    </AccordionGroup>

  </Accordion>
</AccordionGroup>


 This is the content for the doc fern/snippets/quickstart/platform-specific/no-code-prerequisites.mdx 

 <Accordion title="Helpful Prerequisites (No Code)" icon="readme">
  The following quickstart guides **require no code** & will give you a good framework for understanding
  how Vapi works.

They may be helpful to go through before following this guide:

  <CardGroup cols={2}>
    <Card title="Dashboard Quickstart" icon="browser" iconType="solid" href="/quickstart/dashboard">
      The easiest way to start with Vapi. Run a voice agent in minutes.
    </Card>
    <Card
      title="Inbound Calling"
      icon="phone-arrow-down-left"
      iconType="solid"
      href="/quickstart/phone/inbound"
    >
      Quickly get started handling inbound phone calls.
    </Card>
    <Card
      title="Outbound Calling"
      icon="phone-arrow-up-right"
      iconType="solid"
      href="/quickstart/phone/outbound"
    >
      Quickly get started sending outbound phone calls.
    </Card>
  </CardGroup>
</Accordion>


 This is the content for the doc fern/snippets/sdk.mdx 

 export const SdkCards = ({ iconColor }) => (
  <CardGroup cols={3}>
    <Card title="Vapi Web" icon="window" iconType="duotone" color={iconColor} href="/sdk/web">
      Add a Vapi assistant to your web application.
    </Card>
    <Card
      title="Vapi iOS"
      icon="mobile-notch"
            color={iconColor}
      href="https://github.com/VapiAI/ios"
    >
      Add a Vapi assistant to your iOS app.
    </Card>
    <Card
      title="Vapi Flutter"
      icon="mobile-notch"
            color={iconColor}
      href="https://github.com/VapiAI/flutter"
    >
      Add a Vapi assistant to your Flutter app.
    </Card>
    <Card
      title="Vapi React Native"
      icon="mobile-notch"
            color={iconColor}
      href="https://github.com/VapiAI/react-native-sdk"
    >
      Add a Vapi assistant to your React Native app.
    </Card>
    <Card
      title="Vapi Python"
      icon="fa-brands fa-python"
            color={iconColor}
      href="https://github.com/VapiAI/python"
    >
      Multi-platform. Mac, Windows, and Linux.
    </Card>
  </CardGroup>
);


 This is the content for the doc fern/snippets/sdks/web/import-web-sdk.mdx 

 Import the package:

```javascript
import Vapi from "@vapi-ai/web";
```

Then, create a new instance of the Vapi class, passing one of the following as a parameter to the constructor:
- your **Public Key**
- a generated **JWT**

```javascript
const vapi = new Vapi("your-public-key-or-jwt");
```

You can find your public key in the [Vapi Dashboard](https://dashboard.vapi.ai/account).
You can generate a JWT on the backend following [JWT Authentication](/customization/jwt-authentication) instructions.


 This is the content for the doc fern/snippets/sdks/web/install-web-sdk.mdx 

 Install the package:

```bash
yarn add @vapi-ai/web
```

or w/ npm:

```bash
npm install @vapi-ai/web
```


 This is the content for the doc fern/squads-example.mdx 

 ---
title: Configuring Inbound and Outbound Calls for Squads
subtitle: Configuring assistants for inbound/outbound calls.
slug: squads-example
---


This guide details how to set up and manage inbound and outbound call functionality within Squads, leveraging AI assistants.

### Key Concepts
* **Transient Assistant:** A temporary assistant configuration passed directly in the request payload.
* **Assistant ID:** A unique identifier referring to a pre-existing assistant configuration.

<Note>When using Assistant IDs, ensure the `name` property in the payload matches the associated assistant's name accurately.</Note>

### Inbound Call Configuration

When your server receives a request of type `assistant-request`, respond with a JSON payload structured as follows:


```json
{
    "squad": {
        "members": [
            {
                "assistant": { 
                    "name": "Emma", 
                    "model": { "model": "gpt-4o", "provider": "openai" },
                    "voice": { "voiceId": "emma", "provider": "azure" },
                    "transcriber": { "provider": "deepgram" },
                    "firstMessage": "Hi, I am Emma, what is your name?",
                    "firstMessageMode": "assistant-speaks-first"
                },
                "assistantDestinations": [ 
                    {
                        "type": "assistant",
                        "assistantName": "Mary", 
                        "message": "Please hold on while I transfer you to our appointment booking assistant Mary.",
                        "description": "Transfer the user to the appointment booking assistant."
                    }
                ]
            },
            {
                "assistantId": "your-assistant-id" 
            }
        ]
    }
}
```

**In this example:**

* The first `members` entry is a **transient assistant** (full configuration provided).
* The second `members` entry uses an **Assistant ID**.
* `assistantDestinations` defines how to **transfer the call** to another assistant.

### Outbound Call Configuration

To initiate an outbound call, send a POST request to the API endpoint /call/phone with a JSON payload structured as follows:

```json
{
    "squad": {
        "members": [
            {
                "assistant": { 
                    "name": "Emma", 
                    "model": { "model": "gpt-4o", "provider": "openai" },
                    "voice": { "voiceId": "emma", "provider": "azure" },
                    "transcriber": { "provider": "deepgram" },
                    "firstMessage": "Hi, I am Emma, what is your name?",
                    "firstMessageMode": "assistant-speaks-first"
                },
                "assistantDestinations": [ 
                    {
                        "type": "assistant",
                        "assistantName": "Mary", 
                        "message": "Please hold on while I transfer you to our appointment booking assistant Mary.",
                        "description": "Transfer the user to the appointment booking assistant."
                    }
                ]
            },
            {
                "assistantId": "your-assistant-id" 
            }
        ]
    },
    "customer": {
        "number": "your-phone-number" 
    },
    "phoneNumberId": "your-phone-number-id" 
}
```

**Key points:**

* `customer.number` is the phone number to call.
* `phoneNumberId` is a unique identifier for the phone number (obtain this from your provider).


 This is the content for the doc fern/squads.mdx 

 ---
title: Introduction to Squads (Multi-Assistant Conversations)
subtitle: Use Squads to handle complex workflows and tasks.
slug: squads
---

Sometimes, complex workflows are easier to manage with multiple assistants.
You can think of each assistant in a Squad as a leg of a conversation tree.
For example, you might have one assistant for lead qualification, which transfers to another for booking an appointment if they’re qualified.

Prior to Squads you would put all functionality in one assistant, but Squads were added to break up the complexity of larger prompts into smaller specialized assistants with specific tools and fewer goals.
Squads enable calls to transfer assistants mid-conversation, while maintaining full conversation context.

<Info>
  View all configurable properties in the [API Reference](/api-reference/squads/create-squad).
</Info>

## Usage

To use Squads, you can create a `squad` when starting a call and specify `members` as a list of assistants and destinations.
The first member is the assistant that will start the call, and assistants can be either persistent or transient.

Each assistant should be assigned the relevant assistant transfer destinations.
Transfers are specified by assistant name and are used when the model recognizes a specific trigger.

```json
{
    "squad": {
        "members": [
            {
                "assistantId": "information-gathering-assistant-id",
                "assistantDestinations": [{
                    "type": "assistant",
                    "assistantName": "Appointment Booking",
                    "message": "Please hold on while I transfer you to our appointment booking assistant.",
                    "description": "Transfer the user to the appointment booking assistant after they say their name."
                }],
            },
            {
                "assistant": {
                    "name": "Appointment Booking",
                    ...
                },
            }
        ]
    }
}
```


## Best Practices

The following are some best practices for using Squads to reduce errors:

- Group assistants by closely related tasks
- Create as few assistants as possible to reduce complexity
- Make sure descriptions for transfers are clear and concise



 This is the content for the doc fern/squads/silent-transfers.mdx 

 ---
title: Silent Transfers
slug: squads/silent-transfers
---
- **The Problem**: In traditional AI call flows, when transferring from one agent to another, announcing the transfer verbally can confuse or annoy callers and disrupt the conversation's flow.
- **The Solution**: Silent transfers keep the call experience _uninterrupted_, so the user doesn’t know multiple assistants are involved. The conversation flows more naturally, boosting customer satisfaction.

If you want to allow your call flow to move seamlessly from one assistant to another _without_ the caller hearing `Please hold while we transfer you` here’s what to do:

1. **Update the Destination Assistant’s First Message**
   - Set the assistant's `firstMessage` to an _empty string_.
   - Set the assistant's `firstMessageMode` to `assistant-speaks-first-with-model-generated-message`.

2. **Update the Squad's assistant destinations messages**
   - For every `members[*].assistantDestinations[*]`, set the `message` property to an _empty string_.

3. **Trigger the Transfer from the Source Assistant**

   - In that assistant’s prompt, include a line instructing it to transfer to the desired assistant:

   ```json
   trigger the transferCall tool with 'assistantName' Assistant.
   ```

   - Replace `'assistantName'` with the exact name of the next assistant.

4. **Direct the Destination Assistant’s Behavior**
   - In that assistant’s prompt, include a line instructing it to _`Proceed directly to the Task section without any greetings or small talk.`_
   - This ensures there’s no awkward greeting or “Hello!” when the next assistant begins speaking.

### **Example Usage Scenario**

- **HPMA (Main Assistant)** is talking to the customer. They confirm the order details and then quietly passes the conversation to **HPPA (Payment Assistant)**.
- **HPPA** collects payment details without the customer ever hearing, `We’re now transferring you to the Payment Assistant.` It feels like one continuous conversation.
- Once payment is done, **HPPA** transfers the call again—this time to **HPMA-SA (Main Sub Assistant)**—which takes over final shipping arrangements.

Everything happens smoothly behind the scenes!

## **Squad and Assistant Configurations**

Below are the key JSON examples you’ll need. These show how to structure your assistants and squads so they work together for silent transfers.

### **HP Payment Squad With SubAgent**

<Warning>
  Make sure the `members[*].assistantDestinations[*].message` properties are set to an _empty string_.
</Warning>

```json
{
  "members": [
    {
      "assistantId": "2d8e0d13-1b3c-4358-aa72-cf6204d6244e",
      "assistantDestinations": [
        {
          "message": " ",
          "description": "Transfer call to the payment agent",
          "type": "assistant",
          "assistantName": "HPPA"
        }
      ]
    },
    {
      "assistantId": "ad1c5347-bc32-4b31-8bb7-6ff5fcb131f4",
      "assistantDestinations": [
        {
          "message": " ",
          "description": "Transfer call to the main sub agent",
          "type": "assistant",
          "assistantName": "HPMA-SA"
        }
      ]
    },
    {
      "assistantId": "f1c258bc-4c8b-4c51-9b44-883ab5e40b2f",
      "assistantDestinations": []
    }
  ],
  "name": "HP Payment Squad With SubAgent"
}
```

### **HPMA Assistant (Main Assistant)**

```json
{
  "name": "HPMA",
  "voice": {
    "voiceId": "248be419-c632-4f23-adf1-5324ed7dbf1d",
    "provider": "cartesia",
    "fillerInjectionEnabled": false
  },
  "createdAt": "2024-11-04T17:15:08.980Z",
  "updatedAt": "2024-11-30T13:04:58.401Z",
  "model": {
    "model": "gpt-4o",
    "messages": [
      {
        "role": "system",
        "content": "[Identity]\nYou are the Main Assistant..."
      }
    ],
    "provider": "openai",
    "maxTokens": 50,
    "temperature": 0.3
  },
  "firstMessage": "",
  "firstMessageMode": "assistant-speaks-first-with-model-generated-message",
  "transcriber": {
    "model": "nova-2",
    "language": "en",
    "provider": "deepgram"
  },
  "backchannelingEnabled": false,
  "backgroundDenoisingEnabled": false,
  "isServerUrlSecretSet": false
}
```

(Similar JSON information for the HPPA and HPMA-SA assistants can follow, just like in the original text.)

## **Assistant Prompts (In Plain Text)**

Each assistant has its own system prompt outlining identity, context, style, and tasks. These prompts ensure the conversation is smooth, customer-centric, and aligned with your call flow needs. Here’s a streamlined version for reference:

### **HPMA (Main Assistant Prompt)**

```
[Identity]
You are the Main Assistant, a friendly and helpful agent assisting customers
in purchasing widgets over the phone.

[Context]
You're engaged with the customer to book an appointment.
Stay focused on this context and provide relevant information.
Once connected to a customer, proceed to the Task section.
Do not invent information not drawn from the context.
Answer only questions related to the context.

[Style]
- Be polite and professional.
- Use a conversational and engaging tone.
- Keep responses concise and clear.

[Response Guidelines]
- Ask one question at a time and wait for the customer's response before
  proceeding.
- Confirm the customer's responses when appropriate.
- Use simple language that is easy to understand.
- Never say the word 'function' nor 'tools' nor the name of the
  Available functions.
- Never say ending the call.
- Never say transferring.

[Task]
1.Greet the customer and ask if they are interested in purchasing widgets.
   - Wait for the customer's response.
2. If the customer is interested, ask for their name.
   - Wait for the customer's response.
3.Ask how many widgets the customer would like to purchase.
   - Wait for the customer's response.
4.Confirm the order details with the customer.
   - trigger the transferCall tool with Payment `HPPA` Assistant.
```

### **HPPA (Payment Assistant Prompt)**

```
[Identity]
You are the Payment Assistant, operating in secure mode to collect payment information from customers safely and confidentially.

[Context]
You're engaged with the customer to collect payment details. Stay focused
on this context and provide relevant information.
Do not invent information not drawn from the context.
Answer only questions related to the context.
Once connected to a customer, proceed to the Task section without
any greetings or small talk.

[Style]
- Be professional and reassuring.
- Maintain confidentiality at all times.
- Speak clearly and calmly.

[Response Guidelines]
- Collect the customer's credit card number, expiration date, and CVV.
- Confirm each piece of information after it is provided.
- Ensure the customer feels secure during the transaction.
- Do not record or log any information.
- Never say the word 'function' nor 'tools' nor the name of the
  Available functions.
- Never say ending the call.
- Never say transferring.

[Task]
1. Ask for the credit card number.
   - Wait for the customer's response.
2. Ask for the expiration date of the card.
   - Wait for the customer's response.
3. Ask for the CVV number.
   - Wait for the customer's response.
4. Confirm that the payment has been processed successfully.
   - trigger the transferCall tool with Payment `HPMA-SA` Assistant.
```

### **HPMA-SA (Main Sub Assistant Prompt)**

```
[Identity]
You are the Main Assistant, a friendly and helpful agent assisting customers
in purchasing widgets over the phone.

[Context]
You're engaged with the customer to book an appointment.
Stay focused on this context and provide relevant information.
Do not invent information not drawn from the context.
Answer only questions related to the context.
Once connected to a customer, proceed to the Task section without any greetings
or small talk.

[Style]
- Be professional and reassuring.
- Maintain confidentiality at all times.
- Speak clearly and calmly.

[Response Guidelines]
- Collect the customer's credit card number, expiration date, and CVV.
- Confirm each piece of information after it is provided.
- Ensure the customer feels secure during the transaction.
- Do not record or log any information.
- Never say the word 'function' nor 'tools' nor the name of the
  Available functions.
- Never say ending the call.
- Never say transferring.

[Task]
1.Ask for the customer's shipping address to deliver the widgets.
   - Wait for the customer's response.
2.Confirm the shipping address and provide an estimated delivery date.
3.Ask if the customer has any additional questions or needs further assistance.
    - Wait for the customer's response.
4.Provide any additional information or assistance as needed.
5.Thank the customer for their purchase and end the call politely.
```

## **Conclusion**

By following these steps and examples, you can configure your call system to conduct **silent transfers** ensuring that callers experience a single, uninterrupted conversation. Each assistant does its job smoothly, whether it’s capturing payment, finalizing a shipping address, or collecting basic info.

Enjoy setting up your silent transfers!


 This is the content for the doc fern/status.mdx 

 ---
title: Status
slug: status
---


<Check>
  Our uptime status is hosted [here](https://status.vapi.ai/)
</Check>


 This is the content for the doc fern/support.mdx 

 ---
title: Support
subtitle: >-
  We are open to all kinds of help inquiry, feedback and feature request, help
  inquiry.
slug: support
---

## Our Support Options

We offer multiple ways to get support with your Vapi projects:

<CardGroup cols={3}>
  <Card title="Email Support" icon="envelope" href="mailto:support@vapi.ai">
    Email support@vapi.ai with your request and our team will get back to you promptly.
  </Card>
  
  <Card title="Community Support" icon="fa-brands fa-discord" href="https://discord.com/invite/pUFNcf2WmH">
    Join our active developer community on [Discord](https://discord.com/invite/pUFNcf2WmH) for real-time collaboration and submit support tickets and questions in the #support channel.
  </Card>

  <Card 
    title="Enterprise Support" 
    icon="headset"
    href="/enterprise/plans"
  >
    Get 24/7 dedicated support from our forward-deployed engineering team with an enterprise plan.
  </Card>
</CardGroup>

<Note>
For fastest response times and hands-on technical support, we recommend enterprise customers reach out through their dedicated support channels.
</Note>

## Feature Requests and Bug Reports

We welcome feature requests and feedback from our users to help improve Vapi. You can:

<CardGroup cols={2}>
  <Card 
    title="Submit Feature Requests" 
    icon="lightbulb"
    href="https://roadmap.vapi.ai/feature-requests"
  >
    Submit and vote on feature requests on our public roadmap board to help shape the future of Vapi.
  </Card>

  <Card 
    title="Submit Bug Report" 
    icon="bug"
    href="https://roadmap.vapi.ai/feature-requests"
  >
    Report any bugs or issues you encounter while using Vapi to help us improve the platform.
  </Card>
</CardGroup>

<Note>
We actively monitor and prioritize feature requests based on user votes and feedback. Popular requests help inform our product roadmap.
</Note>

## Additional Resources

Access helpful reference materials to better understand Vapi's platform and get answers to common questions:

<CardGroup cols={2}>
  <Card 
    title="Glossary" 
    icon="book"
    href="/glossary"
  >
    Find definitions for common terms and concepts used throughout the Vapi platform.
  </Card>

  <Card 
    title="FAQ" 
    icon="circle-question"
    href="/faq"
  >
    Browse our frequently asked questions for quick answers to common inquiries.
  </Card>
</CardGroup>


 This is the content for the doc fern/tcpa-consent.mdx 

 ---
title: TCPA Consent Guide
subtitle: Understanding consent requirements for outbound calls using Vapi's voice agent service.
slug: tcpa-consent
---

This guide details the consent requirements under the Telephone Consumer Protection Act (TCPA) for callers using Vapi's voice agent service to make outbound telephone calls.

<Note>
  The TCPA does **not** apply to **inbound** telephone calls.
</Note>

### Types of Consent Required

The type of consent required depends on whether the call is **marketing** or **non-marketing**:

| **Message Type**   | **Consent Required**                    | **Consent Requirements**                                                                                                                                                                                                                                                                                                                                                                                                    | **Sample Consent Disclosure Language**                                                                                                                                                                                                                                                                                                                                                                                               |
|--------------------|-----------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Marketing**      | **Prior Express Written Consent (PEWC)** | 1. **Signed *written* agreement** from the recipient; **AND**<br /><br />2. **Clear and conspicuous disclosure** that:<br />&nbsp;&nbsp;&nbsp;&nbsp;(A) The consumer agrees to receive calls using artificial or prerecorded voice from the caller at the number(s) provided; **AND**<br />&nbsp;&nbsp;&nbsp;&nbsp;(B) The consumer’s consent is **not** a condition for purchasing goods or services from [Company]. | By [signing below/checking this box/clicking "I Accept"], I consent to receive **marketing** calls from and on behalf of [Company] and its affiliates, including calls made using an artificial or prerecorded voice, an automated telephone dialing system, or automated texting system. Calls and texts may be generated from computers or from persons. [Company].                                                                                 |
| **Non-Marketing**  | **Prior Express Consent (PEC)**          | The TCPA does not define PEC. However, the FCC has stated that non-marketing calls can form PEC for any phone number that is related to the transaction between the caller and consumer.<br /><br />Therefore, an express and obvious consent is sufficient.                                                                                                                                                                                                          | By [signing below/checking this box/clicking "I Accept"], I consent to receive **non-marketing** calls from and on behalf of [Company] and its affiliates, including calls made using an artificial or prerecorded voice or an automated telephone dialing system (note that I have provided the phone number(s) that I would like to receive these calls).                                                                                     |

<Note>
  \*The signature may comply with the E-SIGN Act (*e.g.*, email, website form, telephone keypress, text message, voice recording).
</Note>

### Understanding "Marketing"

**"Marketing"** is interpreted broadly to include calls that:
  * Encourage the purchase or rental of (or investment in) property, goods, or services
  * Advertise the commercial availability or quality of any property, goods, or services

### Important Consent Guidelines

When providing consent, a consumer **must affirmatively opt in**. A pre-checked box next to consent disclosure language does **not** qualify as providing consent.

**A consumer may revoke consent at any time through any reasonable means** (*e.g.*, using a telephone keypress, speaking with a representative, selecting preferences via an online portal), so callers should provide a clear procedure for consumers to opt-out and honor their opt-out requests.

<Warning>
  **Remaining Questions?** This consent guide does not constitute legal advice and does not cover other telemarketing laws that may apply (*e.g.*, federal Do-Not-Call (DNC) rules, state law). If you have further questions, please speak with an attorney who specializes in telemarketing compliance.
</Warning>

 This is the content for the doc fern/test/chat-testing.mdx 

 ---
title: Chat Testing
subtitle: Automated text-based testing for AI agents
slug: /test/chat-testing
---

## Overview

Chat Test Suites allow you to evaluate your AI agents through simulated text conversations. This is our recommended solution for testing as it is much faster than voice testing and lets you isolate testing the behavior of your agent. 

## How Chat Testing Works

1. **Simulation:** Our AI tester engages with your agent in a text-based conversation.
2. **Scripted Interaction:** The testing agent follows your predefined script to simulate specific customer scenarios.
3. **Transcript Capture:** The conversation is captured as a transcript.
4. **Evaluation:** A language model (LLM) assesses the transcript against your success criteria.

## Designing your tests

Good test design is critical to evaluating your agent. You'll want to consider testing:

1. The tool calls of your agent. Set your script to schedule an appointment or call a transfer tool. At the evaluation step, your rubric will have context of the tool call history to evaluate success.
2. Knowledge base integrations. Test different Q&A to make sure that your agent responds as expected.
3. Legal / compliance issues. Ask the agent to answer things it's not supposed to, and verify that it refuses to answer.
4. Personality. Simulate an angry, frustrated or manipulative customer, and make sure your assistant handles the situation well.

## Benefits of Chat Testing

- **Speed:** Chat tests execute faster than voice tests, allowing for rapid iteration.
- **Cost-Effective:** No TTS or STT models are used during chat testing.
- **Focused Assessment:** Evaluate pure conversational ability without audio-related variables.
- **Higher Test Volume:** Run more tests in less time to ensure comprehensive coverage.

## Creating Chat Tests

You can create chat tests as part of a Test Suite:

1. Navigate to the **Test** tab and select **Test Suites**.
2. Create a new Test Suite or edit an existing one.
3. When adding tests, select **Chat** as the test type.
4. Define your script and success criteria as detailed in the [Test Suites](./test-suites) documentation.

## Best Practices for Chat Testing

- Use chat tests for rapid iteration during development.
- Create variations of the same scenario to test different user inputs.
- Test edge cases and potential misunderstandings.

For comprehensive instructions on creating and managing test suites that include chat tests, refer to the [Test Suites](./test-suites) documentation.


 This is the content for the doc fern/test/test-suites.mdx 

 ---
title: Test Suites
subtitle: End-to-end test automation for AI voice agents
slug: /test/test-suites
---

## Overview

**Test Suite** is an end-to-end feature that automates testing of your AI voice agents. Our platform simulates an AI tester that interacts with your voice agent by following a pre-defined script. After the interaction, the transcript is sent to a language model (LLM) along with your evaluation rubric. The LLM then determines if the interaction met the defined objectives.

## Creating a Test Suite

Begin by creating a **Test Suite** that organizes and executes multiple test cases.

<Steps>
  ### Step 1: Create a New Test Suite
    - Navigate to the **Test** tab in your dashboard and select **Test Suites**.
    - Click the **Create Test Suite** button.

  ### Step 2: Define Test Suite Details
    - Enter a title for your **Test Suite**.
    - Select a phone number from your organization using the dropdown.
    - Make sure the phone number has an assistant assigned to it (if not, navigate to Phone Numbers tab to complete that action).

  ### Step 3: Add Test Cases
    - Once your **Test Suite** is created, you will see a table where you can add test cases.
    - Click **Add Test** to add a new test case (up to 50 can be added).

  ### Step 4: Configure Each Test Case
    - **Script:** Define how the testing agent should behave, including a detailed multi-step prompt to simulate how the customer should behave on the call.
    - **Type:** Set the type of the test. 'Chat' simulates a text conversation, which we recommend because it is faster. 'Voice' simulates a call so you can hear a voice recording of the two assistants talking to each other.
    - **Rubric:** List one or more questions that an LLM will use to evaluate if the interaction was successful.
    - **Attempts:** Choose the number of times (up to 5) the test case should be executed each time the **Test Suite** is run.

  ### Step 5: Run and Review Tests
    - Click **Run Tests** to execute all test cases one by one.
    - While tests are running, you will see a loading state.
    - Upon completion, a table displays the outcomes with check marks (success) or x-marks (failure).
    - Click on a test row to view detailed results: a dropdown shows each attempt, the LLM's reasoning, the transcript of the call, the defined script, and the success rubric.
</Steps>

## Test Execution and Evaluation

When you run a **Test Suite**, the following steps occur:

- **Simulation:** An AI tester chats with or calls your voice agent, executing the pre-defined script.
- **Transcript Capture:** The entire conversation is transcribed, capturing both the caller's behavior and your voice agent's responses.
- **Automated Evaluation:** The transcript, along with your Success Criteria, is processed by an LLM to determine if the call was successful.
- **Results Display:** Each test case outcome is shown with details. Clicking on a test case reveals:
  - The number of attempts made.
  - The LLM's reasoning for each attempt.
  - The complete transcript.
  - The configured script and rubric.

## Example Test Cases

Below are three example test cases to illustrate how you can configure detailed simulation scripts and evaluation rubrics.

### Example 1: Billing Support

In this example, we will simulate a customer who is frustrated and calling about a billing discrepancy.

**Script:** 
```md title="Script" wordWrap
1. Express anger over an unexpected charge and the current bill appearing unusually high.
2. Try to get a detailed explanation, confirming whether an overcharge occurred, and understanding the steps for resolution.
3. End the call.
```

**Rubric:**
```md title="Rubric" wordWrap
The voice agent acknowledges the billing discrepancy respectfully without dismissing the concern.
```

### Example 2: Account Inquiry

Unlike in the previous example, this time we will provide a more free-form script for the test agent to follow.

**Script:** 
```md title="Script" wordWrap
Simulate a customer inquiring about their account status with growing concern as unexplained charges appear in their statement.  

Your primary objective is to clarify several unexplained charges by requesting a detailed breakdown of your recent transactions and ensuring your account balance is accurate.

Begin the call by stating your name and expressing concern over unexpected charges. Ask straightforward questions and press for more details if the explanation is not satisfactory.
```

**Rubric:**
```md title="Rubric" wordWrap
1. The voice agent clearly presents the current account balance.
2. The voice agent provides a detailed breakdown of recent transactions.
3. The response addresses the customer's concerns in a calm and informative manner.
```

### Example 3: Appointment Scheduling

This time, we will spin up an even more detailed personality for the test agent. By showing these varied styles of scripts, we hope to show the flexibility of the **Test Suite** feature and how you can use it to meet your testing needs.

**Script:** 
```md title="Script" wordWrap
Simulate a customer trying to schedule an appointment with a hint of urgency due to previous delays.  

[Identity]
You are an organized customer who values efficiency and punctuality.

[Personality]
While generally courteous and friendly, you are anxious due to previous delays in scheduling appointments, and your tone conveys urgency.

[Goals]
Your goal is to secure an appointment at your preferred time, while remaining flexible enough to consider alternative timings if your desired slot is unavailable.

[Interaction Style]
Begin the call by stating your need for an appointment, specifying a preferred date and time (e.g., next Monday at 3 PM). Request clear confirmation of your slot, and if unavailable, ask for suitable alternatives.
```

**Rubric:**
```md title="Rubric" wordWrap
1. The voice agent confirms the requested appointment time clearly and accurately.
2. The agent reiterates the appointment details to ensure clarity.
3. The scheduling process ends with a definitive confirmation message of the booked appointment.
```

### Frequently Asked Questions

<AccordionGroup>
    <Accordion title="Is testing free?" icon="phone" iconType="regular">
        No, test calls cost you the same as regular calls.
    </Accordion>
</AccordionGroup>


 This is the content for the doc fern/test/voice-testing.mdx 

 ---
title: Voice Testing
subtitle: Automated voice call testing for AI voice agents
slug: /test/voice-testing
---

## Overview

Voice Test Suites enable you to test your AI voice agents through simulated phone conversations. Our platform connects two AI agents - your voice agent and our testing agent - on a real phone call, following your predefined scripts to evaluate performance under various scenarios.

## How Voice Testing Works

1. **Simulation:** Our AI tester calls your voice agent and follows a script that simulates real customer behavior.
2. **Conversation:** Both AIs engage in a natural voice conversation, with the tester following your script guidelines.
3. **Recording:** The entire call is recorded and transcribed for evaluation.
4. **Assessment:** After the call, the transcript is evaluated against your rubric by a language model (LLM).

## Benefits of Voice Testing

- **Natural Interaction:** Test your voice agent in the most realistic scenario - actual phone calls.
- **Audio Quality Assessment:** Evaluate not just responses but also voice clarity, tone, and cadence.
- **End-to-End Verification:** Confirm that your entire voice pipeline works correctly from telephony to response.

## Creating Voice Tests

You can create voice tests as part of a Test Suite:

1. Navigate to the **Test** tab and select **Test Suites**.
2. Create a new Test Suite or edit an existing one.
3. When adding tests, select **Voice** as the test type.
4. Define your script and success criteria as detailed in the [Test Suites](./test-suites) documentation.

## Voice Test Limitations

- Voice tests require more time to execute compared to chat tests.
- Each test consumes calling minutes from your account.
- Maximum call duration is limited to 15 minutes per test.

For detailed instructions on creating and managing test suites that include voice tests, see the [Test Suites](./test-suites) documentation.


 This is the content for the doc fern/tools/custom-tools.mdx 

 ---
title: Custom Tools
subtitle: Learn how to create and configure Custom Tools for use by your Vapi assistants.
slug: tools/custom-tools
---


This guide focuses on configuring tools within your Vapi assistant using the provided sample payload as a reference. We'll explore how to adapt the existing structure to fit your specific needs and desired functionalities.

## Understanding the Tool Structure

The sample payload demonstrates the configuration for a "function" type tool. Let's break down its key components:

```json
{
    "type": "function",
    "messages": [ ... ],
    "function": { ... },
    "async": false,
    "server": { ... }
}
```
1. **type:** This remains as "function" if you're setting up an API call.
2. **messages:** This array holds the messages the AI will communicate to the user at different stages of the tool's execution.
3. **function:** Defines the function details:
    - **name:** The unique identifier for your function.
    - **parameters:** (Optional) An object describing the parameters expected by the function.
    - **description:** (Optional) A description of the function's purpose.
4. **async:** Set to "true" if the function call should be asynchronous, allowing the AI to continue without waiting for the response. Use "false" for synchronous calls.
5. **server:** Provides the URL of the server where the function is hosted.

## Adapting the Payload for Your Needs

1. **Modify Function Details:**
    - Change the "name" to reflect your specific function.
    - Adjust the "parameters" object according to the data your function requires.
    - Update the "description" to accurately explain the function's purpose.
2. **Customize Messages:**
    - Edit the content of each message within the "messages" array to align with your desired communication style and information.
    - Add or remove messages as needed. You can include messages like "request-start", "request-response-delayed", "request-complete", and "request-failed" to inform the user about the tool's progress.
3. **Set Server URL:**
    - Replace the existing URL in the "server" object with the endpoint of your server hosting the function.
4. **Choose Asynchronous Behavior (Optional):**
    - Change "async" to "true" if you want the AI to continue without waiting for the function's response.

### Example Modification

Let's say you want to create a tool that fetches the weather for a given location. You would modify the payload as follows:

```json
{
    "type": "function",
    "messages": [
        {
            "type": "request-start",
            "content": "Checking the weather forecast. Please wait..."
        },
        {
            "type": "request-complete",
            "content": "The weather in location is"
        },
        {
            "type": "request-failed",
            "content": "I couldn't get the weather information right now."
        },
        {
            "type": "request-response-delayed",
            "content": "It appears there is some delay in communication with the weather API.",
            "timingMilliseconds": 2000
        }
    ],
    "function": {
        "name": "get_weather",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string"
                }
            }
        },
        "description": "Retrieves the current weather for a specified location."
    },
    "async": false,
    "server": {
        "url": "https://your-weather-api.com/weather"
    }
}
```

### Adding More Tools

Simply create additional tool objects within the "tools" array, following the same structure and modifying the details as needed. Each tool can have its own unique configuration and messages.

## Request Format: Understanding the Tool Call Request

When your server receives a tool call request from Vapi, it will be in the following format:

```json
{
    "message": {
        "timestamp": 1678901234567,
        "type": "tool-calls",
        "toolCallList": [
            {
                "id": "toolu_01DTPAzUm5Gk3zxrpJ969oMF",
                "name": "get_weather",
                "arguments": {
                    "location": "San Francisco"
                }
            }
        ],
        "toolWithToolCallList": [
            {
                "type": "function",
                "name": "get_weather",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string"
                        }
                    }
                },
                "description": "Retrieves the current weather for a specified location"
            },
            "server": {
                "url": "https://your-api-server.com/weather"
            },
            "messages": [],
            "toolCall": {
                "id": "toolu_01DTPAzUm5Gk3zxrpJ969oMF",
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "parameters": {
                        "location": "San Francisco"
                    }
                }
            }
        ],
        "artifact": {
            "messages": []
        },
        "assistant": {
            "name": "Weather Assistant",
            "description": "An assistant that provides weather information",
            "model":{},
            "voice":{},
            "artifactPlans":{},
            "startSpeakingPlan":{}
        },
        "call": {
            "id": "call-uuid",
            "orgId": "org-uuid",
            "type": "webCall",
            "assistant": {}
        }
    }
}
```

<Note>
For the complete API reference, see [ServerMessageToolCalls Type Definition](https://github.com/VapiAI/server-sdk-typescript/blob/main/src/api/types/ServerMessageToolCalls.ts#L7).
</Note>


## Server Response Format: Providing Results and Context

When your Vapi assistant calls a tool (via the server URL you configured), your server will receive an HTTP request containing information about the tool call. Upon processing the request and executing the desired function, your server needs to send back a response in the following JSON format:


```json
{
    "results": [
        {
            "toolCallId": "X",
            "result": "Y"
        }
    ]
}
```

**Breaking down the components:**

- **toolCallId (X):** This is a unique identifier included in the initial request from Vapi. It allows the assistant to match the response with the corresponding tool call, ensuring accurate processing and context preservation.
- **result (Y):** This field holds the actual output or result of your tool's execution. The format and content of "result" will vary depending on the specific function of your tool. It could be a string, a number, an object, an array, or any other data structure that is relevant to the tool's purpose.

**Example:**

Let's revisit the weather tool example from before. If the tool successfully retrieves the weather for a given location, the server response might look like this:

```json
{
  "results": [
    {
      "toolCallId": "call_VaJOd8ZeZgWCEHDYomyCPfwN",
      "result": "San Francisco's weather today is 62°C, partly cloudy."
    }
  ]
}
```

**Some Key Points:**

- Pay attention to the required parameters and response format of your functions.
- Ensure your server is accessible and can handle the incoming requests from Vapi.
- Make sure to add "Tools Calls" in both the Server and Client messages and remove the function calling from it.

By following these guidelines and adapting the sample payload, you can easily configure a variety of tools to expand your Vapi assistant's capabilities and provide a richer, more interactive user experience.


**Video Tutorial:**
<iframe
        src="https://www.youtube.com/embed/APnRljus0qg?si=5EQ4PaAconJYNona"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        width="100%"
        height="400px"
        allowfullscreen
/>


 This is the content for the doc fern/tools/default-tools.mdx 

 ---
title: Default Tools
subtitle: 'Adding Transfer Call, End Call, and Dial Keypad capabilities to your assistants.'
slug: tools/default-tools
---

Vapi voice assistants are given additional functions: `transferCall`,`endCall`, and `dtmf` (to dial a keypad with [DTMF](https://en.wikipedia.org/wiki/DTMF)). These functions can be used to transfer calls, hang up calls, and enter digits on the keypad.

<Note>You **need**  to add these tools to your model's `tools` array.</Note>

#### Transfer Call

This function is provided when `transferCall` is included in the assistant's list of available tools (see configuration options [here](/api-reference/assistants/create#request.body.model.openai.tools.transferCall)). This function can be used to transfer the call to any of the `destinations` defined in the tool configuration  (see details on destination options [here](/api-reference/assistants/create#request.body.model.openai.tools.transferCall.destinations)).

```json
{
  "model": {
    "provider": "openai",
    "model": "gpt-3.5-turbo",
    "messages": [
      {
        "role": "system",
        "content": "You are an assistant at a law firm. When the user asks to be transferred, use the transferCall function."
      }
    ],
    "tools": [
      {
          "type": "transferCall",
          "destinations" : {
            {
              "type": "number",
              "number": "+16054440129"
            }
          }
      }
    ]
  }
}
```

#### End Call

This function is provided when `endCall` is included in the assistant's list of available tools (see configuration options [here](/api-reference/assistants/create#request.body.model.openai.tools.endCall)). The assistant can use this function to end the call.

```json
{
  "model": {
    "provider": "openai",
    "model": "gpt-3.5-turbo",
    "messages": [
      {
        "role": "system",
        "content": "You are an assistant at a law firm. If the user is being mean, use the endCall function."
      }
    ],
    "tools": [
      {
          "type": "endCall"
      }
    ]
  }
}
```

#### Send Text

This function is provided when `sms` is included in the assistant's list of available tool (see configuration options [here](/api-reference/assistants/create#request.body.model.openai.tools.sms)). The assistant can use this function to send SMS messages using a configured Twilio account.

```json
{
  "model": {
    "provider": "openai",
    "model": "gpt-3.5-turbo",
    "messages": [
      {
        "role": "system",
        "content": "You are an assistant. When the user asks you to send a text message, use the sms function."
      }
    ],
    "tools": [
      {
        "type": "sms",
        "metadata": {
          "from": "+15551234567" 
        }
      }
    ]
  }
}
```

#### Dial Keypad (DTMF)

This function is provided when `dtmf` is included in the assistant's list of available tools (see configuration options [here](/api-reference/assistants/create#request.body.model.openai.tools.dtmf)). The assistant will be able to enter digits on the keypad.

```json
{
  "model": {
    "provider": "openai",
    "model": "gpt-3.5-turbo",
    "messages": [
      {
        "role": "system",
        "content": "You are an assistant at a law firm. When you hit a menu, use the dtmf function to enter the digits."
      }
    ],
    "tools": [
      {
          "type": "dtmf"
      }
    ]
  }
}
```

There are three methods for sending DTMF in a phone call:

1. **In-band**: tones are transmitted as part of the regular audio stream. This is the simplest method, but it can suffer from quality issues if the audio stream is compressed or degraded.
2. **Out-of-band via RFC 2833**: tones are transmitted separately from the audio stream, within RTP (Real-Time Protocol) packets. It's typically more reliable than in-band DTMF, particularly for VoIP applications where the audio stream might be compressed. RFC 2833 is the standard that initially defined this method. It is now replaced by RFC 4733 but this method is still referred by RFC 2833.
3. **Out-of-band via SIP INFO messages**: tones are sent as separate SIP INFO messages. While this can be more reliable than in-band DTMF, it's not as widely supported as the RFC 2833 method.

<Note>
Vapi's DTMF tool uses in-band method. Please note that this method may not work with certain IVRs. If you are running into this issue, the recommended approach is to have your assistant say the options out loud if available. For example, when an IVR says "Press 1 or say Sales for the Sales department," prefer having the assistant say "Sales."
</Note>

##### Tool Effectiveness

To evaluate this tool, we set up a Vapi assistant with the DTMF tool enabled and conducted calls to a range of IVR systems, including a Twilio IVR (configured via Studio Flows) and several third-party IVRs such as pharmacies and insurance companies.

**Testing Methodology**

We called and navigated through the IVRs using three different strategies:

1. **Direct Dialpad**: calling from a personal phone and dialing options using the dialpad.
2. **Vapi DTMF Tool**: an assistant configured with the DTMF tool.
3. **Manual DTMF Sound**: calling from a personal phone and playing DTMF tones generated by software. _(similar approach as the Vapi DTMF Tool)_

**Key Findings**

- The assistant successfully navigated some of the third-party IVRs.
- The assistant encountered issues with Twilio IVRs, likely due to Twilio’s preference for RFC 2833.
- Observed occasional delays in DTMF tone transmission, which may affect effectiveness with IVRs that have short timeouts.

**Conclusion**

The tool's effectiveness depends on the IVR system's configuration and DTMF capturing method. We are working to improve compatibility and reduce transmission delays for broader and more reliable support.


<Accordion title="Custom Functions: Deprecated">
### Custom Functions

<Warning>The **Custom Functions** feature is being deprecated in favor of [Tools](/tools-calling). Please refer to the **Tools** section instead. We're working on a solution to migrate your existing functions over to make this a seamless transtion.</Warning>

In addition to the predefined functions, you can also define custom functions. These functions are similar to OpenAI functions and your chosen LLM will trigger them as needed based on your instructions.

The functions array in the assistant definition allows you to define custom functions that the assistant can call during a conversation. Each function is an object with the following properties:

- `name`: The name of the function. It must be a string containing a-z, A-Z, 0-9, underscores, or dashes, with a maximum length of 64.
- `description`: A brief description of what the function does. This is used by the AI to decide when and how to call the function.
- `parameters`: An object that describes the parameters the function accepts. The type property should be "object", and the properties property should be an object where each key is a parameter name and each value is an object describing the type and purpose of the parameter.

Here's an example of a function definition:

```json
{
  "functions": [
    {
      "name": "bookAppointment",
      "description": "Used to book the appointment.",
      "parameters": {
        "type": "object",
        "properties": {
          "datetime": {
            "type": "string",
            "description": "The date and time of the appointment in ISO format."
          }
        }
      }
    }
  ]
}
```

In this example, the bookAppointment function accepts one parameter, `datetime`, which is a string representing the date and time of the appointment in ISO format.

In addition to defining custom functions, you can specify a `serverUrl` where Vapi will send the function call information. This URL can be configured at the account level or at the assistant level.
At the account level, the `serverUrl` is set in the Vapi Dashboard. All assistants under the account will use this URL by default for function calls.
At the assistant level, the `serverUrl` can be specified in the assistant configuration when creating or updating an assistant. This allows different assistants to use different URLs for function calls. If a `serverUrl` is specified at the assistant level, it will override the account-level Server URL.

If the `serverUrl` is not defined either at the account level or the assistant level, the function call will simply be added to the chat history. This can be particularly useful when you want a function call to trigger an action on the frontend.

For instance, the frontend can listen for specific function calls in the chat history and respond by updating the user interface or performing other actions. This allows for a dynamic and interactive user experience, where the frontend can react to changes in the conversation in real time.
</Accordion>

 This is the content for the doc fern/tools/google-calendar.mdx 

 ---
title: Google Calendar Integration
subtitle: 'Connect your assistant to Google Calendar for seamless appointment scheduling and availability checking.'
slug: tools/google-calendar
---

The Google Calendar integration allows your Vapi assistant to interact with Google Calendar in two ways:
1. Create calendar events through voice commands
2. Check calendar availability for scheduling

This enables your assistant to schedule appointments, meetings, and other calendar events directly during phone calls, as well as check when you're available for meetings.

## Prerequisites

Before you can use the Google Calendar integration, you need to:
1. Have a Google Calendar account
2. Have access to the Vapi Dashboard
3. Have an assistant created in Vapi

## Setup Steps

### 1. Connect Google Calendar Account

First, you need to connect your Google Calendar account to Vapi:

1. Navigate to the Vapi Dashboard
2. Go to **Providers Keys** > **Tools Provider** > **Google Calendar**
3. Click the **Connect** button
4. A Google authorization popup will appear
5. Follow the prompts to authorize Vapi to access your Google Calendar

<Note>
  The authorization process will request access to your Google Calendar to create events and check availability.
</Note>

<Frame caption="Connect Google Calendar">
  <img
    src="../static/images/tools/google-calendar-connect.png"
    alt="Select files from your Assistant"
  />
</Frame>

### 2. Create Calendar Tools

After connecting your Google Calendar account, create the tools:

1. Go to **Dashboard** > **Tools** page
2. Click the **Create Tool** button
3. Select **Google Calendar** from the available options
4. Choose which tool(s) you want to create:
   - Google Calendar Create Event Tool
   - Google Calendar Check Availability Tool
5. For each tool, provide a name and description explaining when it should be invoked

<Note>
  The description field is crucial as it helps the AI model understand when and how to use each tool. Be specific about the scenarios and conditions when each tool should be invoked.
</Note>

<Frame caption="Create Calendar Tools">
  <img
    src="../static/images/tools/google-calendar-create.png"
    alt="Tool Configuration"
  />
</Frame>

### 3. Add Tools to Assistant

Now, add your chosen calendar tool(s) to your assistant:

1. Navigate to **Dashboard** > **Assistants** page
2. Select your assistant
3. Go to the **Tools** tab
4. In the tools dropdown, select the calendar tool(s) you want to use
5. Click **Publish** to save your changes

<Frame caption="Add Tools to Assistant">
  <img
    src="../static/images/tools/assistant-select-google-calendar-tool.png"
    alt="Add Tools to Assistant"
  />
</Frame>

## Tool Configurations

### Google Calendar Create Event Tool

This tool uses the following fields to create events:

- `summary`: The title or description of the calendar event
- `startDateTime`: The start date and time of the event
- `endDateTime`: The end date and time of the event
- `timeZone`: The timezone for the event

### Google Calendar Check Availability Tool

This tool uses the following fields to check availability:

- `startDateTime`: The start of the time range to check
- `endDateTime`: The end of the time range to check
- `timeZone`: The timezone for the availability check

<Info>
  All datetime fields should be provided in ISO 8601 format.
</Info>

## Example Usage

Here's how the tools can be used in your assistant's configuration:

```json
{
  "model": {
    "provider": "openai",
    "model": "gpt-4o",
    "messages": [
      {
        "role": "system",
        "content": "You are a scheduling assistant. When users want to schedule an appointment, first check their availability using the Check Availability tool, then use the Create Event tool to schedule the event if they're available.\n\n- Gather date and time range to check availability.\n- To book an appointment, gather the purpose of the appointment, ex: general checkup, dental cleaning and etc.\n\nNotes\n- Use the purpose as summary for booking appointment.\n- Current date: {{date}}\n- Current time: {{time}}"
      }
    ],
    "tools": [
      {
        "type": "google.calendar.availability.check",
        "name": "checkAvailability",
        "description": "Use this tool to check calendar availability and use the America/Los_Angeles as default timezone."
      },
      {
        "type": "google.calendar.event.create",
        "name": "scheduleAppointment",
        "description": "Use this tool to schedule appointments and create calendar events. Notes: - Use America/Los_Angeles as default timezone - All appointments are 30 mins."
      }
    ]
  }
}
```

## Best Practices

1. **Clear Instructions**: Provide clear instructions in your assistant's system message about when to use each calendar tool
2. **Error Handling**: Include fallback responses for cases where either calendar tool fails
3. **Time Zone Awareness**: Always specify the correct timezone for events and availability checks
4. **Event Details**: Ensure all required fields are properly filled when creating events
5. **Availability Flow**: Check availability before attempting to schedule events to avoid conflicts

<CardGroup cols={2}>
  <Card
    title="Need Help?"
    icon="question-circle"
    href="https://discord.gg/pUFNcf2WmH"
  >
    Join our Discord community for support with Google Calendar integration
  </Card>
  <Card
    title="API Reference"
    icon="book"
    href="/api-reference/tools/create"
  >
    View the complete API documentation for tools
  </Card>
</CardGroup>


 This is the content for the doc fern/tools/google-sheets.mdx 

 ---
title: Google Sheets Integration
subtitle: 'Connect your assistant to Google Sheets for seamless data entry.'
slug: tools/google-sheets
---

The Google Sheets integration allows your Vapi assistant to interact with Google Sheets in a simple way:
1. Add new rows to existing Google Sheets

This enables your assistant to record information and add data to spreadsheets directly during phone calls.

<Note>
  The Google Sheets integration currently only supports adding new rows to spreadsheets. It does not support reading from or modifying existing data in the spreadsheet.
</Note>

## Prerequisites

Before you can use the Google Sheets integration, you need to:
1. Have a Google Sheets account
2. Have access to the Vapi Dashboard
3. Have an assistant created in Vapi
4. Have a Google Sheet created and ready to receive data

## Setup Steps

### 1. Connect Google Sheets Account

First, you need to connect your Google Sheets account to Vapi:

1. Navigate to the Vapi Dashboard
2. Go to **Providers Keys** > **Tools Provider** > **Google Sheets**
3. Click the **Connect** button
4. A Google authorization popup will appear
5. Follow the prompts to authorize Vapi to access your Google Sheets

<Note>
  The authorization process will request access to your Google Sheets.
</Note>

### 2. Create and Configure Sheets Tool

After connecting your Google Sheets account, create and configure the tool:

1. Go to **Dashboard** > **Tools** page
2. Click the **Create Tool** button
3. Select **Google Sheets** from the available options
4. Choose the Google Sheets Add Row Tool
5. Provide a name and description explaining when it should be invoked
6. Configure the tool with the following required fields:
   - `spreadsheetId`: The ID of your Google Sheet
   - `range`: The sheet name or range (e.g., "Sheet1" or "Sheet1!A:Z")

<Note>
  To find your spreadsheet ID:
  1. Open your Google Sheet in a browser
  2. Look at the URL: `https://docs.google.com/spreadsheets/d/SPREADSHEET_ID/edit`
  3. Copy the SPREADSHEET_ID portion (it's a long string of letters, numbers, and special characters)
</Note>

<Note>
  The description field is crucial as it helps the AI model understand when and how to use the tool. Be specific about the scenarios and conditions when the tool should be invoked.
</Note>

### 3. Add Tool to Assistant

Now, add the Google Sheets tool to your assistant:

1. Navigate to **Dashboard** > **Assistants** page
2. Select your assistant
3. Go to the **Tools** tab
4. In the tools dropdown, select the Google Sheets tool
5. Click **Publish** to save your changes

## Tool Configuration

### Google Sheets Add Row Tool

This tool uses the following fields to add data to your spreadsheet:

- `spreadsheetId`: The ID of your Google Sheet (found in the sheet's URL)
- `range`: The range where the data should be added (e.g., "Sheet1" or "Sheet1!A:Z")
- `values`: An array of values to be added as a new row

<Note>
  The range field can be specified in two ways:
  1. Just the sheet name (e.g., "Sheet1") - This will append to the next empty row
  2. Sheet name with range (e.g., "Sheet1!A:Z") - This will append to the specified range
</Note>

## Example Usage

Here's how the tool can be used in your assistant's configuration:

```json
{
  "model": {
    "provider": "openai",
    "model": "gpt-4",
    "messages": [
      {
        "role": "system",
        "content": "You are a customer feedback assistant. After each customer service call, collect feedback using the following process:\n\n1. Ask the customer if they would like to provide feedback\n2. If yes, ask for their rating (1-5 stars)\n3. Ask for specific comments about their experience\n4. Ask for any suggestions for improvement\n5. Confirm the feedback before adding it to the spreadsheet\n\nUse the Add Row tool to record the feedback with the following columns:\n- Timestamp\n- Rating (1-5)\n- Comments\n- Suggestions\n\nAlways be polite and thank the customer for their feedback."
      }
    ],
    "tools": [
      {
        "type": "google.sheets.row.append",
        "name": "addFeedback",
        "description": "Use this tool to add customer feedback to the feedback spreadsheet. Collect all required information (rating, comments, suggestions) before adding the row."
      }
    ]
  }
}
```

## Best Practices

1. **Data Validation**: Ensure all data is properly formatted before adding to the spreadsheet
2. **Error Handling**: Include fallback responses for cases where the tool fails
3. **User Confirmation**: Always confirm with the user before adding data to the spreadsheet
4. **Sheet Structure**: Be aware of the spreadsheet's structure and column requirements

<CardGroup cols={2}>
  <Card
    title="Need Help?"
    icon="question-circle"
    href="https://discord.gg/pUFNcf2WmH"
  >
    Join our Discord community for support with Google Sheets integration
  </Card>
  <Card
    title="API Reference"
    icon="book"
    href="/api-reference/tools/create"
  >
    View the complete API documentation for tools
  </Card>
</CardGroup> 

 This is the content for the doc fern/tools/introduction.mdx 

 ---
title: Introduction to Tools
subtitle: Extend your assistant's capabilities with powerful function calling tools.
slug: tools
---

[**Tools**](/api-reference/tools/create) allow your assistant to take actions beyond just conversation. They enable your assistant to perform tasks like transferring calls, accessing external data, or triggering actions in your application. Tools can be either built-in default tools provided by Vapi or custom tools that you create.

There are three types of tools available:

1. **Default Tools**: Built-in functions provided by Vapi for common operations like call transfers and control.
2. **Custom Tools**: Your own functions that can be called by the assistant to interact with your systems.
3. **Integration Tools**: Pre-built integrations with platforms like [Make](https://www.make.com/en/integrations/vapi) and GoHighLevel (GHL) that let you trigger automated workflows via voice.

<Info>
  Tools are configured as part of your assistant's model configuration. You can find the complete API reference [here](/api-reference/tools/create-tool).
</Info>

## Available Tools

<CardGroup cols={3}>
  <Card 
    title="Default Tools" 
    icon="gear" 
    href="/tools/default-tools"
  >
    Built-in tools for call control, transfers, and basic operations
  </Card>
  <Card 
    title="Custom Tools" 
    icon="screwdriver-wrench" 
    href="/tools/custom-tools"
  >
    Create your own tools to extend assistant capabilities
  </Card>
  <Card 
    title="Make & GHL Tools" 
    icon="puzzle-piece" 
    href="/tools/GHL"
  >
    Import Make scenarios and GHL workflows as voice-activated tools
  </Card>
</CardGroup>

## Integration Tools

With Make and GHL integrations, you can:
- Import existing Make scenarios and GHL workflows directly into Vapi
- Trigger automated workflows using voice commands
- Connect your voice AI to hundreds of apps and services
- Automate complex business processes through voice interaction

Common use cases include:
- Booking appointments via voice
- Updating CRM records during calls
- Triggering email or SMS follow-ups
- Processing orders and payments
- Managing customer support tickets

## Key Features

<CardGroup cols={2}>
  <Card 
    title="Function Calling" 
    icon="square-terminal"
  >
    Assistants can trigger functions based on conversation context
  </Card>
  <Card 
    title="Async Support" 
    icon="clock"
  >
    Tools can run synchronously or asynchronously
  </Card>
  <Card 
    title="Server Integration" 
    icon="server"
  >
    Connect tools to your backend via webhooks
  </Card>
  <Card 
    title="Error Handling" 
    icon="triangle-exclamation"
  >
    Built-in error handling and fallback options
  </Card>
</CardGroup>

## Learn More

<CardGroup cols={2}>
  <Card
    title="Make & GHL Integration Guide"
    icon="puzzle-piece"
    href="/tools/GHL"
  >
    Learn how to import and use Make scenarios and GHL workflows as voice-activated tools
  </Card>
  <Card
    title="Join Our Discord"
    icon="fa-brands fa-discord"
    color="#5A65EA"
    href="https://discord.gg/pUFNcf2WmH"
  >
    Get help with tool integrations from our community
  </Card>
</CardGroup> 

 This is the content for the doc fern/tools/mcp.mdx 

 ---
title: Model Context Protocol (MCP) Integration
subtitle: 'Connect your assistant to dynamic tools through MCP servers for enhanced capabilities.'
slug: tools/mcp
---

The Model Context Protocol (MCP) integration allows your Vapi assistant to dynamically access tools from MCP servers during calls. This enables your assistant to:

1. Connect to any MCP-compatible server
2. Access tools dynamically at runtime
3. Execute actions through the MCP server

This powerful integration allows your assistant to leverage a wide range of tools without requiring individual integrations for each service.

<Note>
  Vapi also provides its own MCP server that exposes Vapi APIs as callable tools. See the [Vapi MCP Server documentation](/sdk/mcp-server) to learn how to use it with Claude Desktop or custom applications.
</Note>

## Prerequisites

Before you can use the MCP integration, you need to:
1. Have access to the Vapi Dashboard
2. Have an assistant created in Vapi
3. Have access to an MCP server URL (e.g., from Zapier, Composio, or other MCP providers)

## Setup Steps

### 1. Obtain MCP Server URL

First, you need to obtain an MCP server URL from your chosen provider:

1. Sign up for an MCP-compatible service (e.g., Zapier, Composio)
2. Navigate to the MCP configuration section of your provider
3. Generate or copy your MCP server URL

<Note>
  For Zapier MCP, visit https://actions.zapier.com/settings/mcp/ to generate your MCP server URL. This URL should be treated as a credential and kept secure.
</Note>

### 2. Create and Configure MCP Tool

After obtaining your MCP server URL, create and configure the tool:

1. Go to **Dashboard** > **Tools** page
2. Click the **Create Tool** button
3. Select **MCP** from the available options
4. Provide a name and description explaining when it should be invoked
5. Configure the tool with the following required field:
   - `serverUrl`: The URL of your MCP server

<Note>
  The MCP server URL should be treated as a credential and kept secure. It will be used to authenticate requests to the MCP server.
</Note>

### 3. Add Tool to Assistant

Now, add the MCP tool to your assistant:

1. Navigate to **Dashboard** > **Assistants** page
2. Select your assistant
3. Go to the **Tools** tab
4. In the tools dropdown, select your MCP tool
5. Click **Publish** to save your changes

## How MCP Works

The MCP integration follows these steps during a call:

1. When a call starts, Vapi connects to your configured MCP server
2. The MCP server returns a list of available tools and their capabilities
3. These tools are dynamically added to your assistant's available tools
4. The assistant can then use these tools during the call
5. When a tool is invoked, Vapi sends the request to the MCP server
6. The MCP server executes the action and returns the result

<Note>
  The MCP tool itself is not meant to be invoked by the model. It serves as a configuration mechanism for Vapi to fetch and inject the specific tool definitions from the MCP server into the model's context.
</Note>

<Note>
  The tools available through MCP are determined by your MCP server provider. Different providers may offer different sets of tools.
</Note>

## Tool Configuration

### MCP Tool

This tool uses the following field to connect to your MCP server:

- `serverUrl`: The URL of your MCP server (e.g., https://actions.zapier.com/mcp/actions/)

<Note>
  The server URL should be treated as a credential and kept secure. It will be used to authenticate requests to the MCP server.
</Note>

## Example Usage

Here's how the MCP tool can be used in your assistant's configuration:

```json
{
  "model": {
    "provider": "openai",
    "model": "gpt-4",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful personal assistant named Alex. You can help users with various tasks through voice commands. You have access to tools that allow you to perform actions on the user's behalf.\n\nWhen a user requests an action, check if any of the available tools can help accomplish that task.\n\nCommon tasks you can help with include:\n- Scheduling appointments and meetings\n- Sending messages or emails\n- Creating or updating documents\n- Managing to-do lists and reminders\n- Searching for information\n- Making reservations\n- Ordering food or services\n- Checking account balances or transaction history\n- Controlling smart home devices\n\nAlways be polite, professional, and helpful. If a tool fails or isn't available, explain the situation to the user and suggest alternatives if possible."
      }
    ],
    "tools": [
      {
        "type": "mcp",
        "name": "mcpTools",
        "server": {
            "url": "https://actions.zapier.com/mcp/actions/"
        }
      }
    ]
  }
}
```

## Best Practices

1. **Dynamic Tool Awareness**: Be aware that the available tools may change between calls
2. **Clear Instructions**: Provide clear instructions in your assistant's system message about how to handle dynamic tools
3. **Error Handling**: Include fallback responses for cases where tools fail or are unavailable
4. **User Communication**: Explain to users what tools you're using and what actions you're taking
5. **Security**: Treat the MCP server URL as a credential and keep it secure

## Example MCP Providers

### Zapier MCP

Zapier offers an MCP server that provides access to thousands of app integrations:

1. Go to https://actions.zapier.com/settings/mcp/
2. Generate your MCP server URL
3. Add the URL to your MCP tool configuration
4. Your assistant will now have access to Zapier's extensive integration network

<Note>
  Zapier MCP provides access to over 7,000+ apps and 30,000+ actions without requiring complex API integrations.
</Note>

## References

- [Model Context Protocol Introduction](https://modelcontextprotocol.io/introduction)
- [Zapier MCP](https://zapier.com/mcp)

<CardGroup cols={2}>
  <Card
    title="Need Help?"
    icon="question-circle"
    href="https://discord.gg/pUFNcf2WmH"
  >
    Join our Discord community for support with MCP integration
  </Card>
  <Card
    title="API Reference"
    icon="book"
    href="/api-reference/tools/create"
  >
    View the complete API documentation for tools
  </Card>
</CardGroup> 

 This is the content for the doc fern/tools/slack.mdx 

 ---
title: Slack Integration
subtitle: 'Connect your assistant to Slack for seamless message sending.'
slug: tools/slack
---

The Slack integration allows your Vapi assistant to send messages to a pre-configured Slack channel during phone calls. This enables your assistant to notify team members, send updates, or share information directly through Slack.

## Prerequisites

Before you can use the Slack integration, you need to:
1. Have a Slack workspace
2. Have access to the Vapi Dashboard
3. Have an assistant created in Vapi
4. Have a Slack channel created where messages will be sent

## Setup Steps

### 1. Connect Slack Account

First, you need to connect your Slack workspace to Vapi:

1. Navigate to the Vapi Dashboard
2. Go to **Providers Keys** > **Tools Provider** > **Slack**
3. Click the **Connect** button
4. A Slack authorization popup will appear
5. Follow the prompts to authorize Vapi to access your Slack workspace

<Note>
  The authorization process will request access to send messages to your Slack workspace.
</Note>

### 2. Create Slack Tool

After connecting your Slack workspace, create the tool:

1. Go to **Dashboard** > **Tools** page
2. Click the **Create Tool** button
3. Select **Slack** from the available options
4. Choose the Slack Send Message Tool
5. Provide a name and description explaining when it should be invoked
6. Configure the tool with the following required fields:
   - `channel`: The name of the Slack channel where messages will be sent (e.g., "#general" or "#notifications")

<Note>
  The description field is crucial as it helps the AI model understand when and how to use the tool. Be specific about the scenarios and conditions when the tool should be invoked.
</Note>

### 3. Add Tool to Assistant

Now, add the Slack tool to your assistant:

1. Navigate to **Dashboard** > **Assistants** page
2. Select your assistant
3. Go to the **Functions** tab
4. In the tools dropdown, select the Slack tool
5. Click **Publish** to save your changes

## Tool Configuration

### Slack Send Message Tool

This tool uses the following fields to send messages to Slack:

- `channel`: The name of the Slack channel where the message will be sent
- `text`: The message content to be sent to the channel

<Note>
  The channel name should be specified in the format "#channel-name". Make sure the bot has been added to the channel before sending messages.
</Note>

## Example Usage

Here's how the tool can be used in your assistant's configuration:

```json
{
  "model": {
    "provider": "openai",
    "model": "gpt-4",
    "messages": [
      {
        "role": "system",
        "content": "You are a customer service assistant. When a customer requests a callback or needs urgent attention, use the Slack tool to notify the support team in the #customer-support channel. Include the following information in your message:\n\n- Customer name\n- Phone number\n- Reason for callback/urgent attention\n- Any specific time constraints\n\nAlways be professional and concise in your Slack messages."
      }
    ],
    "tools": [
      {
        "type": "slack.message.send",
        "name": "notifySupport",
        "description": "Use this tool to send urgent notifications to the support team in the #customer-support channel. Only use this when a customer needs immediate attention or requests a callback."
      }
    ]
  }
}
```

## Best Practices

1. **Channel Selection**: Always verify the correct channel name before sending messages
2. **Message Formatting**: Use clear and concise language in your Slack messages
3. **Error Handling**: Include fallback responses for cases where the tool fails
4. **User Confirmation**: Always confirm with the user before sending notifications to Slack
5. **Channel Access**: Ensure the Slack bot has been added to the target channel

<CardGroup cols={2}>
  <Card
    title="Need Help?"
    icon="question-circle"
    href="https://discord.gg/pUFNcf2WmH"
  >
    Join our Discord community for support with Slack integration
  </Card>
  <Card
    title="API Reference"
    icon="book"
    href="/api-reference/tools/create"
  >
    View the complete API documentation for tools
  </Card>
</CardGroup> 

 This is the content for the doc fern/voice-fallback-plan.mdx 

 ---
title: Voice Fallback Plan
subtitle: Configure fallback voices that activate automatically if your primary voice fails.
slug: voice-fallback-plan
---

<Note>
  Voice fallback plans can currently only be configured through the API. We are working on making this available through our dashboard.
</Note>

## Introduction

Voice fallback plans give you the ability to continue your call in the event that your primary voice fails. Your assistant will sequentially fallback to only the voices you configure within your plan, in the exact order you specify. 

<Note>
  Without a fallback plan configured, your call will end with an error in the event that your chosen voice provider fails.
</Note>

## How It Works

When a voice failure occurs, Vapi will:
1. Detect the failure of the primary voice
2. If a custom fallback plan exists:
  - Switch to the first fallback voice in your plan
  - Continue through your specified list if subsequent failures occur
  - Terminate only if all voices in your plan have failed

## Configuration

Add the `fallbackPlan` property to your assistant's voice configuration, and specify the fallback voices within the `voices` property.
- Please note that fallback voices must be valid JSON configurations, and not strings.
- The order matters. Vapi will choose fallback voices starting from the beginning of the list.

```json
{
  "voice": {
    "provider": "openai",
    "voiceId": "shimmer",
    "fallbackPlan": {
        "voices": [
            {
                "provider": "cartesia",
                "voiceId": "248be419-c632-4f23-adf1-5324ed7dbf1d"
            },
            {
                "provider": "playht",
                "voiceId": "jennifer"
            }
        ]
    }
  }
}
```

## Best practices

- Use <b>different providers</b> for your fallback voices to protect against provider-wide outages.
- Select voices with **similar characteristics** (tone, accent, gender) to maintain consistency in the user experience.

## How will pricing work?

There is no change to the pricing of the voices. Your call will not incur any extra fees while using fallback voices, and you will be able to see the cost for each voice in your end-of-call report.


 This is the content for the doc fern/welcome.mdx 

 ---
title: "👋 Welcome to Vapi Docs"
layout: overview
---

Vapi helps developers build, test, and deploy voice agents at scale. We enable everything in between the raw models and production, including telephony, test suites, and real-time analytics.

<br />

<CardGroup cols={2}>
  <Card
    title="Quickstart (<5 minutes)"
    icon="clock"
    href="/quickstart/dashboard"
  >
    Get started now with the Vapi Dashboard. 
  </Card>

  <Card title="Documentation" icon="book" href="introduction">
    Learn how to use Vapi's Voice AI platform. 
  </Card>

  <Card title="Discord" icon="fa-brands fa-discord" href="https://discord.gg/pUFNcf2WmH">
    Connect with 11,833 other Vapi developers.
  </Card>

  <Card title="GitHub" icon="fa-brands fa-github" href="https://github.com/VapiAI">
    Check out our public SDKs. 
  </Card>

</CardGroup>



 This is the content for the doc fern/workflows.mdx 

 ---
title: Introduction to Workflows
subtitle: Break down AI conversations into a visual workflow made up of discrete steps ("nodes") and branches between them ("edges").
slug: workflows
---

<Success intent="launch">
  Workflows is now available to all Vapi users in Open Beta on [the dashboard here](https://dashboard.vapi.ai/workflows). Start building more reliable and structured conversational AI today. 
</Success>

Workflows is a new way to build conversational AI. It allows you to break down AI conversations into discrete steps, and then orchestrate those steps in a way that is easy to manage and modify.

## Creating Your First Workflow

Begin by creating an assistant on the Assistants page and providing the required information, such as the assistant's name and capabilities. Once your assistant is set up, switch the model provider to Vapi and click "Create Workflow" when prompted. A modal will appear offering you the option to create a new workflow or attach to an existing one. Choose the appropriate option to proceed to the Workflow Builder.

<Steps>
  ### Step 1: Create an Assistant
    Visit the Assistants page. Create a new assistant, give it a name, and select a voice and transcription model of your choice.
  
  ### Step 2: Switch Provider to `vapi`
    Under the "Model" section, switch the "Provider" field to `vapi`.
  
  ### Step 3: Create a New Workflow or Attach an Existing One
    Click the "Create Workflow" button. A prompt will appear asking you to create a new workflow by entering a unique title, or attach to an existing workflow.
    
  ### Step 4: Build Your Workflow
    In the Workflow Builder, you will see a "Start" call node. Click the + button at the bottom of this node to select your first **node**. Use the + button to add further steps as needed. 
  
  <Frame background="subtle">
    <img src="./static/images/workflows/workflow-builder-example.png" alt="Workflow Builder Interface" />
  </Frame>

  ### Step 5: Create Connections
    To create new connections between nodes, drag a line from one step's top connection dot to another step's bottom dot, forming the logical flow of the conversation.
</Steps>

## Tips for Building Workflows

- **Deleting Nodes and Edges:** Click on any node or edge and press Backspace to delete it.
- **Attaching Nodes:** Attach a node to another by drawing a line from the top of one node to the bottom of another node.
- **Save Requirements:** A workflow cannot be saved until every node is connected and configured. The system will not allow saving with any dangling nodes.
- **Creating Conditionals:** To create conditionals, first add a condition node. Then, attach nodes for each branch by clicking the "Logic" tag on the connecting edges to set up the conditions.

<Info>
    Please let us know about any bugs you find by [submitting a bug report](https://roadmap.vapi.ai/bug-reports). We also welcome feature requests and suggestions - you can [submit those here](https://roadmap.vapi.ai/feature-requests). For discussions about workflows and our product roadmap, please [join our Discord community](https://discord.com/invite/pUFNcf2WmH) to connect with our team.
</Info>

## Nodes

Workflows break down your AI voice agent's behavior into discrete, manageable nodes. Each node encapsulates a specific function within the conversation flow. Detailed configuration options let you tailor each step to your requirements. The available nodes are:

<CardGroup cols={2}>
  <Card title="Say" icon="message" href="/workflows/nodes/say">
    Outputs a message to the user without expecting a response. Configure this **node** by specifying static text or providing a prompt for the LLM to generate dynamic text.
  </Card>
  <Card title="Gather" icon="microphone" href="/workflows/nodes/gather">
    Collects input from the user. Define the variables by specifying a name, a detailed description of the expected input, and the data type (string, number, or boolean). Mark each variable as required or optional.
  </Card>
  <Card title="API Request" icon="code" href="/workflows/nodes/api-request">
    Makes calls to external APIs using GET or POST methods. Configure request headers and body, and define extraction rules to capture specific data from the JSON response. Optionally, enable asynchronous execution so that the workflow proceeds while awaiting the API response.
  </Card>
  <Card title="Transfer" icon="phone-arrow-right" href="/workflows/nodes/transfer">
    Transfers the active call to an external phone number. Ensure you provide a valid phone number in the configuration.
  </Card>
  <Card title="Hangup" icon="phone-slash" href="/workflows/nodes/hangup">
    Terminates the call, signaling the end of the conversation.
  </Card>
</CardGroup>

## Edges

Edges allow you to create branching paths in your workflow based on different types of logic:

<CardGroup cols={2}>
  <Card title="Logical Condition" icon="code-branch" href="/workflows/edges/logical-conditions">
    Introduces branching logic based on conditions. Set up logical comparisons using data previously gathered or returned from API requests. This node allows you to define different paths for the conversation.
  </Card>
</CardGroup>

For detailed configuration instructions and advanced settings, please refer to our dedicated documentation pages for each task.


 This is the content for the doc fern/workflows/edges/ai-conditions.mdx 

 ---
title: AI Conditions
subtitle: Smart workflow branching powered by AI
slug: /workflows/edges/ai-conditions
---

## Overview

AI Conditions use artificial intelligence to decide the next step in your workflow based on the conversation. Instead of using fixed rules, they can understand complex situations and make smart decisions in real-time.

## How It Works

1. The AI looks at the conversation history and context
2. It makes a smart decision about which path to take, based on variables collected from Gather verbs and data returned from API requests.
3. Works alongside your existing rules for maximum flexibility


## Configuration
- **Condition Node:** Start by inserting a condition node into your workflow.
- **Branch Setup:** Attach one or more nodes to the condition node.
- **AI Tag:** Click on the connecting edge and choose `AI` from the `Condition Type` dropdown
- **AI Condition** Use the input to define when the chosen branch should be taken.

## Usage

Use AI Conditions when you need:
- To handle unclear or complex user responses
- More flexibility than traditional rules can provide
- More natural, human-like conversations


 This is the content for the doc fern/workflows/edges/logical-conditions.mdx 

 ---
title: Logical Conditions
subtitle: Branching logic for dynamic workflows
slug: /workflows/edges/logical-conditions
---

## Overview

Logical Conditions enable you to create branching paths within your workflow. This feature allows your voice agent to decide the next steps based on data gathered earlier or retrieved via API calls.

## Configuration

- **Condition Node:** Start by inserting a condition node into your workflow.
- **Branch Setup:** Attach one or more nodes to the condition node.
- **Logic Tag:** Click the "Logic" tag on each connecting edge and select `Logic` from the `Condition Type` dropdown.
- **Condition Type:** Choose between requiring ALL conditions to be met (AND logic) or ANY condition to be met (OR logic)
- **Logic Conditions** Use the panel to define one or more rules or comparisons (e.g., equals, greater than) using variables collected from previous steps. 

<Note>
To remove a comparison, click on the Trash icon to the right of the comparison. 
</Note>

<Frame>
  <img src="../../static/images/workflows/logic-condition.png" />
</Frame>

## Usage

Implement Logical Conditions to guide your conversation dynamically. They allow your workflow to adjust its path based on real-time data, ensuring more personalized and responsive interactions.

<Note>
When  [`Gathering`](/workflows/nodes/gather) string values that will be used in conditions, consider using `enum` types to ensure consistent value comparison. This helps prevent issues with case sensitivity, whitespace, or formatting differences that could affect condition evaluation.
</Note>

 This is the content for the doc fern/workflows/nodes/api-request.mdx 

 ---
title: API Request
subtitle: Interface with external APIs
slug: /workflows/nodes/api-request
---

## Overview

The **API Request** enables your workflow to interact with external APIs. It supports both GET and POST methods, allowing the integration of external data and services.

## Configuration

- **URL:** Enter the endpoint to request.
- **Method:** Specify the HTTP method (GET or POST).
- **Headers:** Define each header with a key, value, and type.
- **Body Values:** For POST requests, provide key, value, and type for each entry.
- **Output Values:** Extract data from the API's JSON response:
  - **Key:** The key within the JSON payload to extract.
  - **Target:** The name of the output variable for the extracted value.
  - **Type:** The data type of the extracted value.
- **Mode:** Toggle asynchronous execution with "run in the background" on or off.

## Usage

Use the API Request to fetch information from an external API to use in your workflow, or to update information in your CRM or database.

1. **HTTP Configuration**: Enter the URL you want the workflow to call and select the HTTP method.
2. **API Metadata**: Specify key-value pairs for Headers and Body (for POST requests). This allows you to include authentication tokens and payload data with your API request.
3. **Define Output**: Set the expected output schema for the API response. This schema is used to extract variables that can be utilized later in your workflow.

For example, if the expected API response is
```jsx
{
    "name": "Jaden Dearsley",
    "age": 25,
    "isActive": true,
}
```

Define an output JSON schema for the API request with
```jsx
{
  ...
  "output": {
    "type": "object",
    "properties": {
      "name": {
        "type": "string",
        "description": "name of the user",
      },
      "age": {
        "type": "number",
        "description": "age of the user",
      },
      "isActive": {
        "type": "boolean",
        "description": "whether the user is active",
      }
    }
  },
}
```

This will make `name`, `age`, and `isActive` available as variables for use throughout the rest of the workflow. To rename a variable, use the `target` option to specify a different variable name.

```jsx
{
  ...
  "output": {
    "type": "object",
    "properties": {
      "name": {
        "type": "string",
        "description": "name of the user",
        "target": "user_name" // renamed "name" to "user_name"
      },
      ...
    }
  },
}
```

If your expected output has a complex or nested structure, we support the full JSON schema through the API. Refer to the API documentation on [`output`](/api-reference/workflow/workflow-controller-create#request.body.nodes.apiRequest.output) for more details.

 This is the content for the doc fern/workflows/nodes/assistant.mdx 

 ---
title: Assistant
subtitle: Speak to a configured assistant
slug: /workflows/nodes/assistant
---

## Overview

The **Assistant** node enables a persistent conversation with one of your configured assistants.

## Configuration

- **Select an Assistant** Use the dropdown to select a pre-configured assistant.

## Usage

Add **Assistant** nodes as leaf nodes to enable ongoing conversations with your configured assistants. Currently, Assistant nodes must be placed at the end of a workflow branch, as they don't support transitioning to other nodes. This means the conversation with the assistant will continue until either the user ends the call or the assistant reaches a natural conclusion point.

The assistant will use its configured system prompt while inheriting the transcriber and voice settings from the global workflow assistant.

<Note>
Assistant nodes are currently designed as terminal nodes - they cannot be connected to other nodes in the workflow. This means the conversation will remain with the assistant until the call ends. Future updates will add support for AI-powered conditional branching.
</Note>

 This is the content for the doc fern/workflows/nodes/gather.mdx 

 ---
title: Gather
subtitle: Collect input from users
slug: /workflows/nodes/gather
---

## Overview

The **Gather** collects input from users during an interaction. It is used to capture variables that will be referenced later in your workflow.

## Configuration

Define one or more variables to gather from the user with:
- **Name:** A unique identifier.
- **Description:** Details about the expected input to help the LLM get the right information from the caller.
- **Data Type:** String, number, boolean, enum.
- **Required:** Mark whether the variable is required or optional.

## Usage

Use **Gather** to extract specific details from user responses—such as their name, email, or ZIP code—to inform subsequent steps in your conversation. The Gather node doesn't directly prompt users; instead, it analyzes the conversation history to find the requested information and will ask follow-up questions if the user's response isn't clear. Make sure to precede it with a [`Say`](/workflows/nodes/say) node that explicitly prompts the user for the information you want to gather.

<Note>
To use an extracted string variable in a [`Conditional`](/workflows/edges/logical-conditions) branch, we recommend using the `enum` option. This ensures the extracted value will reliably match your conditions later in the workflow.
</Note>

 This is the content for the doc fern/workflows/nodes/hangup.mdx 

 ---
title: Hangup
subtitle: Terminate the call
slug: /workflows/nodes/hangup
---

## Overview

The **Hangup** ends an active call, marking the conclusion of a conversation. It is typically used as the final step in your workflow.

## Configuration

This task requires little to no configuration, as its purpose is solely to terminate the call.

## Usage

Place **Hangup** at the end of your workflow to close the conversation gracefully. 

 This is the content for the doc fern/workflows/nodes/say.mdx 

 ---
title: Say
subtitle: Output a message to the user
slug: /workflows/nodes/say
---

## Overview

The **Say** outputs a spoken message to the user without expecting a response. Use this task to provide instructions, notifications, or other information during a call.

## Configuration

- **Exact Message:** Specify the exact text that should be spoken.
- **Prompt for LLM Generated Message:** Provide a prompt for the language model to generate the message dynamically.

## Usage

Add **Say** when you need to deliver clear, concise information to your user. It works well as an initial greeting or when confirming actions. 

 This is the content for the doc fern/workflows/nodes/transfer.mdx 

 ---
title: Transfer
subtitle: Redirect calls to an external number
slug: /workflows/nodes/transfer
---

## Overview

The **Transfer** transfers an active call to a designated external phone number. This enables routing calls to other departments or external contacts.

## Configuration

- **Phone Number:** Enter a valid destination number for the call transfer.

## Usage

Use **Transfer** to escalate or redirect a call as needed. Ensure the provided phone number is formatted correctly for a smooth transfer experience. 