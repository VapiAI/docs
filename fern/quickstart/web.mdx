---
title: Web integration
subtitle: Integrate voice calls into your web application
slug: quickstart/web
---

## Overview

This guide shows you how to integrate live, two-way voice conversations into any web application. Use Vapi with plain JavaScript, React, Next.js, or any other web framework to add voice capabilities directly to your app.

**In this guide, you'll learn to:**
- Install and configure the Vapi Web SDK
- Connect to existing assistants from your dashboard
- Handle call lifecycle events in your application

<Note>See the full Next.js [demo here on v0](https://v0.dev/chat/vapi-quickstart-nextjs-z3lv02T7Dd5). To try it live and make edits, follow these steps:</Note>

1. Fork the app in v0
2. Go to settings â†’ environment variables 
3. Create a new environment variable called `NEXT_PUBLIC_VAPI_API_KEY`
4. Add your [public API key from the dashboard](https://dashboard.vapi.ai/org/api-keys)

## Installation

<Steps>
  ### Install the SDK
    <Markdown src="../snippets/sdks/web/install-web-sdk.mdx" />

  ### Import the SDK
    <Markdown src="../snippets/sdks/web/import-web-sdk.mdx" />
</Steps>

## Integration approaches

<Tabs>
  <Tab title="Using dashboard assistants">
    <Steps>
      <Step title="Create an assistant in the dashboard">
        First, create and configure an assistant in the [Vapi dashboard](https://dashboard.vapi.ai). Follow the [Phone calls quickstart](/quickstart/phone) to set up your assistant.
      </Step>
      <Step title="Get your assistant ID">
        Copy your assistant's ID from the dashboard:

        <Frame background="subtle">
          <img src="/static/images/quickstart/assistant-id-dashboard.png" alt="Assistant ID in dashboard" />
        </Frame>
      </Step>
      <Step title="Start a call with your assistant">
        ```javascript
        // Start a call with your pre-configured assistant
        vapi.start("YOUR_ASSISTANT_ID_FROM_THE_DASHBOARD");
        ```
      </Step>
      <Step title="Override assistant settings (optional)">
        Customize settings or pass template variables at runtime:

        ```javascript
        const assistantOverrides = {
          transcriber: {
            provider: "deepgram",
            model: "nova-2",
            language: "en-US",
          },
          recordingEnabled: false,
          variableValues: {
            customerName: "John",
            accountType: "premium"
          },
        };

        vapi.start("YOUR_ASSISTANT_ID", assistantOverrides);
        ```
      </Step>
    </Steps>
  </Tab>
  <Tab title="Inline assistant configuration">
    <Steps>
      <Step title="Define your assistant">
        Create an assistant configuration directly in your code:

        ```javascript
        const assistantOptions = {
          name: "Customer Support Assistant",
          firstMessage: "Hi! How can I help you today?",
          transcriber: {
            provider: "deepgram",
            model: "nova-2",
            language: "en-US",
          },
          voice: {
            provider: "playht",
            voiceId: "jennifer",
          },
          model: {
            provider: "openai",
            model: "gpt-4",
            messages: [
              {
                role: "system",
                content: `You are a helpful customer support assistant. Keep responses brief and friendly since this is a voice conversation.`,
              },
            ],
          },
        };
        ```
      </Step>
      <Step title="Start the call">
        ```javascript
        vapi.start(assistantOptions);
        ```
      </Step>
    </Steps>
  </Tab>
</Tabs>

## Handle call events

Listen to call lifecycle events to update your UI and handle user interactions:

```javascript
// Call started
vapi.on('call-start', () => {
  console.log('Call has started');
  // Update UI to show call is active
});

// Call ended
vapi.on('call-end', () => {
  console.log('Call has ended');
  // Reset UI state
});

// Message received from assistant
vapi.on('message', (message) => {
  console.log('Assistant said:', message.content);
  // Display assistant's response in UI
});

// Speech recognition results
vapi.on('speech-start', () => {
  console.log('User started speaking');
});

vapi.on('speech-end', () => {
  console.log('User stopped speaking');
});
```

## Common integration patterns

### Voice button component
Add a simple "Talk to Assistant" button:

```javascript
const VoiceButton = () => {
  const [isCallActive, setIsCallActive] = useState(false);
  
  const toggleCall = () => {
    if (isCallActive) {
      vapi.stop();
    } else {
      vapi.start("your-assistant-id");
    }
  };
  
  useEffect(() => {
    vapi.on('call-start', () => setIsCallActive(true));
    vapi.on('call-end', () => setIsCallActive(false));
  }, []);
  
  return (
    <button onClick={toggleCall}>
      {isCallActive ? 'End Call' : 'Start Voice Chat'}
    </button>
  );
};
```

### Context-aware conversations
Pass dynamic data to your assistant based on the current page or user state:

```javascript
const startContextualCall = (userContext) => {
  const assistantOverrides = {
    variableValues: {
      userName: userContext.name,
      currentPage: window.location.pathname,
      userPreferences: userContext.preferences
    }
  };
  
  vapi.start("your-assistant-id", assistantOverrides);
};
```

## Next steps

- **Phone integration:** Enable phone calling with the [Phone calls guide](/quickstart/phone)
- **Custom tools:** Connect your assistant to your APIs with [Custom tools](/tools/custom-tools)
- **Advanced features:** Explore [Variables](/assistants/dynamic-variables) and [Hooks](/assistants/assistant-hooks)
