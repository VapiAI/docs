# Evaluation System Foundation

1. **Evaluation Framework**: You can now systematically test your Vapi voice assistants with the new [`Eval`](https://api.vapi.ai/api#:~:text=Eval) system. Create comprehensive test scenarios to validate assistant behavior, conversation flow, and tool usage through mock conversations.

2. **Mock Conversation Builder**: Design test conversations using [`Eval.messages`](https://api.vapi.ai/api#:~:text=Eval.messages) with support for multiple message types:
    - [`ChatEvalUserMessageMock`](https://api.vapi.ai/api#:~:text=ChatEvalUserMessageMock): Simulate user inputs and questions
    - [`ChatEvalSystemMessageMock`](https://api.vapi.ai/api#:~:text=ChatEvalSystemMessageMock): Inject system messages mid-conversation
    - [`ChatEvalToolResponseMessageMock`](https://api.vapi.ai/api#:~:text=ChatEvalToolResponseMessageMock): Mock tool responses for consistent testing
    - [`ChatEvalAssistantMessageEvaluation`](https://api.vapi.ai/api#:~:text=ChatEvalAssistantMessageEvaluation): Define evaluation checkpoints

3. **Evaluation Types**: Currently focused on `chat.mockConversation` type evaluations, with the framework designed to support additional evaluation methods in future releases.

4. **Evaluation Management**: Organize your tests with [`CreateEvalDTO`](https://api.vapi.ai/api#:~:text=CreateEvalDTO) and [`UpdateEvalDTO`](https://api.vapi.ai/api#:~:text=UpdateEvalDTO):
    - `name`: Descriptive names up to 80 characters (e.g., "Customer Support Flow Validation")
    - `description`: Detailed descriptions up to 500 characters explaining the test purpose
    - `messages`: The complete mock conversation flow

5. **Evaluation Endpoints**: Access your evaluations through the new [`/eval`](https://api.vapi.ai/api#:~:text=/eval) endpoint family:
    - `GET /eval`: List all evaluations with pagination support
    - `POST /eval`: Create new evaluations
    - `GET /eval/{id}`: Retrieve specific evaluation details
    - `PUT /eval/{id}`: Update existing evaluations

6. **Judge Plan Architecture**: Define how assistant responses are validated using [`AssistantMessageJudgePlan`](https://api.vapi.ai/api#:~:text=AssistantMessageJudgePlan) with three evaluation methods:
    - **Exact Match**: [`AssistantMessageJudgePlanExact`](https://api.vapi.ai/api#:~:text=AssistantMessageJudgePlanExact) for precise content and tool call validation
    - **Regex Pattern**: [`AssistantMessageJudgePlanRegex`](https://api.vapi.ai/api#:~:text=AssistantMessageJudgePlanRegex) for flexible pattern-based evaluation
    - **AI Judge**: [`AssistantMessageJudgePlanAI`](https://api.vapi.ai/api#:~:text=AssistantMessageJudgePlanAI) for intelligent evaluation using LLM-as-a-judge

<Info>
    This is the foundation release for the evaluation system. Evaluation execution and results processing will be available in upcoming releases. Start designing your test scenarios now to be ready for full evaluation capabilities.
</Info>

## Testing Capabilities
<CardGroup cols={2}>
  <Card title="Mock Conversations" icon="message-square">
    Create realistic test scenarios with user messages, system prompts, and expected assistant responses for comprehensive flow validation.
  </Card>
  <Card title="Tool Call Testing" icon="wrench">
    Validate that your assistant calls the right tools with correct parameters using <code>ChatEvalAssistantMessageMockToolCall</code>.
  </Card>
  <Card title="Flexible Validation" icon="check-circle">
    Choose from exact matching, regex patterns, or AI-powered evaluation to suit different testing needs and complexity levels.
  </Card>
  <Card title="Evaluation Organization" icon="folder">
    Organize tests with descriptive names and detailed documentation to maintain clear testing workflows across your team.
  </Card>
</CardGroup>