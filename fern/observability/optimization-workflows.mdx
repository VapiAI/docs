---
title: Optimization workflows
subtitle: Use observability data to continuously improve your assistant
slug: observability/optimization-workflows
---

## What is optimization?

**Optimization** is the continuous improvement loop: using observability data to refine prompts, improve tool calls, and enhance conversation flows.

Unlike the previous stages (INSTRUMENT, TEST, EXTRACT, MONITOR), **OPTIMIZE is not a dedicated tool or feature** — it's a workflow that combines tools from all previous stages to drive systematic improvement.

**The optimization mindset**: Voice AI quality improves through iteration, not perfection. The best teams:
- Start with "good enough" (not perfect)
- Deploy to production with instrumentation and monitoring
- Use real-world data to identify improvement opportunities
- Test changes before deploying
- Track impact systematically

**Why optimization matters**: Without a systematic optimization workflow, teams either:
- ❌ Over-engineer before launch (trying to predict every edge case)
- ❌ React to problems ad-hoc (fixing symptoms, not root causes)
- ❌ Stagnate after launch (no process for continuous improvement)

**The goal**: Establish a repeatable workflow that turns observability data into measurable improvements.

---

## Optimization workflow at a glance

| Stage | Tools Used | What you do |
|-------|-----------|-------------|
| **1. Detect patterns** | Boards, Insights API, Analytics API | Spot trends in monitoring dashboards (success rate dropping, cost increasing, etc.) |
| **2. Extract details** | Webhooks, Structured Outputs, Transcripts | Pull call data to understand WHY the pattern exists |
| **3. Form hypothesis** | Manual analysis | Identify root cause (e.g., "prompt doesn't handle edge case X") |
| **4. Make changes** | Assistant configuration | Update prompts, tools, routing logic based on hypothesis |
| **5. Test changes** | Evals, Simulations | Validate improvement before deploying to production |
| **6. Deploy** | API, Dashboard | Push updated assistant to production |
| **7. Verify** | Boards, Insights API | Track target metric to confirm improvement |

This is a **continuous cycle**, not a one-time activity:

```
MONITOR → EXTRACT → Analyze → Revise → TEST → Deploy → MONITOR (repeat)
```

<span className="vapi-validation">Confirm this optimization workflow accurately reflects how Vapi customers typically iterate on their assistants. Are there tools or stages we're missing? Should we emphasize certain steps more than others?</span>

---

## The optimization loop in detail

**[Placeholder - Full detail sections]**

### Step 1: Detect patterns from monitoring

<span className="internal-note">Placeholder for: How to use Boards/analytics to spot trends (success rate drops, cost spikes, etc.). Include example scenario.</span>

---

### Step 2: Extract detailed data

<span className="internal-note">Placeholder for: Methods for pulling call transcripts, structured outputs, tool call logs. Show how to filter/export data for analysis.</span>

---

### Step 3: Form a hypothesis

<span className="internal-note">Placeholder for: Common hypothesis patterns (prompt issues, tool description problems, routing logic, verbosity, etc.). Show example hypothesis formation process.</span>

---

### Step 4: Make targeted changes

<span className="internal-note">Placeholder for: How to revise prompts, update tool descriptions, refine conversation flows. Include before/after examples.</span>

---

### Step 5: Test before deploying

<span className="internal-note">Placeholder for: Creating Evals for specific failure cases, regression testing strategies. Show example test structure.</span>

---

### Step 6: Deploy

<span className="internal-note">Placeholder for: Deployment strategies (direct deploy, staged rollout, A/B testing). Include decision framework for choosing strategy.</span>

---

### Step 7: Verify improvement

<span className="internal-note">Placeholder for: Time windows for verification (immediate, 24h, 1 week), what to track, when to roll back.</span>

---

## Common optimization scenarios

**[Placeholder - Table of common patterns, root causes, and optimization actions]**

<span className="vapi-validation">What are the most common optimization scenarios Vapi customers encounter? What issues drive the most improvement iterations? Are there voice-specific optimization patterns we should highlight?</span>

---

## Optimization best practices

**[Placeholder - Full detail sections]**

Topics to cover:
- Start with high-impact, low-effort changes
- Track improvement over time (optimization log)
- Don't optimize prematurely (wait for data)
- Make one change at a time (clear cause-and-effect)
- Maintain regression tests

<span className="internal-note">Should we include specific guidance on optimization cadence (weekly reviews, monthly deep dives, quarterly retrospectives)?</span>

---

## What you'll learn in detailed guides

**Optimization is cross-functional** — it references tools from all previous stages:
- [Evals quickstart](/observability/evals-quickstart) — Test improvements before deploying
- [Boards quickstart](/observability/boards-quickstart) — Track metrics over time
- [Structured outputs quickstart](/assistants/structured-outputs-quickstart) — Extract failure data for analysis

(Planned) Optimization playbook — Common scenarios and solutions
(Planned) Advanced optimization — A/B testing, staged rollouts, impact measurement

---

## Key takeaway

**Optimize continuously**. The best teams treat observability as a loop: instrument → test → deploy → monitor → identify improvements → repeat. Data-driven iteration beats guesswork.

Start your optimization practice on day one. Don't wait until you have problems — establish the workflow while things are working, so you're ready when issues arise.

---

## Next steps

<CardGroup cols={2}>
  <Card
    title="Boards quickstart"
    icon="chart-line"
    href="/observability/boards-quickstart"
  >
    Set up monitoring to detect patterns
  </Card>

  <Card
    title="Evals quickstart"
    icon="clipboard-check"
    href="/observability/evals-quickstart"
  >
    Build tests to validate improvements
  </Card>

  <Card
    title="Production readiness"
    icon="check-circle"
    href="/observability/production-readiness"
  >
    Validate you're ready to optimize in production
  </Card>

  <Card
    title="Back to overview"
    icon="arrow-left"
    href="/observability/framework"
  >
    Return to observability framework
  </Card>
</CardGroup>
