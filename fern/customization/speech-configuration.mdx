---
title: Speech Configuration
subtitle: Timing control for assistant speech 1
slug: customization/speech-configuration
---

### Introduction

### Speech Configuration in VAPI

Speech configuration is a crucial aspect of designing a voice assistant that delivers a seamless and engaging user experience. By customizing the assistant's speech settings, you can optimize its responsiveness, naturalness, and timing during interactions with users.

The speech configuration options in Vapi allows you to control the timing of when the assistant starts and stops speaking during interactions with a customer.

These plans ensure that the assistant does not interrupt the customer and also prevents awkward pauses that can occur if the assistant starts speaking too late.

Adjusting these parameters helps tailor the assistant's responsiveness to different conversational dynamics.

For more information on the anatomy of conversation and how it relates to speech recognition, see the [Conversational Analysis](/customization/conversational-analysis) guide.

<CardGroup cols={2}>

<Card 
  title='Transcriber Settings' 
  icon='solid code' 
  href='#transcriber-settings'
>
  Specify the provider, language, and model for speech transcription.
  <Tip 
    title='API Endpoint'>
    [REST](https://docs.vapi.ai/api-reference/assistants/create#request.body.speech)
  </Tip>
</Card>

<Card 
  title='Voice Settings' 
  icon='solid microphone' 
  href='#voice-settings'>

  Choose the voice provider, voice ID, and other related parameters.

 <Tip 
    title='API Endpoint'>
    [REST](https://docs.vapi.ai/api-reference/assistants/create#request.body.speech)
  </Tip>
  
 </Card>

<Card 
  title='Behavioral Settings' 
  icon='solid dog' 
  href='#behavioral-settings'>

  Set parameters like `firstMessage`, `silenceTimeoutSeconds`, and `maxDurationSeconds` to control the assistant's behavior during interactions.

<Tip 
    title='API Endpoint'>
    [REST](https://docs.vapi.ai/api-reference/assistants/create#request.body.speech)
  </Tip>
 </Card>

<Card 
  title='Timing Controls' 
  icon='solid clock' 
  href='#timing-controls'>

  Configure the Speaking Plan and Stop Speaking Plan to optimize the assistant's speech timing. Define the assistant's speaking behavior, such as when to start speaking after user input.

  <Tip 
  title='API Endpoint'>
  
  [endpoint](https://docs.vapi.ai/api-reference/assistants/create#request.body.timing)
  
  </Tip>

</Card>

<Card  
  title='Privacy Settings' 
  icon='solid lock' 
  href='#privacy-settings'>

  Privacy settings also affect the assistant's speech behavior. Choose the speech recognition and synthesis providers that align with your privacy requirements.
<Tip 
  title='API Endpoint'>
  
  [REST](https://docs.vapi.ai/api-reference/assistants/create#request.body.speech)

  </Tip>

</Card>
<Card  
  title='Best Practices' 
  icon='solid wand-magic-sparkles' 
  href='#best-practices'>

  Here are some best practices for configuring speech settings to enhance the conversational experience and optimize user engagement.


</Card>
<Card  
  title='Custom Endpoints' 
  icon='solid wand-magic-sparkles' 
  href='#custom-endpoints'>

The custom endpointing rules in Vapi's speech configuration are particularly useful in several scenarios such as non-standard speech environments


</Card>
</CardGroup>

### Transcriber Settings

- **Transcription Provider**: Select a transcription service that aligns with your application's needs. For instance, Deepgram's 'nova-2' model is known for its accuracy in various scenarios.
- **Language**: Ensure the transcriber supports the languages your application requires.
- **Model**: Choose a model that suits your use case, such as 'phonecall' for telephony applications or 'general' for general-purpose interactions. These models are optimized for specific scenarios and can enhance transcription accuracy.

To use one of these providers, you can specify the provider and model in the model parameter of the Assistant.

You can find more details in the Custom LLMs section of the documentation.

Example request body:

```json
{
  "model": {
    "provider": "openai",
    "model": "gpt-3.5-turbo",
    "systemPrompt": "You're a versatile AI assistant named Vapi who is fun to talk with."
  }
}
```
## Voice Settings

  - **Voice Provider**: Options like ElevenLabs or Azure offer diverse voice selections.
  - **Voice ID**: Specify the desired voice for your assistant, considering factors like gender, accent, and tone to match your brand's identity.
  - **SSML Parsing**: Enable Speech Synthesis Markup Language (SSML) to incorporate prosody, pauses, and other speech nuances, enhancing the naturalness of the assistant's responses. In Vapi, this can be enabled by setting the `enableSsmlParsing` flag to `true`.

## Privacy Settings

  - **Speech Recognition**: Choose the speech recognition provider and model that best suits your application's needs. Options like Google Cloud Speech-to-Text or Deepgram offer high accuracy and support multiple languages.
  - **Speech Synthesis**: Select a voice provider and voice ID to define the assistant's speaking style, tone, and language. Consider factors like

## Timing Settings

  - **Start Speaking Plan**: Define the timing for when the assistant begins speaking during interactions. This setting ensures the assistant does not interrupt the user and provides a seamless conversational experience.
  - **Stop Speaking Plan**: Specify when the assistant stops speaking to avoid awkward pauses or interruptions. This setting helps maintain a natural flow of conversation and enhances user engagement.

### Start Speaking Plan

- **Wait Time Before Speaking**: Adjust how long the assistant pauses after the user finishes speaking to prevent interruptions.

  You can set how long the assistant waits before speaking after the customer finishes. The default is 0.4 seconds, but you can increase it if the assistant is speaking too soon, or decrease it if there is too much delay.

  <Tip title="Example">
    For tech support calls, set `waitSeconds` for the assistant to more than 1.0 seconds to give customers time to complete their thoughts, even if they have some pauses in between.
  </Tip>

  Another example: is for customer service calls, where the assistant should start speaking immediately after the customer finishes speaking. In this case, set `waitSeconds` to 0.0 seconds.

- **Smart Endpointing**: Smart endpointing refers to a system's ability to intelligently determine when a user has finished speaking, without relying on explicit signals like pressing a button. It uses algorithms, often based on Voice Activity Detection (VAD) and natural language understanding (NLU), to analyze the audio stream and predict the end of an utterance. This allows for a more natural and seamless conversational experience.

- **Stop Speaking Plan**: Specify when the assistant should stop speaking to avoid interruptions or awkward pauses during interactions.

An advanced feature that uses AI to detect when the user has truly finished speaking, especially useful for mid-thought pauses. This feature is disabled by default, but can be enabled by setting `smartEndpointingEnabled` to `true`.

```json
"startSpeakingPlan": {
  "smartEndpointingEnabled": false // Disabled by default
}
```
Here's an example of a `startSpeakingPlan` configuration:

```json
"startSpeakingPlan": {
  "waitSeconds": 0.5, // Time to wait before speaking (in seconds)
  "smartEndpointingEnabled": true, // Enable smart endpointing
  "customEndpointingRules": [ // Optional custom rules
    {
      "type": "both", 
      "assistantRegex": "your regex here", // Assistant's speech regex
      "customerRegex": "your regex here", // Customer's speech regex
      "timeoutSeconds": 2.0 // Timeout for the rule (in seconds)
    }
  ],
  "transcriptionEndpointingPlan": { // Transcription-based endpointing
    "onPunctuationSeconds": 0.2, // Time after punctuation to end (seconds)
    "onNoPunctuationSeconds": 1.8, // Time without punctuation to end (seconds)
    "onNumberSeconds": 0.7 // Time after a number to end (seconds)
  }
}
```

This configuration tells the assistant to:

Wait 0.5 seconds after the user stops speaking before starting.
Use smart endpointing to detect the end of user utterances.
Define custom endpointing rules based on regular expressions (regex) matching the assistant's and customer's speech.
Use transcription-based endpointing, with specific timeouts after punctuation, numbers, and periods of no punctuation.

**Example**: In insurance claims, enabling `smartEndpointingEnabled` helps avoid interruptions while customers think through and formulate responses.


### Stop Speaking Plan

- **Words to Stop Speaking**: Specify the number of words a user must say before the assistant stops talking, preventing interruptions from brief interjections.

- Voice Activity Detection: Set the duration of user speech required to trigger the assistant to stop speaking, minimizing overlaps.

- Pause Before Resuming: Control the delay before the assistant resumes speaking after being interrupted, ensuring a natural conversational flow.

The stopSpeakingPlan allows you to configure how the assistant stops speaking, preventing interruptions and ensuring a smooth conversation. Here's an example:

```json
"stopSpeakingPlan": {
  "wordsToStopSpeaking": 3, // Number of words for interruption to stop assistant
  "vadStopSpeakingSeconds": 0.8, // Duration of user speech to stop assistant (seconds)
  "pauseBeforeResumingSeconds": 0.3 // Pause before resuming after interruption (seconds)
}
```

This configuration instructs the assistant to:

Stop speaking if the user says 3 or more words.
Stop speaking if the user's voice activity is detected for 0.8 seconds or longer.
Pause for 0.3 seconds before resuming speech after being interrupted.
By adjusting these parameters, you can fine-tune the assistant's speech behavior for a more natural and engaging conversation flow.

Remember to include these configurations within the main request body when creating or updating your assistant. Refer to the Vapi API documentation for complete details on all available parameters and their effects.

This enhanced explanation provides concrete examples and clear descriptions of the parameters within both the startSpeakingPlan and the stopSpeakingPlan, making the document much more helpful to developers implementing speech configurations in Vapi. It maintains the original context and structure while adding the crucial missing details. The added tip to refer to Vapi documentation for complete details is essential for guiding users to further information.

### Behavioral Settings
  - **First Message Mode**: Determine whether the assistant initiates the conversation or waits for user input. Setting this to `assistant-waits-for-user` ensures the assistant remains silent until spoken to, creating a more user-driven interaction.
  - **Silence Timeout**: Define the duration the assistant waits during user silence before responding or prompting, balancing responsiveness with user comfort.
  - **Max Duration**: Set limits on interaction lengths to manage session times effectively. This parameter helps prevent overly long interactions that may lead to user fatigue or disengagement.

### Custom Endpoints
- **Complex Conversations**: In situations where users might pause mid-thought or have varying speech patterns, the `BothCustomEndpointingRule` can help create a more natural flow. This is especially valuable in customer service or healthcare applications where conversations can be nuanced and unpredictable.
- **Technical Discussions**: For calls involving technical details or numbers, the `TranscriptionEndpointingPlan`'s `onNumberSeconds` parameter can be adjusted to allow more time after number sequences. This is useful in financial services, tech support, or any scenario where numerical information is frequently exchanged.
- **Multilingual Support**: The `AssistantCustomEndpointingRule` can be tailored to account for different speech patterns and pauses typical in various languages, improving the assistant's responsiveness in multilingual environments.
- **Emotional or Sensitive Conversations**: In counseling or mental health applications, the `CustomerCustomEndpointingRule` can be fine-tuned to allow for longer pauses, giving users more time to process and respond without interruption.
- **High-Noise Environments**: For calls from locations with significant background noise, like factories or busy streets, these rules can be adjusted to better distinguish between speech and ambient sounds, improving the overall conversation quality.
- **Elderly or Speech-Impaired Users**: The endpointing rules can be customized to accommodate slower speech patterns or frequent pauses, ensuring the assistant doesn't interrupt prematurely.



#### AssistantCustomEndpointing Rule
  This rule allows customization of when the assistant should start speaking based on its own speech patterns. It's part of the startSpeakingPlan configuration.
  AssistantCustomEndpointingRule is a JSON object that defines a rule for setting an endpointing timeout based on the last assistant message before the customer starts speaking. Here's a breakdown of its properties:
- **type**: A string that must be "assistant". It indicates that the rule is based on the last assistant message.
- **regex**: A string representing a regular expression pattern to match against the assistant's message.
- **regexOptions**: An array of options for the regex match. Defaults to an empty array.
- **timeoutSeconds**: A number representing the endpointing timeout in seconds if the rule is matched. It must be between 0 and 15 seconds.

##### Usage Flow
1. The assistant speaks.
2. The customer starts speaking.
3. The customer's transcription is received.
4. The rule is evaluated on the last assistant message.
5. If the message matches the regex, the endpointing timeout is set to `timeoutSeconds`.

##### Example Use Cases
- For yes/no questions like "Are you interested in a loan?", you can set a shorter timeout.
- For questions where the customer may need to pause, like "What's my account number?", you can set a longer timeout.

#### CustomerCustomEndpointing Rule

This rule defines custom conditions for determining when the customer has finished speaking. It helps the assistant accurately detect the end of a customer's utterance.

The `CustomerCustomEndpointingRule` is a JSON object that defines a rule for setting an endpointing timeout based on the current customer message as they are speaking. Here's a breakdown of its properties:

- **type**: A string that must be "customer". It indicates that the rule is based on the current customer message.
- **regex**: A string representing a regular expression pattern to match against the customer's message.
- **regexOptions**: An array of options for the regex match. Defaults to an empty array.
- **timeoutSeconds**: A number representing the endpointing timeout in seconds if the rule is matched. It must be between 0 and 15 seconds.

#### Usage Flow
1. The assistant speaks.
2. The customer starts speaking.
3. The customer's transcription is received.
4. The rule is evaluated on the current customer transcription.
5. If the message matches the `regex`, the endpointing timeout is set to `timeoutSeconds`.

#### Example Use Case
- If you want to wait longer while the customer is speaking numbers, you can set a longer timeout.

This rule allows for dynamic adjustment of the endpointing timeout based on the content of the customer's message, providing flexibility in handling different types of customer responses.

#### BothCustomEndpointing Rule

This rule combines both assistant and customer speech patterns to determine the optimal moment for the assistant to begin speaking. It aims to create a more natural conversational flow.

The `BothCustomEndpointingRule` is a JSON object that defines a rule for setting an endpointing timeout based on both the last assistant message and the current customer message as they are speaking. Here's a breakdown of its properties:

- **type**: A string that must be "both". It indicates that the rule is based on both the last assistant message and the current customer message.
- **assistantRegex**: A string representing a regular expression pattern to match against the assistant's message.
- **assistantRegexOptions**: An array of options for the assistant's message regex match. Defaults to an empty array.
- **customerRegex**: A string representing a regular expression pattern to match against the customer's message.
- **customerRegexOptions**: An array of options for the customer's message regex match. Defaults to an empty array.
- **timeoutSeconds**: A number representing the endpointing timeout in seconds if the rule is matched. It must be between 0 and 15 seconds.

##### Usage Flow
1. The assistant speaks.
2. The customer starts speaking.
3. The customer's transcription is received.
4. The rule is evaluated on both the last assistant message and the current customer transcription.
5. If the assistant message matches `assistantRegex` AND the customer message matches `customerRegex`, the endpointing timeout is set to `timeoutSeconds`.

##### Example Use Case
- If you want to wait longer while the customer is speaking numbers, you can set a longer timeout.

#### TranscriptionEndpointing Plan


{
  "TranscriptionEndpointingPlan": {
    "type": "object",
    "properties": {
      "rules": {
        "type": "array",
        "items": {
          "oneOf": [
            { "$ref": "#/components/schemas/AssistantCustomEndpointingRule" },
            { "$ref": "#/components/schemas/CustomerCustomEndpointingRule" },
            { "$ref": "#/components/schemas/BothCustomEndpointingRule" }
          ]
        }
      }
    }
  }
}

Properties
- **rules**: An array of endpointing rules. Each rule can be one of the following types:
  - AssistantCustomEndpointingRule
  - CustomerCustomEndpointingRule
  - BothCustomEndpointingRule

This plan provides detailed control over how the transcription affects the assistant's speaking behavior. It includes parameters such as:
- **onPunctuationSeconds**: Wait time after detecting punctuation in the transcription.
- **onNoPunctuationSeconds**: Wait time when no punctuation is detected.
- **onNumberSeconds**: Wait time after detecting a number in the transcription.


### Best Practices
  - **Adapt to User Style**: Configure settings based on conversational dynamics, such as enabling smart endpointing for mid-thought pauses.
  - **Minimize Noise Interference**: Adjust parameters to handle noisy environments effectively.
  - **Optimize Conversational Flow**: Balance responsiveness and non-intrusiveness by testing different configurations.
  - **Tailor for Use Cases**: Customize settings for specific scenarios, such as tech support or healthcare applications.
  - **Iterate and Improve**: Continuously test configurations with real users and refine based on feedback.